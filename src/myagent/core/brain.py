"""
Brain 模块 - 与 Claude API 交互

负责:
- 发送请求到 Claude API
- 管理对话历史
- 工具调用
- API 故障切换
"""

import logging
import asyncio
import time
from typing import Any, Optional
from dataclasses import dataclass, field

from anthropic import Anthropic
from anthropic.types import Message, MessageParam, ToolParam

from ..config import settings

logger = logging.getLogger(__name__)


@dataclass
class Response:
    """LLM 响应"""
    content: str
    tool_calls: list[dict] = field(default_factory=list)
    stop_reason: str = ""
    usage: dict = field(default_factory=dict)


@dataclass
class Context:
    """对话上下文"""
    messages: list[MessageParam] = field(default_factory=list)
    system: str = ""
    tools: list[ToolParam] = field(default_factory=list)


@dataclass
class LLMEndpoint:
    """LLM API 端点配置"""
    name: str
    api_key: str
    base_url: str
    model: str
    priority: int = 0  # 优先级，数字越小优先级越高
    healthy: bool = True
    last_check: float = 0
    fail_count: int = 0


class Brain:
    """Agent 大脑 - LLM 交互层（支持故障切换）"""
    
    # 健康检查间隔（秒）
    HEALTH_CHECK_INTERVAL = 60
    # 失败阈值，超过则标记为不健康
    FAIL_THRESHOLD = 3
    # 请求超时（秒）
    REQUEST_TIMEOUT = 30
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
    ):
        self.max_tokens = max_tokens or settings.max_tokens
        
        # 配置多个 API 端点
        self._endpoints: list[LLMEndpoint] = []
        
        # 主端点（云雾 AI）
        primary_key = api_key or settings.anthropic_api_key
        primary_url = base_url or settings.anthropic_base_url
        primary_model = model or settings.default_model
        
        if primary_key:
            self._endpoints.append(LLMEndpoint(
                name="primary",
                api_key=primary_key,
                base_url=primary_url,
                model=primary_model,
                priority=0,
            ))
        
        # 备用端点（MiniMax）
        backup_key = getattr(settings, 'backup_api_key', None) or "MINIMAX_KEY_REMOVED"
        backup_url = getattr(settings, 'backup_base_url', None) or "https://api.minimaxi.com/anthropic"
        backup_model = getattr(settings, 'backup_model', None) or "MiniMax-M2.1"
        
        if backup_key:
            self._endpoints.append(LLMEndpoint(
                name="backup (MiniMax)",
                api_key=backup_key,
                base_url=backup_url,
                model=backup_model,
                priority=1,
            ))
        
        if not self._endpoints:
            raise ValueError(
                "At least one API key is required. "
                "Set ANTHROPIC_API_KEY environment variable or pass api_key parameter."
            )
        
        # 按优先级排序
        self._endpoints.sort(key=lambda x: x.priority)
        
        # 当前使用的端点索引
        self._current_endpoint_idx = 0
        
        # 创建客户端（设置超时和禁用自动重试）
        self._clients: dict[str, Anthropic] = {}
        for ep in self._endpoints:
            self._clients[ep.name] = Anthropic(
                api_key=ep.api_key,
                base_url=ep.base_url,
                timeout=self.REQUEST_TIMEOUT,  # 请求超时
                max_retries=0,  # 禁用 SDK 自动重试，由我们自己控制故障切换
            )
        
        # 公开属性（兼容旧代码）
        self._update_public_attrs()
        
        logger.info(f"Brain initialized with {len(self._endpoints)} endpoints")
        for ep in self._endpoints:
            logger.info(f"  - {ep.name}: {ep.model} @ {ep.base_url}")
    
    def _update_public_attrs(self) -> None:
        """更新公开属性"""
        ep = self._endpoints[self._current_endpoint_idx]
        self.api_key = ep.api_key
        self.base_url = ep.base_url
        self.model = ep.model
        self.client = self._clients[ep.name]
    
    def _get_healthy_endpoint(self) -> Optional[LLMEndpoint]:
        """获取健康的端点"""
        for ep in self._endpoints:
            if ep.healthy:
                return ep
        # 如果所有都不健康，重置并返回第一个
        for ep in self._endpoints:
            ep.healthy = True
            ep.fail_count = 0
        return self._endpoints[0] if self._endpoints else None
    
    def _mark_endpoint_failed(self, endpoint: LLMEndpoint) -> None:
        """标记端点失败"""
        endpoint.fail_count += 1
        if endpoint.fail_count >= self.FAIL_THRESHOLD:
            endpoint.healthy = False
            logger.warning(f"Endpoint {endpoint.name} marked as unhealthy after {endpoint.fail_count} failures")
    
    def _mark_endpoint_success(self, endpoint: LLMEndpoint) -> None:
        """标记端点成功"""
        endpoint.fail_count = 0
        endpoint.healthy = True
        endpoint.last_check = time.time()
    
    async def health_check(self, endpoint: Optional[LLMEndpoint] = None) -> bool:
        """
        健康检查
        
        Args:
            endpoint: 要检查的端点，None 则检查当前端点
        
        Returns:
            是否健康
        """
        if endpoint is None:
            endpoint = self._endpoints[self._current_endpoint_idx]
        
        client = self._clients[endpoint.name]
        
        try:
            # 发送简单测试请求
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.messages.create,
                    model=endpoint.model,
                    max_tokens=10,
                    messages=[{"role": "user", "content": "hi"}],
                ),
                timeout=10,
            )
            self._mark_endpoint_success(endpoint)
            logger.info(f"Health check passed for {endpoint.name}")
            return True
        except Exception as e:
            self._mark_endpoint_failed(endpoint)
            logger.warning(f"Health check failed for {endpoint.name}: {e}")
            return False
    
    def switch_to_backup(self) -> bool:
        """
        切换到备用端点
        
        Returns:
            是否成功切换
        """
        old_ep = self._endpoints[self._current_endpoint_idx]
        
        # 查找下一个健康的端点
        for i, ep in enumerate(self._endpoints):
            if i != self._current_endpoint_idx and ep.healthy:
                self._current_endpoint_idx = i
                self._update_public_attrs()
                logger.info(f"Switched from {old_ep.name} to {ep.name}")
                return True
        
        logger.warning("No healthy backup endpoint available")
        return False
    
    def get_current_endpoint_info(self) -> dict:
        """获取当前端点信息"""
        ep = self._endpoints[self._current_endpoint_idx]
        return {
            "name": ep.name,
            "model": ep.model,
            "base_url": ep.base_url,
            "healthy": ep.healthy,
        }
    
    def messages_create(self, **kwargs) -> Message:
        """
        同步调用 LLM API（带故障切换）
        
        这是对 client.messages.create 的包装，自动处理故障切换。
        Agent 中应使用此方法而不是直接调用 client.messages.create。
        
        故障切换逻辑:
        1. 从当前端点开始尝试
        2. 失败则尝试下一个端点
        3. 所有端点都失败才报错
        4. 成功后如果不是主端点，下次会先尝试主端点（自动恢复）
        
        Args:
            **kwargs: 传递给 messages.create 的参数
        
        Returns:
            LLM 响应
        """
        last_error = None
        
        # 按优先级尝试所有端点
        for i, endpoint in enumerate(self._endpoints):
            client = self._clients[endpoint.name]
            
            # 使用端点的模型（覆盖 kwargs 中的 model）
            request_kwargs = kwargs.copy()
            request_kwargs["model"] = endpoint.model
            
            try:
                logger.info(f"Sending request to {endpoint.name} ({endpoint.model})")
                
                response = client.messages.create(**request_kwargs)
                
                # 成功
                self._mark_endpoint_success(endpoint)
                
                # 如果不是主端点成功，记录一下
                if i > 0:
                    logger.info(f"Request succeeded on backup endpoint: {endpoint.name}")
                    # 下次请求仍会从主端点开始尝试（自动恢复机制）
                
                return response
                
            except Exception as e:
                last_error = str(e)
                logger.warning(f"Request failed for {endpoint.name}: {e}")
                self._mark_endpoint_failed(endpoint)
                
                # 如果还有更多端点，继续尝试
                if i < len(self._endpoints) - 1:
                    logger.info(f"Trying next endpoint...")
                    continue
        
        raise RuntimeError(f"All LLM endpoints failed: {last_error}")
    
    async def think(
        self,
        prompt: str,
        context: Optional[Context] = None,
        system: Optional[str] = None,
        tools: Optional[list[ToolParam]] = None,
    ) -> Response:
        """
        发送思考请求到 LLM（带自动故障切换）
        
        Args:
            prompt: 用户输入
            context: 对话上下文
            system: 系统提示词
            tools: 可用工具列表
        
        Returns:
            LLM 响应
        """
        messages: list[MessageParam] = []
        
        # 添加上下文中的历史消息
        if context and context.messages:
            messages.extend(context.messages)
        
        # 添加当前用户消息
        messages.append({"role": "user", "content": prompt})
        
        # 确定系统提示词
        sys_prompt = system or (context.system if context else "")
        
        # 确定工具列表
        tool_list = tools or (context.tools if context else [])
        
        # 尝试所有端点
        last_error = None
        tried_endpoints = set()
        
        while len(tried_endpoints) < len(self._endpoints):
            endpoint = self._get_healthy_endpoint()
            if endpoint is None or endpoint.name in tried_endpoints:
                # 没有更多端点可尝试
                break
            
            tried_endpoints.add(endpoint.name)
            client = self._clients[endpoint.name]
            
            try:
                # 构建请求参数
                request_params: dict[str, Any] = {
                    "model": endpoint.model,
                    "max_tokens": self.max_tokens,
                    "messages": messages,
                }
                
                if sys_prompt:
                    request_params["system"] = sys_prompt
                
                if tool_list:
                    request_params["tools"] = tool_list
                
                # 发送请求（带超时）
                logger.info(f"Sending request to {endpoint.name} ({endpoint.model})")
                
                response: Message = await asyncio.wait_for(
                    asyncio.to_thread(client.messages.create, **request_params),
                    timeout=self.REQUEST_TIMEOUT,
                )
                
                # 成功，标记端点健康
                self._mark_endpoint_success(endpoint)
                
                # 解析响应
                content = ""
                tool_calls = []
                
                for block in response.content:
                    if block.type == "text":
                        content += block.text
                    elif block.type == "tool_use":
                        tool_calls.append({
                            "id": block.id,
                            "name": block.name,
                            "input": block.input,
                        })
                
                return Response(
                    content=content,
                    tool_calls=tool_calls,
                    stop_reason=response.stop_reason or "",
                    usage={
                        "input_tokens": response.usage.input_tokens,
                        "output_tokens": response.usage.output_tokens,
                    },
                )
                
            except asyncio.TimeoutError:
                last_error = f"Request timeout ({self.REQUEST_TIMEOUT}s) for {endpoint.name}"
                logger.warning(last_error)
                self._mark_endpoint_failed(endpoint)
                # 尝试切换到备用
                self.switch_to_backup()
                
            except Exception as e:
                last_error = str(e)
                logger.warning(f"Request failed for {endpoint.name}: {e}")
                self._mark_endpoint_failed(endpoint)
                # 尝试切换到备用
                self.switch_to_backup()
        
        # 所有端点都失败
        logger.error(f"All endpoints failed. Last error: {last_error}")
        raise RuntimeError(f"All LLM endpoints failed: {last_error}")
    
    async def plan(self, task: str, context: Optional[Context] = None) -> str:
        """
        为任务生成执行计划
        
        Args:
            task: 任务描述
            context: 上下文
        
        Returns:
            执行计划文本
        """
        prompt = f"""请为以下任务制定详细的执行计划:

任务: {task}

要求:
1. 分解为具体的步骤
2. 识别需要的工具和技能
3. 考虑可能的失败情况和备选方案
4. 估计每个步骤的复杂度

请以 Markdown 格式输出计划。"""

        response = await self.think(prompt, context)
        return response.content
    
    async def generate_code(
        self,
        description: str,
        language: str = "python",
        context: Optional[Context] = None,
    ) -> str:
        """
        生成代码
        
        Args:
            description: 代码功能描述
            language: 编程语言
            context: 上下文
        
        Returns:
            生成的代码
        """
        prompt = f"""请生成以下功能的 {language} 代码:

{description}

要求:
1. 代码应该完整、可运行
2. 包含必要的导入语句
3. 添加适当的注释和 docstring
4. 遵循 {language} 的最佳实践
5. 如果是类，包含类型提示

只输出代码，不要解释。"""

        response = await self.think(prompt, context)
        
        # 提取代码块
        code = response.content
        if f"```{language}" in code:
            start = code.find(f"```{language}") + len(f"```{language}")
            end = code.find("```", start)
            if end > start:
                code = code[start:end].strip()
        elif "```" in code:
            start = code.find("```") + 3
            end = code.find("```", start)
            if end > start:
                code = code[start:end].strip()
        
        return code
    
    async def analyze_error(
        self,
        error: str,
        context: Optional[str] = None,
    ) -> dict[str, Any]:
        """
        分析错误并提供解决方案
        
        Args:
            error: 错误信息
            context: 错误上下文
        
        Returns:
            分析结果和建议
        """
        prompt = f"""请分析以下错误并提供解决方案:

错误信息:
{error}

{"上下文:" + context if context else ""}

请提供:
1. 错误原因分析
2. 可能的解决方案（按优先级排序）
3. 如何避免类似错误

以 JSON 格式输出:
{{
    "cause": "错误原因",
    "solutions": ["解决方案1", "解决方案2"],
    "prevention": "预防措施"
}}"""

        response = await self.think(prompt)
        
        # 尝试解析 JSON
        import json
        try:
            content = response.content
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                content = content[start:end].strip()
            
            return json.loads(content)
        except json.JSONDecodeError:
            return {
                "cause": "Unable to parse error analysis",
                "solutions": [response.content],
                "prevention": "",
            }
