"""
æ¨ç†-è¡ŒåŠ¨å¼•æ“ (ReAct Pattern)

ä» agent.py çš„ _chat_with_tools_and_context é‡æ„ä¸ºæ˜¾å¼çš„
Reason -> Act -> Observe ä¸‰é˜¶æ®µå¾ªç¯ã€‚

æ ¸å¿ƒèŒè´£:
- æ˜¾å¼æ¨ç†å¾ªç¯ç®¡ç†ï¼ˆReason / Act / Observeï¼‰
- LLM å“åº”è§£æä¸ Decision åˆ†ç±»
- å·¥å…·è°ƒç”¨ç¼–æ’ï¼ˆå§”æ‰˜ç»™ ToolExecutorï¼‰
- ä¸Šä¸‹æ–‡å‹ç¼©è§¦å‘ï¼ˆå§”æ‰˜ç»™ ContextManagerï¼‰
- å¾ªç¯æ£€æµ‹ï¼ˆç­¾åé‡å¤ã€è‡ªæ£€é—´éš”ã€å®‰å…¨é˜ˆå€¼ï¼‰
- æ¨¡å‹åˆ‡æ¢é€»è¾‘
- ä»»åŠ¡å®Œæˆåº¦éªŒè¯ï¼ˆå§”æ‰˜ç»™ ResponseHandlerï¼‰
"""

import asyncio
import copy
import hashlib
import json
import logging
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any

from ..config import settings
from ..tracing.tracer import get_tracer
from .agent_state import AgentState, TaskState, TaskStatus
from .context_manager import ContextManager, _CancelledError as _CtxCancelledError
from .errors import UserCancelledError
from .response_handler import ResponseHandler, clean_llm_response, strip_thinking_tags
from .tool_executor import ToolExecutor

logger = logging.getLogger(__name__)


class DecisionType(Enum):
    """LLM å†³ç­–ç±»å‹"""
    FINAL_ANSWER = "final_answer"  # çº¯æ–‡æœ¬å“åº”
    TOOL_CALLS = "tool_calls"  # éœ€è¦å·¥å…·è°ƒç”¨


@dataclass
class Decision:
    """LLM æ¨ç†å†³ç­–"""
    type: DecisionType
    text_content: str = ""
    tool_calls: list[dict] = field(default_factory=list)
    thinking_content: str = ""
    raw_response: Any = None
    stop_reason: str = ""
    # å®Œæ•´çš„ assistant_contentï¼ˆä¿ç•™ thinking å—ç­‰ï¼‰
    assistant_content: list[dict] = field(default_factory=list)


@dataclass
class Checkpoint:
    """
    å†³ç­–æ£€æŸ¥ç‚¹ï¼Œç”¨äºå¤šè·¯å¾„æ¢ç´¢å’Œå›æ»šã€‚

    åœ¨å…³é”®å†³ç­–ç‚¹ä¿å­˜æ¶ˆæ¯å†å²å’Œä»»åŠ¡çŠ¶æ€çš„å¿«ç…§ï¼Œ
    å½“æ£€æµ‹åˆ°å¾ªç¯ã€è¿ç»­å¤±è´¥ç­‰é—®é¢˜æ—¶å¯å›æ»šåˆ°ä¹‹å‰çš„æ£€æŸ¥ç‚¹ï¼Œ
    é™„åŠ å¤±è´¥ç»éªŒæç¤ºåé‡æ–°æ¨ç†ã€‚
    """

    id: str
    messages_snapshot: list[dict]  # æ·±æ‹·è´æ¶ˆæ¯å†å²
    state_snapshot: dict  # åºåˆ—åŒ–çš„ TaskState å…³é”®å­—æ®µ
    decision_summary: str  # åšå‡ºçš„å†³ç­–æ‘˜è¦
    iteration: int  # ä¿å­˜æ—¶çš„è¿­ä»£æ¬¡æ•°
    timestamp: float = field(default_factory=time.time)
    tool_names: list[str] = field(default_factory=list)  # è¯¥å†³ç­–è°ƒç”¨çš„å·¥å…·


class ReasoningEngine:
    """
    æ˜¾å¼æ¨ç†-è¡ŒåŠ¨å¼•æ“ã€‚

    æ›¿ä»£ agent.py ä¸­çš„ _chat_with_tools_and_context()ï¼Œ
    å°†éšå¼å¾ªç¯é‡æ„ä¸ºæ¸…æ™°çš„ Reason -> Act -> Observe ä¸‰é˜¶æ®µã€‚
    æ”¯æŒ Checkpoint + Rollback å¤šè·¯å¾„æ¢ç´¢ã€‚
    """

    # æ£€æŸ¥ç‚¹é…ç½®
    MAX_CHECKPOINTS = 5  # ä¿ç•™æœ€è¿‘ N ä¸ªæ£€æŸ¥ç‚¹
    CONSECUTIVE_FAIL_THRESHOLD = 3  # åŒä¸€å·¥å…·è¿ç»­å¤±è´¥ N æ¬¡è§¦å‘å›æ»š

    def __init__(
        self,
        brain: Any,
        tool_executor: ToolExecutor,
        context_manager: ContextManager,
        response_handler: ResponseHandler,
        agent_state: AgentState,
    ) -> None:
        self._brain = brain
        self._tool_executor = tool_executor
        self._context_manager = context_manager
        self._response_handler = response_handler
        self._state = agent_state

        # Checkpoint ç®¡ç†
        self._checkpoints: list[Checkpoint] = []
        self._tool_failure_counter: dict[str, int] = {}  # tool_name -> consecutive_failures

        # æ€ç»´é“¾: æš‚å­˜æœ€è¿‘ä¸€æ¬¡æ¨ç†çš„ react_traceï¼Œä¾› agent_handler è¯»å–
        self._last_react_trace: list[dict] = []

        # ä¸Šä¸€æ¬¡æ¨ç†çš„é€€å‡ºåŸå› ï¼šnormal / ask_user
        # _finalize_session æ®æ­¤å†³å®šæ˜¯å¦è‡ªåŠ¨å…³é—­ Plan
        self._last_exit_reason: str = "normal"

        # æµè§ˆå™¨"è¯»é¡µé¢çŠ¶æ€"å·¥å…·
        self._browser_page_read_tools = frozenset({
            "browser_get_content", "browser_screenshot",
        })

    # ==================== ask_user ç­‰å¾…ç”¨æˆ·å›å¤ ====================

    async def _wait_for_user_reply(
        self,
        question: str,
        state: TaskState,
        *,
        timeout_seconds: int = 60,
        max_reminders: int = 1,
        poll_interval: float = 2.0,
    ) -> str | None:
        """
        ç­‰å¾…ç”¨æˆ·å›å¤ ask_user çš„é—®é¢˜ï¼ˆä»… IM æ¨¡å¼ç”Ÿæ•ˆï¼‰ã€‚

        åˆ©ç”¨ Gateway çš„ä¸­æ–­é˜Ÿåˆ—æœºåˆ¶ï¼šIM ç”¨æˆ·åœ¨ Agent å¤„ç†ä¸­å‘é€çš„æ¶ˆæ¯
        ä¼šè¢« Gateway æ”¾å…¥ interrupt_queueï¼Œæœ¬æ–¹æ³•è½®è¯¢è¯¥é˜Ÿåˆ—è·å–å›å¤ã€‚

        æµç¨‹:
        1. é€šè¿‡ Gateway å‘é€é—®é¢˜ç»™ç”¨æˆ·
        2. è½®è¯¢ interrupt_queue ç­‰å¾…å›å¤ï¼ˆtimeout_seconds è¶…æ—¶ï¼‰
        3. ç¬¬ä¸€æ¬¡è¶…æ—¶ â†’ å‘é€æé†’ï¼Œå†ç­‰ä¸€è½®
        4. ç¬¬äºŒæ¬¡è¶…æ—¶ â†’ è¿”å› Noneï¼Œç”±è°ƒç”¨æ–¹æ³¨å…¥ç³»ç»Ÿæ¶ˆæ¯è®© LLM è‡ªè¡Œå†³ç­–

        Args:
            question: è¦å‘é€ç»™ç”¨æˆ·çš„é—®é¢˜æ–‡æœ¬
            state: å½“å‰ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºå–æ¶ˆæ£€æŸ¥ï¼‰
            timeout_seconds: æ¯è½®ç­‰å¾…è¶…æ—¶ï¼ˆç§’ï¼‰
            max_reminders: æœ€å¤§è¿½é—®æé†’æ¬¡æ•°
            poll_interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰

        Returns:
            ç”¨æˆ·å›å¤æ–‡æœ¬ï¼Œæˆ– Noneï¼ˆè¶…æ—¶/æ—  gateway/è¢«å–æ¶ˆï¼‰
        """
        # è·å– gateway å’Œ session å¼•ç”¨
        session = self._state.current_session
        if not session:
            return None

        gateway = session.get_metadata("_gateway") if hasattr(session, "get_metadata") else None
        session_key = session.get_metadata("_session_key") if gateway else None

        if not gateway or not session_key:
            # CLI æ¨¡å¼æˆ–æ—  gatewayï¼Œä¸åšç­‰å¾…
            return None

        # å‘é€é—®é¢˜åˆ°ç”¨æˆ·
        try:
            await gateway.send_to_session(session, question, role="assistant")
            logger.info(f"[ask_user] Question sent to user, waiting for reply (timeout={timeout_seconds}s)")
        except Exception as e:
            logger.warning(f"[ask_user] Failed to send question via gateway: {e}")
            return None

        reminders_sent = 0

        while reminders_sent <= max_reminders:
            # è½®è¯¢ç­‰å¾…ç”¨æˆ·å›å¤
            elapsed = 0.0

            while elapsed < timeout_seconds:
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                if state.cancelled:
                    logger.info("[ask_user] Task cancelled while waiting for reply")
                    return None

                # æ£€æŸ¥ä¸­æ–­é˜Ÿåˆ—
                try:
                    reply_msg = await gateway.check_interrupt(session_key)
                except Exception as e:
                    logger.warning(f"[ask_user] check_interrupt error: {e}")
                    reply_msg = None

                if reply_msg:
                    # ä» UnifiedMessage æå–æ–‡æœ¬
                    reply_text = (
                        reply_msg.plain_text.strip()
                        if hasattr(reply_msg, "plain_text") and reply_msg.plain_text
                        else str(reply_msg).strip()
                    )
                    if reply_text:
                        logger.info(f"[ask_user] User replied: {reply_text[:80]}")
                        # è®°å½•åˆ° session å†å²
                        try:
                            session.add_message(role="user", content=reply_text, source="ask_user_reply")
                        except Exception:
                            pass
                        return reply_text

                await asyncio.sleep(poll_interval)
                elapsed += poll_interval

            # æœ¬è½®è¶…æ—¶
            if reminders_sent < max_reminders:
                # å‘é€è¿½é—®æé†’
                reminders_sent += 1
                reminder = "â° æˆ‘åœ¨ç­‰ä½ å›å¤ä¸Šé¢çš„é—®é¢˜å“¦ï¼Œçœ‹åˆ°çš„è¯å›å¤ä¸€ä¸‹~"
                try:
                    await gateway.send_to_session(session, reminder, role="assistant")
                    logger.info(f"[ask_user] Timeout #{reminders_sent}, reminder sent")
                except Exception as e:
                    logger.warning(f"[ask_user] Failed to send reminder: {e}")
            else:
                # è¿½é—®æ¬¡æ•°ç”¨å°½ï¼Œè¿”å› None
                logger.info(
                    f"[ask_user] Final timeout after {reminders_sent} reminder(s), "
                    f"total wait ~{timeout_seconds * (max_reminders + 1)}s"
                )
                return None

        return None

    # ==================== Checkpoint / Rollback ====================

    def _save_checkpoint(
        self,
        messages: list[dict],
        state: TaskState,
        decision: Decision,
        iteration: int,
    ) -> None:
        """
        åœ¨å…³é”®å†³ç­–ç‚¹ä¿å­˜æ£€æŸ¥ç‚¹ã€‚

        ä»…åœ¨å·¥å…·è°ƒç”¨å†³ç­–æ—¶ä¿å­˜ï¼ˆçº¯æ–‡æœ¬å“åº”ä¸éœ€è¦å›æ»šï¼‰ã€‚
        ä¿ç•™æœ€è¿‘ MAX_CHECKPOINTS ä¸ªæ£€æŸ¥ç‚¹ä»¥æ§åˆ¶å†…å­˜ã€‚
        """
        tool_names = [tc.get("name", "") for tc in decision.tool_calls]
        summary = f"iteration={iteration}, tools=[{', '.join(tool_names)}]"

        cp = Checkpoint(
            id=str(uuid.uuid4())[:8],
            messages_snapshot=copy.deepcopy(messages),
            state_snapshot={
                "iteration": state.iteration,
                "status": state.status.value,
                "executed_tools": list(state.tools_executed),
            },
            decision_summary=summary,
            iteration=iteration,
            tool_names=tool_names,
        )
        self._checkpoints.append(cp)

        # ä¿ç•™æœ€è¿‘ N ä¸ª
        if len(self._checkpoints) > self.MAX_CHECKPOINTS:
            self._checkpoints = self._checkpoints[-self.MAX_CHECKPOINTS:]

        logger.debug(f"[Checkpoint] Saved: {cp.id} at iteration {iteration}")

    def _record_tool_result(self, tool_name: str, success: bool) -> None:
        """è®°å½•å·¥å…·æ‰§è¡Œç»“æœï¼Œç”¨äºè¿ç»­å¤±è´¥æ£€æµ‹ã€‚"""
        if success:
            self._tool_failure_counter[tool_name] = 0
        else:
            self._tool_failure_counter[tool_name] = (
                self._tool_failure_counter.get(tool_name, 0) + 1
            )

    def _should_rollback(self, tool_results: list[dict]) -> tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘å›æ»šã€‚

        è§¦å‘æ¡ä»¶:
        1. åŒä¸€å·¥å…·è¿ç»­å¤±è´¥ >= CONSECUTIVE_FAIL_THRESHOLD æ¬¡
        2. æ•´æ‰¹å·¥å…·å…¨éƒ¨å¤±è´¥

        Returns:
            (should_rollback, reason)
        """
        if not self._checkpoints:
            return False, ""

        # æ£€æŸ¥æœ¬æ‰¹æ¬¡å·¥å…·æ‰§è¡Œç»“æœ
        batch_failures = []
        for result in tool_results:
            content = ""
            if isinstance(result, dict):
                content = str(result.get("content", ""))
            elif isinstance(result, str):
                content = result

            has_error = any(marker in content for marker in [
                "âŒ", "âš ï¸ å·¥å…·æ‰§è¡Œé”™è¯¯", "é”™è¯¯ç±»å‹:", "ToolError",
            ])
            has_success = any(marker in content for marker in [
                "âœ…", '"status": "delivered"', '"ok": true',
            ])

            # éƒ¨åˆ†æˆåŠŸï¼ˆå¦‚ deliver_artifacts 2å¼ å›¾å‘äº†1å¼ ï¼‰ä¸ç®—å¤±è´¥ï¼Œ
            # é¿å…å›æ»šå·²ç»å‘å‡ºçš„ä¸å¯æ’¤å›å†…å®¹
            is_failed = has_error and not has_success
            batch_failures.append(is_failed)

        # æ•´æ‰¹å…¨éƒ¨å¤±è´¥
        if batch_failures and all(batch_failures):
            return True, "æœ¬è½®æ‰€æœ‰å·¥å…·è°ƒç”¨å‡å¤±è´¥"

        # å•å·¥å…·è¿ç»­å¤±è´¥
        for tool_name, count in self._tool_failure_counter.items():
            if count >= self.CONSECUTIVE_FAIL_THRESHOLD:
                return True, f"å·¥å…· '{tool_name}' è¿ç»­å¤±è´¥ {count} æ¬¡"

        return False, ""

    def _rollback(self, reason: str) -> tuple[list[dict], int] | None:
        """
        æ‰§è¡Œå›æ»š: æ¢å¤åˆ°ä¸Šä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚

        åœ¨æ¢å¤çš„æ¶ˆæ¯å†å²æœ«å°¾é™„åŠ å¤±è´¥ç»éªŒæç¤ºï¼Œ
        å¸®åŠ© LLM é¿å…é‡è¹ˆè¦†è¾™ã€‚

        Returns:
            (restored_messages, checkpoint_iteration) or None if no checkpoints
        """
        if not self._checkpoints:
            return None

        # å¼¹å‡ºæœ€è¿‘çš„æ£€æŸ¥ç‚¹ï¼ˆé¿å…å›æ»šåˆ°åŒä¸€ä¸ªç‚¹ï¼‰
        cp = self._checkpoints.pop()
        restored_messages = copy.deepcopy(cp.messages_snapshot)

        # é™„åŠ å¤±è´¥ç»éªŒ
        failure_hint = (
            f"[ç³»ç»Ÿæç¤º] ä¹‹å‰çš„æ–¹æ¡ˆå¤±è´¥äº†ï¼ˆåŸå› : {reason}ï¼‰ã€‚"
            f"å¤±è´¥çš„å†³ç­–: {cp.decision_summary}ã€‚"
            f"è¯·å°è¯•å®Œå…¨ä¸åŒçš„æ–¹æ³•æ¥å®Œæˆä»»åŠ¡ã€‚"
            f"é¿å…ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„å·¥å…·å‚æ•°ç»„åˆã€‚"
        )
        restored_messages.append({
            "role": "user",
            "content": failure_hint,
        })

        # é‡ç½®å¤±è´¥è®¡æ•°å™¨
        self._tool_failure_counter.clear()

        logger.info(
            f"[Rollback] Rolled back to checkpoint {cp.id} "
            f"(iteration {cp.iteration}). Reason: {reason}"
        )

        return restored_messages, cp.iteration

    async def run(
        self,
        messages: list[dict],
        *,
        tools: list[dict],
        system_prompt: str = "",
        base_system_prompt: str = "",
        task_description: str = "",
        task_monitor: Any = None,
        session_type: str = "cli",
        interrupt_check_fn: Any = None,
        conversation_id: str | None = None,
        thinking_mode: str | None = None,
        thinking_depth: str | None = None,
        progress_callback: Any = None,
    ) -> str:
        """
        ä¸»æ¨ç†å¾ªç¯: Reason -> Act -> Observeã€‚

        Args:
            messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            base_system_prompt: åŸºç¡€ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å«åŠ¨æ€ Planï¼‰
            task_description: ä»»åŠ¡æè¿°
            task_monitor: ä»»åŠ¡ç›‘æ§å™¨
            session_type: ä¼šè¯ç±»å‹
            interrupt_check_fn: ä¸­æ–­æ£€æŸ¥å‡½æ•°
            conversation_id: å¯¹è¯ ID
            thinking_mode: æ€è€ƒæ¨¡å¼è¦†ç›– ('auto'/'on'/'off'/None)
            thinking_depth: æ€è€ƒæ·±åº¦ ('low'/'medium'/'high'/None)
            progress_callback: è¿›åº¦å›è°ƒ async fn(str) -> Noneï¼Œç”¨äº IM å®æ—¶è¾“å‡ºæ€ç»´é“¾

        Returns:
            æœ€ç»ˆå“åº”æ–‡æœ¬
        """
        self._last_exit_reason = "normal"

        state = self._state.current_task
        if not state or not state.is_active or state.cancelled:
            state = self._state.begin_task()
        elif state.status == TaskStatus.ACTING:
            logger.warning(
                f"[State] Previous task stuck in {state.status.value}, force resetting for new message"
            )
            state = self._state.begin_task()

        # å®‰å…¨æ ¡éªŒï¼šbegin_task è¿”å›çš„ state ä¸åº”æºå¸¦å–æ¶ˆæ ‡å¿—
        if state.cancelled:
            logger.error(
                f"[State] CRITICAL: fresh task {state.task_id[:8]} has cancelled=True, "
                f"reason={state.cancel_reason!r}. Force clearing."
            )
            state.cancelled = False
            state.cancel_reason = ""
            state.cancel_event = asyncio.Event()

        self._context_manager.set_cancel_event(state.cancel_event)

        tracer = get_tracer()
        tracer.begin_trace(session_id=state.session_id, metadata={
            "task_description": task_description[:200] if task_description else "",
            "session_type": session_type,
            "model": self._brain.model,
        })

        max_iterations = settings.max_iterations

        # è¿›åº¦å›è°ƒè¾…åŠ©ï¼ˆå®‰å…¨è°ƒç”¨ï¼Œå¿½ç•¥å¼‚å¸¸ï¼‰
        async def _emit_progress(text: str) -> None:
            if progress_callback and text:
                try:
                    await progress_callback(text)
                except Exception:
                    pass

        # ä¿å­˜åŸå§‹ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºæ¨¡å‹åˆ‡æ¢æ—¶é‡ç½®ä¸Šä¸‹æ–‡ï¼‰
        state.original_user_messages = [
            msg for msg in messages if self._is_human_user_message(msg)
        ]

        working_messages = list(messages)
        current_model = self._brain.model

        # ForceToolCall é…ç½®
        if session_type == "im":
            base_force_retries = 0
        else:
            base_force_retries = max(0, int(getattr(settings, "force_tool_call_max_retries", 1)))

        max_no_tool_retries = self._effective_force_retries(base_force_retries, conversation_id)
        max_verify_retries = 3
        max_confirmation_text_retries = 1

        # è¿½è¸ªå˜é‡
        executed_tool_names: list[str] = []
        delivery_receipts: list[dict] = []
        _last_browser_url = ""

        # å¾ªç¯è®¡æ•°å™¨
        consecutive_tool_rounds = 0
        no_tool_call_count = 0
        verify_incomplete_count = 0
        no_confirmation_text_count = 0
        tools_executed_in_task = False

        # å¾ªç¯æ£€æµ‹
        recent_tool_signatures: list[str] = []
        tool_pattern_window = 8
        llm_self_check_interval = 10
        extreme_safety_threshold = 50

        def _build_effective_system_prompt() -> str:
            """åŠ¨æ€è¿½åŠ æ´»è·ƒ Plan"""
            try:
                from ..tools.handlers.plan import get_active_plan_prompt
                _cid = conversation_id
                prompt = base_system_prompt or system_prompt
                if _cid:
                    plan_section = get_active_plan_prompt(_cid)
                    if plan_section:
                        prompt += f"\n\n{plan_section}\n"
                return prompt
            except Exception:
                return base_system_prompt or system_prompt

        def _make_tool_signature(tc: dict) -> str:
            """ç”Ÿæˆå·¥å…·ç­¾å"""
            nonlocal _last_browser_url
            name = tc.get("name", "")
            inp = tc.get("input", {})

            if name == "browser_navigate":
                _last_browser_url = inp.get("url", "")

            try:
                param_str = json.dumps(inp, sort_keys=True, ensure_ascii=False)
            except Exception:
                param_str = str(inp)

            if name in self._browser_page_read_tools and len(param_str) <= 20 and _last_browser_url:
                param_str = f"{param_str}|url={_last_browser_url}"

            param_hash = hashlib.md5(param_str.encode()).hexdigest()[:8]
            return f"{name}({param_hash})"

        # ==================== ä¸»å¾ªç¯ ====================
        logger.info(f"[ReAct] === Loop started (max_iterations={max_iterations}, model={current_model}) ===")

        react_trace: list[dict] = []
        _trace_started_at = datetime.now().isoformat()

        for iteration in range(max_iterations):
            state.iteration = iteration

            # æ£€æŸ¥å–æ¶ˆ
            if state.cancelled:
                logger.info(f"[ReAct] Task cancelled at iteration start: {state.cancel_reason}")
                self._save_react_trace(react_trace, conversation_id, session_type, "cancelled", _trace_started_at)
                tracer.end_trace(metadata={"result": "cancelled", "iterations": iteration})
                return "âœ… ä»»åŠ¡å·²åœæ­¢ã€‚"

            # ä»»åŠ¡ç›‘æ§
            if task_monitor:
                task_monitor.begin_iteration(iteration + 1, current_model)
                # æ¨¡å‹åˆ‡æ¢æ£€æŸ¥
                switch_result = self._check_model_switch(
                    task_monitor, state, working_messages, current_model
                )
                if switch_result:
                    current_model, working_messages = switch_result
                    no_tool_call_count = 0
                    tools_executed_in_task = False
                    verify_incomplete_count = 0
                    executed_tool_names = []
                    consecutive_tool_rounds = 0
                    recent_tool_signatures = []
                    no_confirmation_text_count = 0

            _ctx_compressed_info: dict | None = None
            if len(working_messages) > 2:
                _before_tokens = self._context_manager.estimate_messages_tokens(working_messages)
                try:
                    working_messages = await self._context_manager.compress_if_needed(
                        working_messages,
                        system_prompt=_build_effective_system_prompt(),
                        tools=tools,
                    )
                except _CtxCancelledError:
                    raise UserCancelledError(reason=state.cancel_reason or "ç”¨æˆ·è¯·æ±‚åœæ­¢", source="context_compress")
                _after_tokens = self._context_manager.estimate_messages_tokens(working_messages)
                if _after_tokens < _before_tokens:
                    _ctx_compressed_info = {
                        "before_tokens": _before_tokens,
                        "after_tokens": _after_tokens,
                    }
                    await _emit_progress(
                        f"ğŸ“¦ ä¸Šä¸‹æ–‡å‹ç¼©: {_before_tokens//1000}k â†’ {_after_tokens//1000}k tokens"
                    )
                    logger.info(
                        f"[ReAct] Context compressed: {_before_tokens} â†’ {_after_tokens} tokens"
                    )

            # ==================== REASON é˜¶æ®µ ====================
            logger.info(f"[ReAct] Iter {iteration+1}/{max_iterations} â€” REASON (model={current_model})")
            if state.status != TaskStatus.REASONING:
                state.transition(TaskStatus.REASONING)

            _thinking_t0 = time.time()  # æ€ç»´é“¾: è®°å½• thinking å¼€å§‹æ—¶é—´
            try:
                decision = await self._reason(
                    working_messages,
                    system_prompt=_build_effective_system_prompt(),
                    tools=tools,
                    current_model=current_model,
                    conversation_id=conversation_id,
                    thinking_mode=thinking_mode,
                    thinking_depth=thinking_depth,
                )

                if task_monitor:
                    task_monitor.reset_retry_count()

            except UserCancelledError:
                raise
            except Exception as e:
                logger.error(f"[LLM] Brain call failed: {e}")
                retry_result = self._handle_llm_error(
                    e, task_monitor, state, working_messages, current_model
                )
                if retry_result == "retry":
                    # sleep å¯è¢« cancel_event ä¸­æ–­
                    _sleep = asyncio.create_task(asyncio.sleep(2))
                    _cw = asyncio.create_task(cancel_event.wait())
                    _done, _pend = await asyncio.wait({_sleep, _cw}, return_when=asyncio.FIRST_COMPLETED)
                    for _t in _pend:
                        _t.cancel()
                        try:
                            await _t
                        except (asyncio.CancelledError, Exception):
                            pass
                    if _cw in _done:
                        raise UserCancelledError(reason=state.cancel_reason or "ç”¨æˆ·è¯·æ±‚åœæ­¢", source="retry_sleep")
                    continue
                elif isinstance(retry_result, tuple):
                    current_model, working_messages = retry_result
                    no_tool_call_count = 0
                    tools_executed_in_task = False
                    verify_incomplete_count = 0
                    executed_tool_names = []
                    consecutive_tool_rounds = 0
                    recent_tool_signatures = []
                    no_confirmation_text_count = 0
                    continue
                else:
                    raise

            _thinking_duration_ms = int((time.time() - _thinking_t0) * 1000)

            # === IM è¿›åº¦: thinking å†…å®¹ ===
            if decision.thinking_content:
                _think_preview = decision.thinking_content[:200].strip().replace("\n", " ")
                if len(decision.thinking_content) > 200:
                    _think_preview += "..."
                await _emit_progress(f"ğŸ’­ {_think_preview}")

            # === IM è¿›åº¦: LLM æ¨ç†æ„å›¾ ===
            _decision_text_run = (decision.text_content or "").strip().replace("\n", " ")
            if _decision_text_run and decision.type == DecisionType.TOOL_CALLS:
                _text_preview = _decision_text_run[:300]
                if len(_decision_text_run) > 300:
                    _text_preview += "..."
                await _emit_progress(_text_preview)

            if task_monitor:
                task_monitor.end_iteration(decision.text_content or "")

            # -- æ”¶é›† ReAct trace æ•°æ® --
            # token ä¿¡æ¯ä» raw_response.usage æå–ï¼ˆDecision æœ¬èº«ä¸æºå¸¦ tokenï¼‰
            _raw = decision.raw_response
            _usage = getattr(_raw, "usage", None) if _raw else None
            _in_tokens = getattr(_usage, "input_tokens", 0) if _usage else 0
            _out_tokens = getattr(_usage, "output_tokens", 0) if _usage else 0
            _iter_trace: dict = {
                "iteration": iteration + 1,
                "timestamp": datetime.now().isoformat(),
                "decision_type": decision.type.value if hasattr(decision.type, "value") else str(decision.type),
                "model": current_model,
                "thinking": (decision.thinking_content or "")[:500] if decision.thinking_content else None,
                "thinking_duration_ms": _thinking_duration_ms,
                "text": (decision.text_content or "")[:2000] if decision.text_content else None,
                "tool_calls": [
                    {
                        "name": tc.get("name"),
                        "id": tc.get("id"),
                        "input_preview": str(tc.get("input", {}))[:500],
                    }
                    for tc in (decision.tool_calls or [])
                ],
                "tool_results": [],  # å°†åœ¨å·¥å…·æ‰§è¡Œåå¡«å……
                "tokens": {
                    "input": _in_tokens,
                    "output": _out_tokens,
                },
                "context_compressed": _ctx_compressed_info,
            }
            tool_names_for_log = [tc.get("name", "?") for tc in (decision.tool_calls or [])]
            logger.info(
                f"[ReAct] Iter {iteration+1} â€” decision={_iter_trace['decision_type']}, "
                f"tools={tool_names_for_log}, "
                f"tokens_in={_in_tokens}, tokens_out={_out_tokens}"
            )

            # ==================== stop_reason=max_tokens æ£€æµ‹ ====================
            # å½“ LLM è¾“å‡ºè¢« max_tokens é™åˆ¶æˆªæ–­æ—¶ï¼Œå·¥å…·è°ƒç”¨çš„ JSON å¯èƒ½ä¸å®Œæ•´ã€‚
            # æ£€æµ‹æ­¤æƒ…å†µå¹¶è®°å½•æ˜ç¡®è­¦å‘Šï¼Œå¸®åŠ©æ’æŸ¥ã€‚
            if decision.stop_reason == "max_tokens":
                logger.warning(
                    f"[ReAct] Iter {iteration+1} â€” âš ï¸ LLM output truncated (stop_reason=max_tokens). "
                    f"The response hit the max_tokens limit ({self._brain.max_tokens}). "
                    f"Tool calls may have incomplete JSON arguments. "
                    f"Consider increasing endpoint max_tokens or reducing tool argument size."
                )
                _iter_trace["truncated"] = True

            # ==================== å†³ç­–åˆ†æ”¯ ====================

            if decision.type == DecisionType.FINAL_ANSWER:
                # çº¯æ–‡æœ¬å“åº” - å¤„ç†å®Œæˆåº¦éªŒè¯
                answer_preview = (decision.text_content or "")[:80].replace("\n", " ")
                logger.info(f"[ReAct] Iter {iteration+1} â€” FINAL_ANSWER: \"{answer_preview}...\"")
                consecutive_tool_rounds = 0

                result = await self._handle_final_answer(
                    decision=decision,
                    working_messages=working_messages,
                    original_messages=messages,
                    tools_executed_in_task=tools_executed_in_task,
                    executed_tool_names=executed_tool_names,
                    delivery_receipts=delivery_receipts,
                    no_tool_call_count=no_tool_call_count,
                    verify_incomplete_count=verify_incomplete_count,
                    no_confirmation_text_count=no_confirmation_text_count,
                    max_no_tool_retries=max_no_tool_retries,
                    max_verify_retries=max_verify_retries,
                    max_confirmation_text_retries=max_confirmation_text_retries,
                    base_force_retries=base_force_retries,
                    conversation_id=conversation_id,
                )

                if isinstance(result, str):
                    # æœ€ç»ˆç­”æ¡ˆ
                    react_trace.append(_iter_trace)
                    logger.info(
                        f"[ReAct] === COMPLETED after {iteration+1} iterations, "
                        f"tools: {list(set(executed_tool_names))} ==="
                    )
                    self._save_react_trace(react_trace, conversation_id, session_type, "completed", _trace_started_at)
                    state.transition(TaskStatus.COMPLETED)
                    tracer.end_trace(metadata={
                        "result": "completed",
                        "iterations": iteration + 1,
                        "tools_used": list(set(executed_tool_names)),
                    })
                    return result
                else:
                    # éœ€è¦ç»§ç»­å¾ªç¯ï¼ˆéªŒè¯ä¸é€šè¿‡ï¼‰
                    await _emit_progress("ğŸ”„ ä»»åŠ¡å°šæœªå®Œæˆï¼Œç»§ç»­å¤„ç†...")
                    logger.info(f"[ReAct] Iter {iteration+1} â€” VERIFY: incomplete, continuing loop")
                    react_trace.append(_iter_trace)
                    state.transition(TaskStatus.VERIFYING)
                    (
                        working_messages,
                        no_tool_call_count,
                        verify_incomplete_count,
                        no_confirmation_text_count,
                        max_no_tool_retries,
                    ) = result
                    continue

            elif decision.type == DecisionType.TOOL_CALLS:
                # ==================== ACT é˜¶æ®µ ====================
                tool_names = [tc.get("name", "?") for tc in decision.tool_calls]
                logger.info(f"[ReAct] Iter {iteration+1} â€” ACT: {tool_names}")
                state.transition(TaskStatus.ACTING)

                # ---- ask_user æ‹¦æˆª ----
                # å¦‚æœ LLM è°ƒç”¨äº† ask_userï¼Œç«‹å³ä¸­æ–­å¾ªç¯ï¼Œå°†é—®é¢˜è¿”å›ç»™ç”¨æˆ·
                ask_user_calls = [tc for tc in decision.tool_calls if tc.get("name") == "ask_user"]
                other_calls = [tc for tc in decision.tool_calls if tc.get("name") != "ask_user"]

                if ask_user_calls:
                    logger.info(
                        f"[ReAct] Iter {iteration+1} â€” ask_user intercepted, "
                        f"pausing for user input (other_tools={[tc.get('name') for tc in other_calls]})"
                    )

                    # æ·»åŠ  assistant æ¶ˆæ¯ï¼ˆä¿ç•™å®Œæ•´çš„ tool_use å†…å®¹ç”¨äºä¸Šä¸‹æ–‡è¿è´¯ï¼‰
                    working_messages.append({
                        "role": "assistant",
                        "content": decision.assistant_content,
                    })

                    # å¦‚æœåŒæ—¶è¿˜æœ‰å…¶ä»–å·¥å…·è°ƒç”¨ï¼Œå…ˆæ‰§è¡Œå®ƒä»¬
                    # æ”¶é›†å…¶ä»–å·¥å…·çš„ tool_resultï¼ˆClaude API è¦æ±‚æ¯ä¸ª tool_use éƒ½æœ‰å¯¹åº” tool_resultï¼‰
                    other_tool_results: list[dict] = []
                    if other_calls:
                        other_results, other_executed, other_receipts = (
                            await self._tool_executor.execute_batch(
                                other_calls,
                                state=state,
                                task_monitor=task_monitor,
                                allow_interrupt_checks=self._state.interrupt_enabled,
                                capture_delivery_receipts=True,
                            )
                        )
                        if other_executed:
                            tools_executed_in_task = True
                            executed_tool_names.extend(other_executed)
                            state.record_tool_execution(other_executed)
                        # ä¿ç•™å…¶ä»–å·¥å…·çš„ tool_result å†…å®¹
                        other_tool_results = other_results if other_results else []

                    # æå– ask_user çš„é—®é¢˜æ–‡æœ¬
                    question = ask_user_calls[0].get("input", {}).get("question", "")
                    ask_tool_id = ask_user_calls[0].get("id", "ask_user_0")

                    # åˆå¹¶ LLM çš„æ–‡æœ¬å›å¤ + é—®é¢˜
                    text_part = strip_thinking_tags(decision.text_content or "").strip()
                    if text_part and question:
                        final_text = f"{text_part}\n\n{question}"
                    elif question:
                        final_text = question
                    else:
                        final_text = text_part or "ï¼ˆç­‰å¾…ç”¨æˆ·å›å¤ï¼‰"

                    state.transition(TaskStatus.WAITING_USER)

                    # ---- IM æ¨¡å¼ï¼šç­‰å¾…ç”¨æˆ·å›å¤ï¼ˆè¶…æ—¶ + è¿½é—®ï¼‰ ----
                    user_reply = await self._wait_for_user_reply(
                        final_text, state, timeout_seconds=60, max_reminders=1,
                    )

                    # æ„å»º tool_result æ¶ˆæ¯ï¼ˆå…¶ä»–å·¥å…·ç»“æœ + ask_user ç»“æœå¿…é¡»åœ¨åŒä¸€æ¡ user æ¶ˆæ¯ä¸­ï¼‰
                    def _build_ask_user_tool_results(
                        ask_user_content: str,
                        _other_results: list[dict] = other_tool_results,
                        _ask_id: str = ask_tool_id,
                    ) -> list[dict]:
                        """æ„å»ºåŒ…å«æ‰€æœ‰ tool_result çš„ user æ¶ˆæ¯ content"""
                        results = list(_other_results)  # å…¶ä»–å·¥å…·çš„ tool_result
                        results.append({
                            "type": "tool_result",
                            "tool_use_id": _ask_id,
                            "content": ask_user_content,
                        })
                        return results

                    if user_reply:
                        # ç”¨æˆ·åœ¨è¶…æ—¶å†…å›å¤äº† â†’ æ³¨å…¥å›å¤ï¼Œç»§ç»­ ReAct å¾ªç¯
                        logger.info(
                            f"[ReAct] Iter {iteration+1} â€” ask_user: user replied, resuming loop"
                        )
                        react_trace.append(_iter_trace)
                        working_messages.append({
                            "role": "user",
                            "content": _build_ask_user_tool_results(f"ç”¨æˆ·å›å¤ï¼š{user_reply}"),
                        })
                        state.transition(TaskStatus.REASONING)
                        continue  # ç»§ç»­ ReAct å¾ªç¯

                    elif user_reply is None and self._state.current_session and (
                        self._state.current_session.get_metadata("_gateway")
                        if hasattr(self._state.current_session, "get_metadata")
                        else None
                    ):
                        # IM æ¨¡å¼ï¼Œç”¨æˆ·è¶…æ—¶æœªå›å¤ â†’ æ³¨å…¥ç³»ç»Ÿæç¤ºè®© LLM è‡ªè¡Œå†³ç­–
                        logger.info(
                            f"[ReAct] Iter {iteration+1} â€” ask_user: user timeout, "
                            f"injecting auto-decide prompt"
                        )
                        react_trace.append(_iter_trace)
                        working_messages.append({
                            "role": "user",
                            "content": _build_ask_user_tool_results(
                                "[ç³»ç»Ÿ] ç”¨æˆ· 2 åˆ†é’Ÿå†…æœªå›å¤ä½ çš„æé—®ã€‚"
                                "è¯·è‡ªè¡Œå†³ç­–ï¼šå¦‚æœèƒ½åˆç†æ¨æ–­ç”¨æˆ·æ„å›¾ï¼Œç»§ç»­æ‰§è¡Œä»»åŠ¡ï¼›"
                                "å¦åˆ™ç»ˆæ­¢å½“å‰ä»»åŠ¡å¹¶å‘ŠçŸ¥ç”¨æˆ·ä½ éœ€è¦ä»€ä¹ˆä¿¡æ¯ã€‚"
                            ),
                        })
                        state.transition(TaskStatus.REASONING)
                        continue  # ç»§ç»­ ReAct å¾ªç¯ï¼Œè®© LLM è‡ªè¡Œå†³ç­–

                    else:
                        # CLI æ¨¡å¼æˆ–æ—  gateway â†’ ç›´æ¥è¿”å›é—®é¢˜æ–‡æœ¬
                        tracer.end_trace(metadata={
                            "result": "waiting_user",
                            "iterations": iteration + 1,
                            "tools_used": list(set(executed_tool_names)),
                        })
                        react_trace.append(_iter_trace)
                        self._save_react_trace(react_trace, conversation_id, session_type, "waiting_user", _trace_started_at)
                        logger.info(
                            f"[ReAct] === WAITING_USER (CLI) after {iteration+1} iterations ==="
                        )
                        return final_text

                # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåœ¨å·¥å…·æ‰§è¡Œå‰ï¼‰
                self._save_checkpoint(working_messages, state, decision, iteration)

                # æ·»åŠ  assistant æ¶ˆæ¯
                working_messages.append({
                    "role": "assistant",
                    "content": decision.assistant_content,
                })

                # æ£€æŸ¥å–æ¶ˆ
                if state.cancelled:
                    react_trace.append(_iter_trace)
                    self._save_react_trace(react_trace, conversation_id, session_type, "cancelled", _trace_started_at)
                    tracer.end_trace(metadata={"result": "cancelled", "iterations": iteration + 1})
                    return "âœ… ä»»åŠ¡å·²åœæ­¢ã€‚"

                # === IM è¿›åº¦: æè¿°å³å°†æ‰§è¡Œçš„å·¥å…· ===
                for tc in (decision.tool_calls or []):
                    _tc_name = tc.get("name", "unknown")
                    _tc_args = tc.get("input", tc.get("arguments", {}))
                    await _emit_progress(f"ğŸ”§ {self._describe_tool_call(_tc_name, _tc_args)}")

                # æ‰§è¡Œå·¥å…·
                tool_results, executed, receipts = await self._tool_executor.execute_batch(
                    decision.tool_calls,
                    state=state,
                    task_monitor=task_monitor,
                    allow_interrupt_checks=self._state.interrupt_enabled,
                    capture_delivery_receipts=True,
                )

                if executed:
                    tools_executed_in_task = True
                    executed_tool_names.extend(executed)
                    state.record_tool_execution(executed)

                    # è®°å½•å·¥å…·æˆåŠŸ/å¤±è´¥çŠ¶æ€ + IM è¿›åº¦
                    for i, tool_name in enumerate(executed):
                        result_content = ""
                        if i < len(tool_results):
                            r = tool_results[i]
                            result_content = str(r.get("content", "")) if isinstance(r, dict) else str(r)
                        is_error = any(m in result_content for m in ["âŒ", "âš ï¸ å·¥å…·æ‰§è¡Œé”™è¯¯", "é”™è¯¯ç±»å‹:"])
                        self._record_tool_result(tool_name, success=not is_error)
                        # IM è¿›åº¦: å·¥å…·ç»“æœæ‘˜è¦
                        _r_summary = self._summarize_tool_result(tool_name, result_content)
                        if _r_summary:
                            _icon = "âŒ" if is_error else "âœ…"
                            await _emit_progress(f"{_icon} {_r_summary}")

                if receipts:
                    delivery_receipts = receipts

                # ==================== OBSERVE é˜¶æ®µ ====================
                logger.info(
                    f"[ReAct] Iter {iteration+1} â€” OBSERVE: "
                    f"{len(tool_results)} results from {executed or []}"
                )
                state.transition(TaskStatus.OBSERVING)

                # æ”¶é›†å·¥å…·ç»“æœåˆ° trace
                _iter_trace["tool_results"] = [
                    {
                        "tool_use_id": tr.get("tool_use_id", ""),
                        "result_preview": str(tr.get("content", ""))[:1000],
                    }
                    for tr in tool_results
                    if isinstance(tr, dict)
                ]
                for tr in tool_results:
                    if isinstance(tr, dict):
                        t_id = tr.get("tool_use_id", "")
                        r_len = len(str(tr.get("content", "")))
                        logger.info(f"[ReAct] Iter {iteration+1} â€” tool_result id={t_id} len={r_len}")
                react_trace.append(_iter_trace)

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å›æ»š
                should_rb, rb_reason = self._should_rollback(tool_results)
                if should_rb:
                    rollback_result = self._rollback(rb_reason)
                    if rollback_result:
                        working_messages, _ = rollback_result
                        logger.info("[Rollback] å›æ»šæˆåŠŸï¼Œå°†ç”¨ä¸åŒæ–¹æ³•é‡æ–°æ¨ç†")
                        continue

                if state.cancelled:
                    self._save_react_trace(react_trace, conversation_id, session_type, "cancelled", _trace_started_at)
                    tracer.end_trace(metadata={"result": "cancelled", "iterations": iteration + 1})
                    return "âœ… ä»»åŠ¡å·²åœæ­¢ã€‚"

                # æ·»åŠ å·¥å…·ç»“æœ
                working_messages.append({
                    "role": "user",
                    "content": tool_results,
                })

                # å¾ªç¯æ£€æµ‹
                consecutive_tool_rounds += 1

                # stop_reason æ£€æŸ¥
                if decision.stop_reason == "end_turn":
                    cleaned_text = strip_thinking_tags(decision.text_content)
                    if cleaned_text and cleaned_text.strip():
                        logger.info(f"[LoopGuard] stop_reason=end_turn after {consecutive_tool_rounds} rounds")
                        self._save_react_trace(react_trace, conversation_id, session_type, "completed_end_turn", _trace_started_at)
                        state.transition(TaskStatus.COMPLETED)
                        tracer.end_trace(metadata={
                            "result": "completed_end_turn",
                            "iterations": iteration + 1,
                            "tools_used": list(set(executed_tool_names)),
                        })
                        return cleaned_text

                # å·¥å…·ç­¾åå¾ªç¯æ£€æµ‹
                round_signatures = [_make_tool_signature(tc) for tc in decision.tool_calls]
                round_sig_str = "+".join(sorted(round_signatures))
                recent_tool_signatures.append(round_sig_str)
                if len(recent_tool_signatures) > tool_pattern_window:
                    recent_tool_signatures = recent_tool_signatures[-tool_pattern_window:]

                loop_result = self._detect_loops(
                    recent_tool_signatures,
                    consecutive_tool_rounds,
                    working_messages,
                    decision.text_content,
                    llm_self_check_interval,
                    extreme_safety_threshold,
                    conversation_id,
                )
                if loop_result == "terminate":
                    cleaned = strip_thinking_tags(decision.text_content)
                    self._save_react_trace(react_trace, conversation_id, session_type, "loop_terminated", _trace_started_at)
                    state.transition(TaskStatus.FAILED)
                    tracer.end_trace(metadata={"result": "loop_terminated", "iterations": iteration + 1})
                    return cleaned or "âš ï¸ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨é™·å…¥æ­»å¾ªç¯ï¼Œä»»åŠ¡å·²è‡ªåŠ¨ç»ˆæ­¢ã€‚è¯·é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"
                if loop_result == "disable_force":
                    max_no_tool_retries = 0

        self._save_react_trace(react_trace, conversation_id, session_type, "max_iterations", _trace_started_at)
        state.transition(TaskStatus.FAILED)
        tracer.end_trace(metadata={"result": "max_iterations", "iterations": max_iterations})
        return "å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œè¯·é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"

    # ==================== æµå¼è¾“å‡º (SSE) ====================

    async def reason_stream(
        self,
        messages: list[dict],
        *,
        tools: list[dict] | None = None,
        system_prompt: str = "",
        base_system_prompt: str = "",
        task_description: str = "",
        task_monitor: Any = None,
        session_type: str = "desktop",
        plan_mode: bool = False,
        endpoint_override: str | None = None,
        conversation_id: str | None = None,
        thinking_mode: str | None = None,
        thinking_depth: str | None = None,
    ):
        """
        æµå¼æ¨ç†å¾ªç¯ï¼Œä¸º HTTP API (SSE) è®¾è®¡ã€‚

        ä¸ run() ä¿æŒç‰¹æ€§å¯¹é½ï¼šTaskMonitorã€å¾ªç¯æ£€æµ‹ã€æ¨¡å‹åˆ‡æ¢ã€
        LLM é”™è¯¯é‡è¯•ã€ä»»åŠ¡å®Œæˆåº¦éªŒè¯ã€Rollback ç­‰ã€‚

        è°ƒç”¨æ–¹ï¼ˆå¦‚ Agent.chat_with_session_streamï¼‰éœ€ä¼ å…¥ tools å’Œ system_promptï¼Œ
        æ–°å¢å‚æ•°å‡ optionalï¼Œå‘åå…¼å®¹è€çš„è°ƒç”¨æ–¹å¼ã€‚

        Yields dict events:
        - {"type": "iteration_start", "iteration": N}
        - {"type": "context_compressed", "before_tokens": N, "after_tokens": M}
        - {"type": "thinking_start"} / {"type": "thinking_delta"} / {"type": "thinking_end"}
        - {"type": "text_delta", "content": "..."}
        - {"type": "tool_call_start"} / {"type": "tool_call_end"}
        - {"type": "plan_created"} / {"type": "plan_step_updated"}
        - {"type": "ask_user", "question": "..."}
        - {"type": "error", "message": "..."}
        - {"type": "done"}
        """
        tools = tools or []
        self._last_exit_reason = "normal"

        # åœ¨ try å¤–åˆå§‹åŒ–ï¼Œé¿å… except/finally å—ä¸­ UnboundLocalError
        react_trace: list[dict] = []
        _trace_started_at = datetime.now().isoformat()
        _endpoint_switched = False

        # Task state
        state = self._state.current_task
        if not state or not state.is_active or state.cancelled:
            state = self._state.begin_task()
        elif state.status == TaskStatus.ACTING:
            logger.warning(
                f"[State] Previous task stuck in {state.status.value}, force resetting for new message"
            )
            state = self._state.begin_task()

        # å®‰å…¨æ ¡éªŒï¼šbegin_task è¿”å›çš„ state ä¸åº”æºå¸¦å–æ¶ˆæ ‡å¿—
        if state.cancelled:
            logger.error(
                f"[State] CRITICAL: fresh task {state.task_id[:8]} has cancelled=True, "
                f"reason={state.cancel_reason!r}. Force clearing."
            )
            state.cancelled = False
            state.cancel_reason = ""
            state.cancel_event = asyncio.Event()

        self._context_manager.set_cancel_event(state.cancel_event)

        try:
            # === åŠ¨æ€ System Promptï¼ˆè¿½åŠ æ´»è·ƒ Planï¼‰ ===
            _base_sp = base_system_prompt or system_prompt

            def _build_effective_prompt() -> str:
                try:
                    from ..tools.handlers.plan import get_active_plan_prompt
                    prompt = _base_sp
                    if conversation_id:
                        plan_section = get_active_plan_prompt(conversation_id)
                        if plan_section:
                            prompt += f"\n\n{plan_section}\n"
                    return prompt
                except Exception:
                    return _base_sp

            effective_prompt = _build_effective_prompt()
            if plan_mode:
                effective_prompt += (
                    "\n\n[PLAN MODE] ç”¨æˆ·è¯·æ±‚ Plan æ¨¡å¼ã€‚"
                    "è¯·å…ˆåˆ¶å®šè¯¦ç»†è®¡åˆ’ï¼ˆä½¿ç”¨ create_plan å·¥å…·ï¼‰ï¼Œç„¶åæŒ‰è®¡åˆ’æ‰§è¡Œã€‚"
                )

            # === ç«¯ç‚¹è¦†ç›– ===
            _endpoint_switched = False
            if endpoint_override:
                if not conversation_id:
                    conversation_id = f"_stream_{uuid.uuid4().hex[:12]}"
                llm_client = getattr(self._brain, "_llm_client", None)
                if llm_client and hasattr(llm_client, "switch_model"):
                    ok, msg = llm_client.switch_model(
                        endpoint_name=endpoint_override,
                        hours=0.05,
                        reason=f"chat endpoint override: {endpoint_override}",
                        conversation_id=conversation_id,
                    )
                    if not ok:
                        yield {"type": "error", "message": f"ç«¯ç‚¹åˆ‡æ¢å¤±è´¥: {msg}"}
                        yield {"type": "done"}
                        return
                    _endpoint_switched = True

            current_model = self._brain.model
            if _endpoint_switched and endpoint_override:
                llm_client = getattr(self._brain, "_llm_client", None)
                if llm_client:
                    _provider = llm_client._providers.get(endpoint_override)
                    if _provider:
                        current_model = _provider.model

            # === ä¸ run() ä¸€è‡´çš„å¾ªç¯æ§åˆ¶å˜é‡ ===
            max_iterations = settings.max_iterations
            working_messages = list(messages)

            # ForceToolCall é…ç½®
            if session_type == "im":
                base_force_retries = 0
            else:
                base_force_retries = max(0, int(getattr(settings, "force_tool_call_max_retries", 1)))

            max_no_tool_retries = self._effective_force_retries(base_force_retries, conversation_id)
            max_verify_retries = 3
            max_confirmation_text_retries = 1

            executed_tool_names: list[str] = []
            delivery_receipts: list[dict] = []
            _last_browser_url = ""
            consecutive_tool_rounds = 0
            no_tool_call_count = 0
            verify_incomplete_count = 0
            no_confirmation_text_count = 0
            tools_executed_in_task = False

            # å¾ªç¯æ£€æµ‹
            recent_tool_signatures: list[str] = []
            tool_pattern_window = 8
            llm_self_check_interval = 10
            extreme_safety_threshold = 50

            def _make_tool_sig(tc: dict) -> str:
                nonlocal _last_browser_url
                name = tc.get("name", "")
                inp = tc.get("input", {})
                if name == "browser_navigate":
                    _last_browser_url = inp.get("url", "")
                try:
                    param_str = json.dumps(inp, sort_keys=True, ensure_ascii=False)
                except Exception:
                    param_str = str(inp)
                if name in self._browser_page_read_tools and len(param_str) <= 20 and _last_browser_url:
                    param_str = f"{param_str}|url={_last_browser_url}"
                param_hash = hashlib.md5(param_str.encode()).hexdigest()[:8]
                return f"{name}({param_hash})"

            # ==================== ä¸»å¾ªç¯ ====================
            logger.info(
                f"[ReAct-Stream] === Loop started (max_iterations={max_iterations}, model={current_model}) ==="
            )

            for _iteration in range(max_iterations):
                state.iteration = _iteration

                # --- å–æ¶ˆæ£€æŸ¥ ---
                if state.cancelled:
                    logger.info(f"[ReAct-Stream] Task cancelled at iteration start: {state.cancel_reason}")
                    self._save_react_trace(react_trace, conversation_id, session_type, "cancelled", _trace_started_at)
                    yield {"type": "text_delta", "content": "âœ… ä»»åŠ¡å·²åœæ­¢ã€‚"}
                    yield {"type": "done"}
                    return

                # --- TaskMonitor: è¿­ä»£å¼€å§‹ + æ¨¡å‹åˆ‡æ¢æ£€æŸ¥ ---
                if task_monitor:
                    task_monitor.begin_iteration(_iteration + 1, current_model)
                    switch_result = self._check_model_switch(
                        task_monitor, state, working_messages, current_model
                    )
                    if switch_result:
                        current_model, working_messages = switch_result
                        no_tool_call_count = 0
                        tools_executed_in_task = False
                        verify_incomplete_count = 0
                        executed_tool_names = []
                        consecutive_tool_rounds = 0
                        recent_tool_signatures = []
                        no_confirmation_text_count = 0

                logger.info(
                    f"[ReAct-Stream] Iter {_iteration+1}/{max_iterations} â€” REASON (model={current_model})"
                )

                # --- çŠ¶æ€è½¬æ¢: REASONINGï¼ˆä¸ run() ä¸€è‡´ï¼‰ ---
                if state.status != TaskStatus.REASONING:
                    state.transition(TaskStatus.REASONING)

                _ctx_compressed_info: dict | None = None
                if len(working_messages) > 2:
                    effective_prompt = _build_effective_prompt()
                    _before_tokens = self._context_manager.estimate_messages_tokens(working_messages)
                    try:
                        working_messages = await self._context_manager.compress_if_needed(
                            working_messages,
                            system_prompt=effective_prompt,
                            tools=tools,
                        )
                    except _CtxCancelledError:
                        async for ev in self._stream_cancel_farewell(
                            working_messages, effective_prompt, current_model, state
                        ):
                            yield ev
                        yield {"type": "done"}
                        return
                    _after_tokens = self._context_manager.estimate_messages_tokens(working_messages)
                    if _after_tokens < _before_tokens:
                        _ctx_compressed_info = {
                            "before_tokens": _before_tokens,
                            "after_tokens": _after_tokens,
                        }
                        logger.info(
                            f"[ReAct-Stream] Context compressed: {_before_tokens} â†’ {_after_tokens} tokens"
                        )
                        yield {
                            "type": "context_compressed",
                            "before_tokens": _before_tokens,
                            "after_tokens": _after_tokens,
                        }

                # --- æ€ç»´é“¾: è¿­ä»£å¼€å§‹äº‹ä»¶ ---
                yield {"type": "iteration_start", "iteration": _iteration + 1}

                # --- Reason phase ---
                _thinking_t0 = time.time()
                yield {"type": "thinking_start"}

                try:
                    decision = None
                    async for hb_event in self._reason_with_heartbeat(
                        working_messages,
                        system_prompt=effective_prompt,
                        tools=tools,
                        current_model=current_model,
                        conversation_id=conversation_id,
                        thinking_mode=thinking_mode,
                        thinking_depth=thinking_depth,
                    ):
                        if hb_event["type"] == "heartbeat":
                            yield {"type": "heartbeat"}
                        elif hb_event["type"] == "decision":
                            decision = hb_event["decision"]
                    if decision is None:
                        raise RuntimeError("_reason returned no decision")

                    if task_monitor:
                        task_monitor.reset_retry_count()

                except UserCancelledError as uce:
                    # --- ç”¨æˆ·å–æ¶ˆä¸­æ–­ï¼šå‘èµ·è½»é‡ LLM æ”¶å°¾ ---
                    logger.info(f"[ReAct-Stream] LLM call interrupted by user cancel: {uce.reason}")
                    _thinking_duration = int((time.time() - _thinking_t0) * 1000)
                    yield {"type": "thinking_end", "duration_ms": _thinking_duration}

                    self._save_react_trace(
                        react_trace, conversation_id, session_type, "cancelled", _trace_started_at
                    )
                    # æµå¼è¾“å‡ºæ”¶å°¾
                    async for ev in self._stream_cancel_farewell(
                        working_messages, effective_prompt, current_model, state
                    ):
                        yield ev
                    yield {"type": "done"}
                    return

                except Exception as e:
                    # --- LLM Error Handlingï¼ˆä¸ run() ä¸€è‡´ï¼‰ ---
                    retry_result = self._handle_llm_error(
                        e, task_monitor, state, working_messages, current_model
                    )
                    _thinking_duration = int((time.time() - _thinking_t0) * 1000)
                    yield {"type": "thinking_end", "duration_ms": _thinking_duration}

                    if retry_result == "retry":
                        _sleep = asyncio.create_task(asyncio.sleep(2))
                        _cw = asyncio.create_task(cancel_event.wait())
                        _done, _pend = await asyncio.wait({_sleep, _cw}, return_when=asyncio.FIRST_COMPLETED)
                        for _t in _pend:
                            _t.cancel()
                            try:
                                await _t
                            except (asyncio.CancelledError, Exception):
                                pass
                        if _cw in _done:
                            async for ev in self._stream_cancel_farewell(
                                working_messages, effective_prompt, current_model, state
                            ):
                                yield ev
                            yield {"type": "done"}
                            return
                        continue
                    elif isinstance(retry_result, tuple):
                        current_model, working_messages = retry_result
                        no_tool_call_count = 0
                        tools_executed_in_task = False
                        verify_incomplete_count = 0
                        executed_tool_names = []
                        consecutive_tool_rounds = 0
                        recent_tool_signatures = []
                        no_confirmation_text_count = 0
                        continue
                    else:
                        self._save_react_trace(
                            react_trace, conversation_id, session_type,
                            f"reason_error: {str(e)[:100]}", _trace_started_at,
                        )
                        yield {"type": "error", "message": f"æ¨ç†å¤±è´¥: {str(e)[:300]}"}
                        yield {"type": "done"}
                        return

                # Emit thinking content
                _thinking_duration = int((time.time() - _thinking_t0) * 1000)
                _has_thinking = bool(decision.thinking_content)
                if _has_thinking:
                    yield {"type": "thinking_delta", "content": decision.thinking_content}
                yield {
                    "type": "thinking_end",
                    "duration_ms": _thinking_duration,
                    "has_thinking": _has_thinking,
                }

                # === chain_text: LLM æ¨ç†æ„å›¾ï¼ˆtext_content ä¸­å¯èƒ½å«æ¨ç†è¯´æ˜ï¼‰ ===
                _decision_text = (decision.text_content or "").strip()
                if _decision_text and decision.type == DecisionType.TOOL_CALLS:
                    # LLM åœ¨è°ƒç”¨å·¥å…·å‰è¾“å‡ºçš„æ€è·¯æ–‡å­—
                    yield {"type": "chain_text", "content": _decision_text[:2000]}

                if task_monitor:
                    task_monitor.end_iteration(decision.text_content or "")

                # -- æ”¶é›† ReAct trace --
                _raw = decision.raw_response
                _usage = getattr(_raw, "usage", None) if _raw else None
                _in_tokens = getattr(_usage, "input_tokens", 0) if _usage else 0
                _out_tokens = getattr(_usage, "output_tokens", 0) if _usage else 0
                _iter_trace: dict = {
                    "iteration": _iteration + 1,
                    "timestamp": datetime.now().isoformat(),
                    "decision_type": decision.type.value if hasattr(decision.type, "value") else str(decision.type),
                    "model": current_model,
                    "thinking": (decision.thinking_content or "")[:500] if decision.thinking_content else None,
                    "thinking_duration_ms": _thinking_duration,
                    "text": (decision.text_content or "")[:2000] if decision.text_content else None,
                    "tool_calls": [
                        {
                            "name": tc.get("name"),
                            "id": tc.get("id"),
                            "input_preview": str(tc.get("input", {}))[:500],
                        }
                        for tc in (decision.tool_calls or [])
                    ],
                    "tool_results": [],
                    "tokens": {"input": _in_tokens, "output": _out_tokens},
                    "context_compressed": _ctx_compressed_info,
                }
                tool_names_log = [tc.get("name", "?") for tc in (decision.tool_calls or [])]
                logger.info(
                    f"[ReAct-Stream] Iter {_iteration+1} â€” decision={_iter_trace['decision_type']}, "
                    f"tools={tool_names_log}, tokens_in={_in_tokens}, tokens_out={_out_tokens}"
                )

                # ==================== stop_reason=max_tokens æ£€æµ‹ï¼ˆä¸ run() ä¸€è‡´ï¼‰====================
                if decision.stop_reason == "max_tokens":
                    logger.warning(
                        f"[ReAct-Stream] Iter {_iteration+1} â€” âš ï¸ LLM output truncated (stop_reason=max_tokens). "
                        f"The response hit the max_tokens limit ({self._brain.max_tokens}). "
                        f"Tool calls may have incomplete JSON arguments."
                    )
                    _iter_trace["truncated"] = True

                # ==================== FINAL_ANSWER ====================
                if decision.type == DecisionType.FINAL_ANSWER:
                    consecutive_tool_rounds = 0

                    # ä»»åŠ¡å®Œæˆåº¦éªŒè¯ï¼ˆä¸ run() ä¸€è‡´ï¼‰
                    result = await self._handle_final_answer(
                        decision=decision,
                        working_messages=working_messages,
                        original_messages=messages,
                        tools_executed_in_task=tools_executed_in_task,
                        executed_tool_names=executed_tool_names,
                        delivery_receipts=delivery_receipts,
                        no_tool_call_count=no_tool_call_count,
                        verify_incomplete_count=verify_incomplete_count,
                        no_confirmation_text_count=no_confirmation_text_count,
                        max_no_tool_retries=max_no_tool_retries,
                        max_verify_retries=max_verify_retries,
                        max_confirmation_text_retries=max_confirmation_text_retries,
                        base_force_retries=base_force_retries,
                        conversation_id=conversation_id,
                    )

                    if isinstance(result, str):
                        # æœ€ç»ˆç­”æ¡ˆ â†’ stream ç»™å‰ç«¯
                        react_trace.append(_iter_trace)
                        self._save_react_trace(
                            react_trace, conversation_id, session_type, "completed", _trace_started_at
                        )
                        try:
                            state.transition(TaskStatus.COMPLETED)
                        except ValueError:
                            state.status = TaskStatus.COMPLETED
                        logger.info(
                            f"[ReAct-Stream] === COMPLETED after {_iteration+1} iterations ==="
                        )
                        chunk_size = 20
                        for i in range(0, len(result), chunk_size):
                            yield {"type": "text_delta", "content": result[i:i + chunk_size]}
                            await asyncio.sleep(0.01)
                        yield {"type": "done"}
                        return
                    else:
                        # éªŒè¯ä¸é€šè¿‡ â†’ ç»§ç»­å¾ªç¯
                        logger.info(
                            f"[ReAct-Stream] Iter {_iteration+1} â€” VERIFY: incomplete, continuing loop"
                        )
                        yield {"type": "chain_text", "content": "ä»»åŠ¡å°šæœªå®Œæˆï¼Œç»§ç»­å¤„ç†..."}
                        react_trace.append(_iter_trace)
                        try:
                            state.transition(TaskStatus.VERIFYING)
                        except ValueError:
                            state.status = TaskStatus.VERIFYING
                        (
                            working_messages,
                            no_tool_call_count,
                            verify_incomplete_count,
                            no_confirmation_text_count,
                            max_no_tool_retries,
                        ) = result
                        continue

                # ==================== TOOL_CALLS ====================
                elif decision.type == DecisionType.TOOL_CALLS and decision.tool_calls:
                    try:
                        state.transition(TaskStatus.ACTING)
                    except ValueError:
                        state.status = TaskStatus.ACTING

                    working_messages.append({
                        "role": "assistant",
                        "content": decision.assistant_content or [{"type": "text", "text": ""}],
                    })

                    # ---- ask_user æ‹¦æˆª ----
                    ask_user_calls = [tc for tc in decision.tool_calls if tc.get("name") == "ask_user"]
                    other_tool_calls = [tc for tc in decision.tool_calls if tc.get("name") != "ask_user"]

                    if ask_user_calls:
                        # å…ˆæ‰§è¡Œé ask_user å·¥å…·
                        tool_results_for_msg: list[dict] = []
                        for tc in other_tool_calls:
                            t_name = tc.get("name", "unknown")
                            t_args = tc.get("input", tc.get("arguments", {}))
                            t_id = tc.get("id", str(uuid.uuid4()))
                            # chain_text: å·¥å…·æè¿°
                            yield {"type": "chain_text", "content": self._describe_tool_call(t_name, t_args)}
                            yield {"type": "tool_call_start", "tool": t_name, "args": t_args, "id": t_id}
                            try:
                                r = await self._tool_executor.execute_tool(
                                    tool_name=t_name,
                                    tool_input=t_args if isinstance(t_args, dict) else {},
                                    session_id=conversation_id,
                                )
                                r = str(r) if r else ""
                            except Exception as exc:
                                r = f"Tool error: {exc}"
                            yield {"type": "tool_call_end", "tool": t_name, "result": r[:8000], "id": t_id}
                            # chain_text: ç»“æœæ‘˜è¦
                            _ask_result_summary = self._summarize_tool_result(t_name, r)
                            if _ask_result_summary:
                                yield {"type": "chain_text", "content": _ask_result_summary}
                            tool_results_for_msg.append({
                                "type": "tool_result", "tool_use_id": t_id, "content": r,
                            })

                        # ask_user äº‹ä»¶
                        ask_input = ask_user_calls[0].get("input", {})
                        ask_q = ask_input.get("question", "")
                        ask_options = ask_input.get("options")
                        ask_allow_multiple = ask_input.get("allow_multiple", False)
                        ask_questions = ask_input.get("questions")
                        text_part = decision.text_content or ""
                        question_text = f"{text_part}\n\n{ask_q}".strip() if text_part else ask_q
                        event: dict = {
                            "type": "ask_user",
                            "question": question_text,
                            "conversation_id": conversation_id,
                        }
                        if ask_options and isinstance(ask_options, list):
                            event["options"] = [
                                {"id": str(o.get("id", "")), "label": str(o.get("label", ""))}
                                for o in ask_options
                                if isinstance(o, dict) and o.get("id") and o.get("label")
                            ]
                        if ask_allow_multiple:
                            event["allow_multiple"] = True
                        if ask_questions and isinstance(ask_questions, list):
                            parsed_questions = []
                            for q in ask_questions:
                                if not isinstance(q, dict) or not q.get("id") or not q.get("prompt"):
                                    continue
                                pq: dict = {"id": str(q["id"]), "prompt": str(q["prompt"])}
                                q_options = q.get("options")
                                if q_options and isinstance(q_options, list):
                                    pq["options"] = [
                                        {"id": str(o.get("id", "")), "label": str(o.get("label", ""))}
                                        for o in q_options
                                        if isinstance(o, dict) and o.get("id") and o.get("label")
                                    ]
                                if q.get("allow_multiple"):
                                    pq["allow_multiple"] = True
                                parsed_questions.append(pq)
                            if parsed_questions:
                                event["questions"] = parsed_questions
                        yield event
                        react_trace.append(_iter_trace)
                        self._save_react_trace(
                            react_trace, conversation_id, session_type, "ask_user", _trace_started_at
                        )
                        self._last_exit_reason = "ask_user"
                        yield {"type": "done"}
                        return

                    # ---- æ­£å¸¸å·¥å…·æ‰§è¡Œï¼ˆæ”¯æŒ cancel_event / skip_event ä¸‰è·¯ç«é€Ÿä¸­æ–­ï¼‰ ----
                    tool_results_for_msg: list[dict] = []
                    _stream_cancelled = False
                    _stream_skipped = False
                    cancel_event = state.cancel_event if state else asyncio.Event()
                    skip_event = state.skip_event if state else asyncio.Event()
                    for tc in decision.tool_calls:
                        # æ¯ä¸ªå·¥å…·æ‰§è¡Œå‰æ£€æŸ¥å–æ¶ˆ
                        if state and state.cancelled:
                            _stream_cancelled = True
                            break

                        tool_name = tc.get("name", "unknown")
                        tool_args = tc.get("input", tc.get("arguments", {}))
                        tool_id = tc.get("id", str(uuid.uuid4()))

                        _tool_desc = self._describe_tool_call(tool_name, tool_args)
                        yield {"type": "chain_text", "content": _tool_desc}

                        yield {"type": "tool_call_start", "tool": tool_name, "args": tool_args, "id": tool_id}

                        # å°†å·¥å…·æ‰§è¡Œä¸ cancel_event / skip_event ä¸‰è·¯ç«é€Ÿ
                        # æ³¨æ„: ä¸åœ¨æ­¤å¤„ clear_skip()ï¼Œè®©å·²åˆ°è¾¾çš„ skip ä¿¡å·è‡ªç„¶è¢«ç«é€Ÿæ¶ˆè´¹
                        try:
                            tool_exec_task = asyncio.create_task(
                                self._tool_executor.execute_tool(
                                    tool_name=tool_name,
                                    tool_input=tool_args if isinstance(tool_args, dict) else {},
                                    session_id=conversation_id,
                                )
                            )
                            cancel_waiter = asyncio.create_task(cancel_event.wait())
                            skip_waiter = asyncio.create_task(skip_event.wait())

                            done_set, pending_set = await asyncio.wait(
                                {tool_exec_task, cancel_waiter, skip_waiter},
                                return_when=asyncio.FIRST_COMPLETED,
                            )

                            for t in pending_set:
                                t.cancel()
                                try:
                                    await t
                                except (asyncio.CancelledError, Exception):
                                    pass

                            if cancel_waiter in done_set and tool_exec_task not in done_set:
                                result_text = f"[å·¥å…· {tool_name} è¢«ç”¨æˆ·ä¸­æ–­]"
                                _stream_cancelled = True
                            elif skip_waiter in done_set and tool_exec_task not in done_set:
                                _skip_reason = state.skip_reason if state else "ç”¨æˆ·è¯·æ±‚è·³è¿‡"
                                if state:
                                    state.clear_skip()
                                result_text = f"[ç”¨æˆ·è·³è¿‡äº†æ­¤æ­¥éª¤: {_skip_reason}]"
                                _stream_skipped = True
                                logger.info(f"[SkipStep-Stream] Tool {tool_name} skipped: {_skip_reason}")
                            elif tool_exec_task in done_set:
                                result_text = tool_exec_task.result()
                                result_text = str(result_text) if result_text else ""
                            else:
                                result_text = f"[å·¥å…· {tool_name} è¢«ç”¨æˆ·ä¸­æ–­]"
                                _stream_cancelled = True
                        except Exception as exc:
                            result_text = f"Tool error: {exc}"

                        # è·³è¿‡æ—¶å‘é€ tool_call_skipped äº‹ä»¶é€šçŸ¥å‰ç«¯
                        if _stream_skipped:
                            yield {"type": "tool_call_end", "tool": tool_name, "result": result_text[:8000], "id": tool_id, "skipped": True}
                        else:
                            yield {"type": "tool_call_end", "tool": tool_name, "result": result_text[:8000], "id": tool_id}

                        if _stream_cancelled:
                            tool_results_for_msg.append({
                                "type": "tool_result",
                                "tool_use_id": tool_id,
                                "content": result_text,
                                "is_error": True,
                            })
                            break

                        if _stream_skipped:
                            tool_results_for_msg.append({
                                "type": "tool_result",
                                "tool_use_id": tool_id,
                                "content": result_text,
                            })
                            _stream_skipped = False
                            continue

                        # === chain_text: ç®€è¿°å·¥å…·è¿”å›ç»“æœ ===
                        _result_summary = self._summarize_tool_result(tool_name, result_text)
                        if _result_summary:
                            yield {"type": "chain_text", "content": _result_summary}

                        # deliver_artifacts å›æ‰§æ”¶é›†ï¼ˆä¸ run() ä¸€è‡´ï¼‰
                        if tool_name == "deliver_artifacts" and result_text:
                            try:
                                _receipts_data = json.loads(result_text)
                                if isinstance(_receipts_data, dict) and "receipts" in _receipts_data:
                                    delivery_receipts = _receipts_data["receipts"]
                            except (json.JSONDecodeError, TypeError):
                                pass

                        # Plan äº‹ä»¶
                        if tool_name == "create_plan" and isinstance(tool_args, dict):
                            raw_steps = tool_args.get("steps", [])
                            plan_steps = []
                            for idx, s in enumerate(raw_steps):
                                if isinstance(s, dict):
                                    plan_steps.append({
                                        "id": str(s.get("id", f"step_{idx + 1}")),
                                        "description": str(s.get("description", s.get("id", ""))),
                                        "status": "pending",
                                    })
                                else:
                                    plan_steps.append({"id": f"step_{idx + 1}", "description": str(s), "status": "pending"})
                            yield {"type": "plan_created", "plan": {
                                "id": str(uuid.uuid4()),
                                "taskSummary": tool_args.get("task_summary", ""),
                                "steps": plan_steps,
                                "status": "in_progress",
                            }}
                        elif tool_name == "update_plan_step" and isinstance(tool_args, dict):
                            step_id = tool_args.get("step_id", "")
                            yield {"type": "plan_step_updated", "stepId": step_id, "status": tool_args.get("status", "completed")}
                        elif tool_name == "complete_plan":
                            yield {"type": "plan_completed"}

                        tool_results_for_msg.append({
                            "type": "tool_result",
                            "tool_use_id": tool_id,
                            "content": result_text,
                        })

                    if decision.tool_calls:
                        tools_executed_in_task = True
                        _executed = [tc.get("name", "") for tc in decision.tool_calls]
                        executed_tool_names.extend(_executed)
                        state.record_tool_execution(_executed)

                        # è®°å½•å·¥å…·æˆåŠŸ/å¤±è´¥çŠ¶æ€ï¼ˆä¸ run() ä¸€è‡´ï¼‰
                        for i, t_name in enumerate(_executed):
                            r_content = ""
                            if i < len(tool_results_for_msg):
                                r_content = str(tool_results_for_msg[i].get("content", ""))
                            is_error = any(m in r_content for m in ["âŒ", "âš ï¸ å·¥å…·æ‰§è¡Œé”™è¯¯", "é”™è¯¯ç±»å‹:"])
                            self._record_tool_result(t_name, success=not is_error)

                    # æ”¶é›†å·¥å…·ç»“æœåˆ° trace
                    _iter_trace["tool_results"] = [
                        {
                            "tool_use_id": tr.get("tool_use_id", ""),
                            "result_preview": str(tr.get("content", ""))[:1000],
                        }
                        for tr in tool_results_for_msg
                    ]
                    react_trace.append(_iter_trace)

                    try:
                        state.transition(TaskStatus.OBSERVING)
                    except ValueError:
                        state.status = TaskStatus.OBSERVING

                    # --- Rollback æ£€æŸ¥ï¼ˆä¸ run() ä¸€è‡´ï¼‰ ---
                    should_rb, rb_reason = self._should_rollback(tool_results_for_msg)
                    if should_rb:
                        rollback_result = self._rollback(rb_reason)
                        if rollback_result:
                            working_messages, _ = rollback_result
                            logger.info("[ReAct-Stream][Rollback] å›æ»šæˆåŠŸï¼Œå°†ç”¨ä¸åŒæ–¹æ³•é‡æ–°æ¨ç†")
                            continue

                    # å–æ¶ˆæ£€æŸ¥ï¼ˆå‡çº§ä¸ºå¸¦ LLM æ”¶å°¾çš„å–æ¶ˆå¤„ç†ï¼‰
                    if state.cancelled or _stream_cancelled:
                        # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
                        working_messages.append({"role": "user", "content": tool_results_for_msg})
                        self._save_react_trace(
                            react_trace, conversation_id, session_type, "cancelled", _trace_started_at
                        )
                        async for ev in self._stream_cancel_farewell(
                            working_messages, effective_prompt, current_model, state
                        ):
                            yield ev
                        yield {"type": "done"}
                        return

                    working_messages.append({
                        "role": "user",
                        "content": tool_results_for_msg,
                    })

                    # === ç»Ÿä¸€å¤„ç† skip åæ€ + ç”¨æˆ·æ’å…¥æ¶ˆæ¯ ===
                    if state:
                        _msg_count_before = len(working_messages)
                        await state.process_post_tool_signals(working_messages)
                        for _new_msg in working_messages[_msg_count_before:]:
                            _content = _new_msg.get("content", "")
                            if "[ç³»ç»Ÿæç¤º-ç”¨æˆ·è·³è¿‡æ­¥éª¤]" in _content:
                                yield {"type": "chain_text", "content": f"ç”¨æˆ·è·³è¿‡äº†å½“å‰æ­¥éª¤"}
                            elif "[ç”¨æˆ·æ’å…¥æ¶ˆæ¯]" in _content:
                                _preview = _content.split("]")[1].split("\n")[0].strip() if "]" in _content else _content[:60]
                                yield {"type": "chain_text", "content": f"ç”¨æˆ·æ’å…¥æ¶ˆæ¯: {_preview[:60]}"}

                    # --- å¾ªç¯æ£€æµ‹ï¼ˆä¸ run() ä¸€è‡´ï¼‰ ---
                    consecutive_tool_rounds += 1

                    # stop_reason æ£€æŸ¥
                    if decision.stop_reason == "end_turn":
                        cleaned_text = strip_thinking_tags(decision.text_content)
                        if cleaned_text and cleaned_text.strip():
                            logger.info(
                                f"[ReAct-Stream][LoopGuard] stop_reason=end_turn after {consecutive_tool_rounds} rounds"
                            )
                            self._save_react_trace(
                                react_trace, conversation_id, session_type,
                                "completed_end_turn", _trace_started_at,
                            )
                            chunk_size = 20
                            for i in range(0, len(cleaned_text), chunk_size):
                                yield {"type": "text_delta", "content": cleaned_text[i:i + chunk_size]}
                                await asyncio.sleep(0.01)
                            yield {"type": "done"}
                            return

                    # å·¥å…·ç­¾åå¾ªç¯æ£€æµ‹
                    round_signatures = [_make_tool_sig(tc) for tc in decision.tool_calls]
                    round_sig_str = "+".join(sorted(round_signatures))
                    recent_tool_signatures.append(round_sig_str)
                    if len(recent_tool_signatures) > tool_pattern_window:
                        recent_tool_signatures = recent_tool_signatures[-tool_pattern_window:]

                    loop_result = self._detect_loops(
                        recent_tool_signatures,
                        consecutive_tool_rounds,
                        working_messages,
                        decision.text_content,
                        llm_self_check_interval,
                        extreme_safety_threshold,
                        conversation_id,
                    )
                    if loop_result == "terminate":
                        cleaned = strip_thinking_tags(decision.text_content)
                        self._save_react_trace(
                            react_trace, conversation_id, session_type,
                            "loop_terminated", _trace_started_at,
                        )
                        try:
                            state.transition(TaskStatus.FAILED)
                        except ValueError:
                            state.status = TaskStatus.FAILED
                        msg = cleaned or "âš ï¸ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨é™·å…¥æ­»å¾ªç¯ï¼Œä»»åŠ¡å·²è‡ªåŠ¨ç»ˆæ­¢ã€‚è¯·é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"
                        yield {"type": "text_delta", "content": msg}
                        yield {"type": "done"}
                        return
                    if loop_result == "disable_force":
                        max_no_tool_retries = 0

                    continue  # Next iteration

            # max_iterations
            self._save_react_trace(
                react_trace, conversation_id, session_type, "max_iterations", _trace_started_at
            )
            try:
                state.transition(TaskStatus.FAILED)
            except ValueError:
                state.status = TaskStatus.FAILED
            logger.info(f"[ReAct-Stream] === MAX_ITERATIONS reached ({max_iterations}) ===")
            yield {"type": "text_delta", "content": "\n\nï¼ˆå·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰"}
            yield {"type": "done"}

        except Exception as e:
            logger.error(f"reason_stream error: {e}", exc_info=True)
            self._save_react_trace(
                react_trace, conversation_id, session_type,
                f"error: {str(e)[:100]}", _trace_started_at,
            )
            yield {"type": "error", "message": str(e)[:500]}
            yield {"type": "done"}

        finally:
            # æ¸…ç† per-conversation endpoint override
            if _endpoint_switched and conversation_id:
                llm_client = getattr(self._brain, "_llm_client", None)
                if llm_client and hasattr(llm_client, "restore_default"):
                    try:
                        llm_client.restore_default(conversation_id=conversation_id)
                    except Exception:
                        pass

    # ==================== æ€ç»´é“¾å™äº‹è¾…åŠ© ====================

    @staticmethod
    def _describe_tool_call(tool_name: str, tool_args: dict) -> str:
        """ä¸ºå·¥å…·è°ƒç”¨ç”Ÿæˆäººç±»å¯è¯»çš„å™äº‹æè¿°ã€‚"""
        args = tool_args if isinstance(tool_args, dict) else {}
        match tool_name:
            case "read_file":
                path = args.get("path") or args.get("file") or ""
                fname = path.rsplit("/", 1)[-1].rsplit("\\", 1)[-1] if path else "æ–‡ä»¶"
                return f"æ­£åœ¨è¯»å– {fname}..."
            case "write_file":
                path = args.get("path") or ""
                fname = path.rsplit("/", 1)[-1].rsplit("\\", 1)[-1] if path else "æ–‡ä»¶"
                return f"æ­£åœ¨å†™å…¥ {fname}..."
            case "edit_file":
                path = args.get("path") or ""
                fname = path.rsplit("/", 1)[-1].rsplit("\\", 1)[-1] if path else "æ–‡ä»¶"
                return f"æ­£åœ¨ç¼–è¾‘ {fname}..."
            case "grep" | "search" | "ripgrep" | "search_files":
                pattern = str(args.get("pattern") or args.get("query") or "")[:50]
                return f'æœç´¢ "{pattern}"...'
            case "web_search":
                query = str(args.get("query") or "")[:50]
                return f'åœ¨ç½‘ä¸Šæœç´¢ "{query}"...'
            case "execute_code" | "run_code" | "run_command":
                cmd = str(args.get("command") or args.get("code") or "")[:60]
                return f"æ‰§è¡Œå‘½ä»¤: {cmd}..." if cmd else "æ‰§è¡Œä»£ç ..."
            case "browser_navigate":
                url = str(args.get("url") or "")[:60]
                return f"è®¿é—® {url}..."
            case "browser_screenshot":
                return "æˆªå–é¡µé¢æˆªå›¾..."
            case "create_plan":
                summary = str(args.get("task_summary") or "")[:40]
                return f"åˆ¶å®šè®¡åˆ’: {summary}..."
            case "update_plan_step":
                idx = args.get("step_index", "")
                status = args.get("status", "")
                return f"æ›´æ–°è®¡åˆ’æ­¥éª¤ {idx} â†’ {status}"
            case "switch_persona":
                preset = args.get("preset_name", "")
                return f"åˆ‡æ¢è§’è‰²: {preset}..."
            case "get_persona_profile":
                return "è·å–å½“å‰äººæ ¼é…ç½®..."
            case "ask_user":
                q = str(args.get("question") or "")[:40]
                return f'å‘ç”¨æˆ·æé—®: "{q}"...'
            case "list_files" | "list_dir":
                path = str(args.get("path") or args.get("directory") or ".")
                return f"åˆ—å‡ºç›®å½• {path}..."
            case "deliver_artifacts":
                return "äº¤ä»˜æ–‡ä»¶..."
            case _:
                params = ", ".join(f"{k}" for k in list(args.keys())[:3])
                return f"è°ƒç”¨ {tool_name}({params})..."

    @staticmethod
    def _summarize_tool_result(tool_name: str, result_text: str) -> str:
        """ä¸ºå·¥å…·ç»“æœç”Ÿæˆç®€çŸ­å™äº‹æ‘˜è¦ã€‚"""
        if not result_text:
            return ""
        r = result_text.strip()
        is_error = any(m in r[:200] for m in ["âŒ", "âš ï¸ å·¥å…·æ‰§è¡Œé”™è¯¯", "é”™è¯¯ç±»å‹:", "Tool error:"])
        if is_error:
            # æå–ç¬¬ä¸€è¡Œé”™è¯¯ä¿¡æ¯
            first_line = r.split("\n")[0][:120]
            return f"å‡ºé”™: {first_line}"
        r_len = len(r)
        match tool_name:
            case "read_file":
                lines = r.count("\n") + 1
                return f"å·²è¯»å– ({lines} è¡Œ, {r_len} å­—ç¬¦)"
            case "grep" | "search" | "ripgrep" | "search_files":
                matches = r.count("\n") + 1 if r else 0
                return f"æ‰¾åˆ° {matches} æ¡ç»“æœ" if matches > 0 else "æ— åŒ¹é…ç»“æœ"
            case "web_search":
                return f"æœç´¢å®Œæˆ ({r_len} å­—ç¬¦)"
            case "execute_code" | "run_code" | "run_command":
                lines = r.count("\n") + 1
                preview = r[:80].replace("\n", " ")
                return f"æ‰§è¡Œå®Œæˆ: {preview}{'...' if r_len > 80 else ''}"
            case "write_file" | "edit_file":
                return "å†™å…¥æˆåŠŸ" if "æˆåŠŸ" in r or "ok" in r.lower() or r_len < 100 else f"å®Œæˆ ({r_len} å­—ç¬¦)"
            case "browser_screenshot":
                return "æˆªå›¾å·²è·å–"
            case "switch_persona":
                return f"åˆ‡æ¢å®Œæˆ"
            case _:
                if r_len < 100:
                    return r[:100]
                return f"å®Œæˆ ({r_len} å­—ç¬¦)"

    # ==================== ReAct æ¨ç†é“¾ä¿å­˜ ====================

    def _save_react_trace(
        self,
        react_trace: list[dict],
        conversation_id: str | None,
        session_type: str,
        result: str,
        started_at: str,
    ) -> None:
        """
        ä¿å­˜å®Œæ•´çš„ ReAct æ¨ç†é“¾åˆ°æ–‡ä»¶ã€‚

        åŒæ—¶æš‚å­˜åˆ° self._last_react_trace ä¾› agent_handler è¯»å–ï¼ˆæ€ç»´é“¾åŠŸèƒ½ï¼‰ã€‚

        è·¯å¾„: data/react_traces/{date}/trace_{conversation_id}_{timestamp}.json
        """
        # æ€ç»´é“¾: æš‚å­˜ trace ä¾›å¤–éƒ¨è¯»å–ï¼ˆå³ä½¿ä¸ºç©ºä¹Ÿæ›´æ–°ï¼Œæ¸…é™¤æ—§æ•°æ®ï¼‰
        self._last_react_trace = react_trace or []

        if not react_trace:
            return

        try:
            date_str = datetime.now().strftime("%Y%m%d")
            trace_dir = Path("data/react_traces") / date_str
            trace_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%H%M%S")
            cid_part = (conversation_id or "unknown")[:16]
            trace_file = trace_dir / f"trace_{cid_part}_{timestamp}.json"

            # æ±‡æ€»ç»Ÿè®¡
            total_in = sum(it.get("tokens", {}).get("input", 0) for it in react_trace)
            total_out = sum(it.get("tokens", {}).get("output", 0) for it in react_trace)
            all_tools = []
            for it in react_trace:
                for tc in it.get("tool_calls", []):
                    name = tc.get("name")
                    if name and name not in all_tools:
                        all_tools.append(name)

            trace_data = {
                "conversation_id": conversation_id or "",
                "session_type": session_type,
                "model": react_trace[0].get("model", "") if react_trace else "",
                "started_at": started_at,
                "ended_at": datetime.now().isoformat(),
                "total_iterations": len(react_trace),
                "total_tokens": {"input": total_in, "output": total_out},
                "tools_used": all_tools,
                "result": result,
                "iterations": react_trace,
            }

            with open(trace_file, "w", encoding="utf-8") as f:
                json.dump(trace_data, f, ensure_ascii=False, indent=2, default=str)

            logger.info(
                f"[ReAct] Trace saved: {trace_file} "
                f"(iterations={len(react_trace)}, tools={all_tools}, "
                f"tokens_in={total_in}, tokens_out={total_out})"
            )

            # æ¸…ç†è¶…è¿‡ 7 å¤©çš„æ—§ trace æ–‡ä»¶
            self._cleanup_old_traces(Path("data/react_traces"), max_age_days=7)

        except Exception as e:
            logger.warning(f"[ReAct] Failed to save trace: {e}")

    def _cleanup_old_traces(self, base_dir: Path, max_age_days: int = 7) -> None:
        """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§ trace æ—¥æœŸç›®å½•"""
        try:
            if not base_dir.exists():
                return
            cutoff = time.time() - max_age_days * 86400
            for date_dir in base_dir.iterdir():
                if date_dir.is_dir() and date_dir.stat().st_mtime < cutoff:
                    import shutil
                    shutil.rmtree(date_dir, ignore_errors=True)
        except Exception:
            pass

    # ==================== å–æ¶ˆæ”¶å°¾ï¼ˆæµå¼ï¼‰ ====================

    async def _stream_cancel_farewell(
        self,
        working_messages: list[dict],
        system_prompt: str,
        current_model: str,
        state: TaskState | None = None,
    ):
        """æµå¼åœºæ™¯ä¸‹çš„å–æ¶ˆæ”¶å°¾ï¼šæ³¨å…¥ä¸­æ–­ä¸Šä¸‹æ–‡ï¼Œå‘èµ·è½»é‡ LLM è°ƒç”¨ï¼Œæµå¼è¾“å‡ºæ”¶å°¾æ–‡æœ¬ã€‚

        Yields:
            {"type": "user_insert", ...} å’Œ {"type": "text_delta", ...} äº‹ä»¶
        """
        cancel_reason = (state.cancel_reason if state else "") or "ç”¨æˆ·è¯·æ±‚åœæ­¢"
        logger.info(
            f"[ReAct-Stream][CancelFarewell] è¿›å…¥æ”¶å°¾æµç¨‹: cancel_reason={cancel_reason!r}, "
            f"model={current_model}, msg_count={len(working_messages)}"
        )

        user_text = ""
        if cancel_reason.startswith("ç”¨æˆ·å‘é€åœæ­¢æŒ‡ä»¤: "):
            user_text = cancel_reason[len("ç”¨æˆ·å‘é€åœæ­¢æŒ‡ä»¤: "):]
        elif cancel_reason.startswith("ç”¨æˆ·å‘é€è·³è¿‡æŒ‡ä»¤: "):
            user_text = cancel_reason[len("ç”¨æˆ·å‘é€è·³è¿‡æŒ‡ä»¤: "):]
        if user_text:
            logger.info(f"[ReAct-Stream][CancelFarewell] å›ä¼ ç”¨æˆ·æŒ‡ä»¤æ–‡æœ¬: {user_text!r}")
            yield {"type": "user_insert", "content": user_text}

        cancel_msg = (
            f"[ç³»ç»Ÿé€šçŸ¥] ç”¨æˆ·å‘é€äº†åœæ­¢æŒ‡ä»¤ã€Œ{cancel_reason}ã€ï¼Œ"
            "è¯·ç«‹å³åœæ­¢å½“å‰æ“ä½œï¼Œç®€è¦å‘ŠçŸ¥ç”¨æˆ·å·²åœæ­¢ä»¥åŠå½“å‰è¿›åº¦ï¼ˆ1~2 å¥è¯å³å¯ï¼‰ã€‚"
            "ä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·ã€‚"
        )
        working_messages.append({"role": "user", "content": cancel_msg})

        farewell_text = "âœ… å¥½çš„ï¼Œå·²åœæ­¢å½“å‰ä»»åŠ¡ã€‚"
        logger.info(
            f"[ReAct-Stream][CancelFarewell] å‘èµ· LLM æ”¶å°¾è°ƒç”¨ (timeout=5s), "
            f"working_messages count={len(working_messages)}"
        )
        try:
            farewell_response = await asyncio.wait_for(
                self._brain.messages_create_async(
                    model=current_model,
                    max_tokens=200,
                    system=system_prompt,
                    tools=[],
                    messages=working_messages,
                ),
                timeout=5.0,
            )
            logger.info(
                f"[ReAct-Stream][CancelFarewell] LLM è°ƒç”¨è¿”å›, "
                f"content_blocks={len(farewell_response.content)}, "
                f"stop_reason={getattr(farewell_response, 'stop_reason', 'N/A')}"
            )
            for block in farewell_response.content:
                logger.debug(
                    f"[ReAct-Stream][CancelFarewell] block type={block.type}, "
                    f"text={getattr(block, 'text', '')[:80]!r}"
                )
                if block.type == "text" and block.text.strip():
                    farewell_text = block.text.strip()
                    break
            logger.info(f"[ReAct-Stream][CancelFarewell] LLM farewell æˆåŠŸ: {farewell_text[:120]}")
        except (TimeoutError, asyncio.TimeoutError):
            logger.warning("[ReAct-Stream][CancelFarewell] LLM farewell è¶…æ—¶ (5s)ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬")
        except Exception as e:
            logger.error(
                f"[ReAct-Stream][CancelFarewell] LLM farewell å¤±è´¥: "
                f"{type(e).__name__}: {e}",
                exc_info=True,
            )

        logger.info(f"[ReAct-Stream][CancelFarewell] æœ€ç»ˆè¾“å‡ºæ–‡æœ¬: {farewell_text[:120]}")
        chunk_size = 20
        for i in range(0, len(farewell_text), chunk_size):
            yield {"type": "text_delta", "content": farewell_text[i:i + chunk_size]}
            await asyncio.sleep(0.01)

    # ==================== å¿ƒè·³ä¿æ´» ====================

    _HEARTBEAT_INTERVAL = 15  # ç§’ï¼šLLM ç­‰å¾…æœŸé—´å¿ƒè·³é—´éš”

    async def _reason_with_heartbeat(
        self,
        messages: list[dict],
        *,
        system_prompt: str,
        tools: list[dict],
        current_model: str,
        conversation_id: str | None = None,
        thinking_mode: str | None = None,
        thinking_depth: str | None = None,
    ):
        """
        åŒ…è£… _reason()ï¼Œåœ¨ç­‰å¾… LLM å“åº”æœŸé—´æ¯éš” HEARTBEAT_INTERVAL ç§’
        äº§å‡º heartbeat äº‹ä»¶ï¼Œé˜²æ­¢å‰ç«¯ SSE idle timeoutã€‚

        åŒæ—¶ç›‘å¬ cancel_eventï¼Œå½“ç”¨æˆ·å–æ¶ˆæ—¶ç«‹å³ä¸­æ–­ LLM è°ƒç”¨å¹¶æŠ›å‡º UserCancelledErrorã€‚

        Yields:
            {"type": "heartbeat"} æˆ– {"type": "decision", "decision": Decision}
        """
        queue: asyncio.Queue = asyncio.Queue()

        # è·å– cancel_event
        state = self._state.current_task
        cancel_event = state.cancel_event if state else asyncio.Event()

        async def _do_reason():
            try:
                decision = await self._reason(
                    messages,
                    system_prompt=system_prompt,
                    tools=tools,
                    current_model=current_model,
                    conversation_id=conversation_id,
                    thinking_mode=thinking_mode,
                    thinking_depth=thinking_depth,
                )
                await queue.put(("result", decision))
            except Exception as exc:
                await queue.put(("error", exc))

        async def _heartbeat_loop():
            try:
                while True:
                    await asyncio.sleep(self._HEARTBEAT_INTERVAL)
                    await queue.put(("heartbeat", None))
            except asyncio.CancelledError:
                pass

        async def _cancel_watcher():
            """ç›‘å¬ cancel_eventï¼Œè§¦å‘æ—¶é€šè¿‡ queue é€šçŸ¥ä¸»å¾ªç¯"""
            try:
                await cancel_event.wait()
                await queue.put(("cancelled", None))
            except asyncio.CancelledError:
                pass

        reason_task = asyncio.create_task(_do_reason())
        hb_task = asyncio.create_task(_heartbeat_loop())
        cancel_task = asyncio.create_task(_cancel_watcher())

        try:
            while True:
                typ, data = await queue.get()
                if typ == "heartbeat":
                    yield {"type": "heartbeat"}
                elif typ == "cancelled":
                    cancel_reason = state.cancel_reason if state else "ç”¨æˆ·è¯·æ±‚åœæ­¢"
                    raise UserCancelledError(
                        reason=cancel_reason,
                        source="llm_call_stream",
                    )
                elif typ == "error":
                    raise data  # ä¼ æ’­ _reason çš„å¼‚å¸¸
                else:
                    yield {"type": "decision", "decision": data}
                    break
        finally:
            hb_task.cancel()
            cancel_task.cancel()
            if not reason_task.done():
                reason_task.cancel()
                try:
                    await reason_task
                except (asyncio.CancelledError, Exception):
                    pass

    # ==================== æ¨ç†é˜¶æ®µ ====================

    async def _reason(
        self,
        messages: list[dict],
        *,
        system_prompt: str,
        tools: list[dict],
        current_model: str,
        conversation_id: str | None = None,
        thinking_mode: str | None = None,
        thinking_depth: str | None = None,
    ) -> Decision:
        """
        æ¨ç†é˜¶æ®µ: è°ƒç”¨ LLMï¼Œè¿”å›ç»“æ„åŒ– Decisionã€‚
        """
        # æ ¹æ® thinking_mode å†³å®š use_thinking å‚æ•°
        use_thinking = None  # None = è®© Brain ä½¿ç”¨é»˜è®¤é€»è¾‘
        if thinking_mode == "on":
            use_thinking = True
        elif thinking_mode == "off":
            use_thinking = False
        # "auto" æˆ– None: use_thinking=None â†’ Brain ä½¿ç”¨è‡ªèº«é»˜è®¤é€»è¾‘

        tracer = get_tracer()
        with tracer.llm_span(model=current_model) as span:
            response = await self._brain.messages_create_async(
                use_thinking=use_thinking,
                thinking_depth=thinking_depth,
                model=current_model,
                max_tokens=self._brain.max_tokens,
                system=system_prompt,
                tools=tools,
                messages=messages,
                conversation_id=conversation_id,
            )

            # è®°å½• token ä½¿ç”¨
            if hasattr(response, "usage"):
                span.set_attribute("input_tokens", getattr(response.usage, "input_tokens", 0))
                span.set_attribute("output_tokens", getattr(response.usage, "output_tokens", 0))

            decision = self._parse_decision(response)
            span.set_attribute("decision_type", decision.type.value)
            span.set_attribute("tool_count", len(decision.tool_calls))
            return decision

    def _parse_decision(self, response: Any) -> Decision:
        """è§£æ LLM å“åº”ä¸º Decision"""
        tool_calls = []
        text_content = ""
        thinking_content = ""
        assistant_content = []

        for block in response.content:
            if block.type == "thinking":
                thinking_text = block.thinking if hasattr(block, "thinking") else str(block)
                thinking_content += thinking_text if isinstance(thinking_text, str) else str(thinking_text)
                assistant_content.append({
                    "type": "thinking",
                    "thinking": thinking_text,
                })
            elif block.type == "text":
                text_content += block.text
                assistant_content.append({"type": "text", "text": block.text})
            elif block.type == "tool_use":
                tool_calls.append({
                    "id": block.id,
                    "name": block.name,
                    "input": block.input,
                })
                assistant_content.append({
                    "type": "tool_use",
                    "id": block.id,
                    "name": block.name,
                    "input": block.input,
                })

        decision_type = DecisionType.TOOL_CALLS if tool_calls else DecisionType.FINAL_ANSWER

        return Decision(
            type=decision_type,
            text_content=text_content,
            tool_calls=tool_calls,
            thinking_content=thinking_content,
            raw_response=response,
            stop_reason=getattr(response, "stop_reason", ""),
            assistant_content=assistant_content,
        )

    # ==================== æœ€ç»ˆç­”æ¡ˆå¤„ç† ====================

    async def _handle_final_answer(
        self,
        *,
        decision: Decision,
        working_messages: list[dict],
        original_messages: list[dict],
        tools_executed_in_task: bool,
        executed_tool_names: list[str],
        delivery_receipts: list[dict],
        no_tool_call_count: int,
        verify_incomplete_count: int,
        no_confirmation_text_count: int,
        max_no_tool_retries: int,
        max_verify_retries: int,
        max_confirmation_text_retries: int,
        base_force_retries: int,
        conversation_id: str | None,
    ) -> str | tuple:
        """
        å¤„ç†çº¯æ–‡æœ¬å“åº”ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰ã€‚

        Returns:
            str: æœ€ç»ˆç­”æ¡ˆ
            tuple: (working_messages, no_tool_call_count, verify_incomplete_count,
                    no_confirmation_text_count, max_no_tool_retries) - éœ€è¦ç»§ç»­å¾ªç¯
        """
        if tools_executed_in_task:
            cleaned_text = strip_thinking_tags(decision.text_content)
            if cleaned_text and len(cleaned_text.strip()) > 0:
                # ä»»åŠ¡å®Œæˆåº¦éªŒè¯
                is_completed = await self._response_handler.verify_task_completion(
                    user_request=ResponseHandler.get_last_user_request(original_messages),
                    assistant_response=cleaned_text,
                    executed_tools=executed_tool_names,
                    delivery_receipts=delivery_receipts,
                    conversation_id=conversation_id,
                )

                if is_completed:
                    return cleaned_text

                verify_incomplete_count += 1

                # æ£€æŸ¥æ´»è·ƒ Plan
                has_plan_pending = self._has_active_plan_pending(conversation_id)
                effective_max = max_verify_retries * 2 if has_plan_pending else max_verify_retries

                if verify_incomplete_count >= effective_max:
                    return cleaned_text

                # ç»§ç»­å¾ªç¯
                working_messages.append({
                    "role": "assistant",
                    "content": [{"type": "text", "text": decision.text_content}],
                })

                if has_plan_pending:
                    working_messages.append({
                        "role": "user",
                        "content": (
                            "[ç³»ç»Ÿæç¤º] å½“å‰ Plan ä»æœ‰æœªå®Œæˆçš„æ­¥éª¤ã€‚"
                            "è¯·ç«‹å³ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ª pending æ­¥éª¤ã€‚"
                        ),
                    })
                else:
                    working_messages.append({
                        "role": "user",
                        "content": (
                            "[ç³»ç»Ÿæç¤º] æ ¹æ®å¤æ ¸åˆ¤æ–­ï¼Œç”¨æˆ·è¯·æ±‚å¯èƒ½è¿˜æœ‰æœªå®Œæˆçš„éƒ¨åˆ†ã€‚"
                            "å¦‚æœä½ è®¤ä¸ºå·²ç»å®Œæˆï¼Œè¯·ç›´æ¥ç»™ç”¨æˆ·ä¸€ä¸ªæ€»ç»“å›å¤ï¼›"
                            "å¦‚æœç¡®å®è¿˜æœ‰å‰©ä½™æ­¥éª¤ï¼Œè¯·ç»§ç»­æ‰§è¡Œã€‚"
                        ),
                    })
                return (working_messages, no_tool_call_count, verify_incomplete_count,
                        no_confirmation_text_count, max_no_tool_retries)
            else:
                # æ— å¯è§æ–‡æœ¬
                no_confirmation_text_count += 1
                if no_confirmation_text_count <= max_confirmation_text_retries:
                    working_messages.append({
                        "role": "user",
                        "content": (
                            "[ç³»ç»Ÿ] ä½ å·²æ‰§è¡Œè¿‡å·¥å…·ï¼Œä½†ä½ åˆšæ‰æ²¡æœ‰è¾“å‡ºä»»ä½•ç”¨æˆ·å¯è§çš„æ–‡å­—ç¡®è®¤ã€‚"
                            "è¯·åŸºäºå·²äº§ç”Ÿçš„ tool_result è¯æ®ï¼Œç»™å‡ºæœ€ç»ˆç­”å¤ã€‚"
                        ),
                    })
                    return (working_messages, no_tool_call_count, verify_incomplete_count,
                            no_confirmation_text_count, max_no_tool_retries)

                return (
                    "âš ï¸ å¤§æ¨¡å‹è¿”å›å¼‚å¸¸ï¼šå·¥å…·å·²æ‰§è¡Œï¼Œä½†å¤šæ¬¡æœªè¿”å›ä»»ä½•å¯è§æ–‡æœ¬ç¡®è®¤ï¼Œä»»åŠ¡å·²ä¸­æ–­ã€‚"
                    "è¯·é‡è¯•ã€æˆ–åˆ‡æ¢åˆ°æ›´ç¨³å®šçš„ç«¯ç‚¹/æ¨¡å‹åå†ç»§ç»­ã€‚"
                )

        # æœªæ‰§è¡Œè¿‡å·¥å…·
        max_no_tool_retries = self._effective_force_retries(base_force_retries, conversation_id)
        no_tool_call_count += 1

        if no_tool_call_count <= max_no_tool_retries:
            if decision.text_content:
                working_messages.append({
                    "role": "assistant",
                    "content": [{"type": "text", "text": decision.text_content}],
                })
            working_messages.append({
                "role": "user",
                "content": "[ç³»ç»Ÿ] è‹¥ç¡®å®éœ€è¦å·¥å…·ï¼Œè¯·è°ƒç”¨ç›¸åº”å·¥å…·ï¼›è‹¥ä¸éœ€è¦å·¥å…·ï¼Œè¯·ç›´æ¥å›ç­”ã€‚",
            })
            return (working_messages, no_tool_call_count, verify_incomplete_count,
                    no_confirmation_text_count, max_no_tool_retries)

        # è¿½é—®æ¬¡æ•°ç”¨å°½
        cleaned_text = clean_llm_response(decision.text_content)
        return cleaned_text or (
            "âš ï¸ å¤§æ¨¡å‹è¿”å›å¼‚å¸¸ï¼šæœªäº§ç”Ÿå¯ç”¨è¾“å‡ºã€‚ä»»åŠ¡å·²ä¸­æ–­ã€‚"
            "è¯·é‡è¯•ã€æˆ–æ›´æ¢ç«¯ç‚¹/æ¨¡å‹åå†æ‰§è¡Œã€‚"
        )

    # ==================== å¾ªç¯æ£€æµ‹ ====================

    def _detect_loops(
        self,
        recent_signatures: list[str],
        consecutive_rounds: int,
        working_messages: list[dict],
        text_content: str,
        self_check_interval: int,
        extreme_threshold: int,
        conversation_id: str | None,
    ) -> str | None:
        """
        å¾ªç¯æ£€æµ‹ã€‚

        Returns:
            "terminate" - ç»ˆæ­¢å¾ªç¯
            "disable_force" - ç¦ç”¨ ForceToolCall
            None - ç»§ç»­
        """
        # ç­¾åé‡å¤æ£€æµ‹
        if len(recent_signatures) >= 3:
            from collections import Counter
            sig_counts = Counter(recent_signatures)
            most_common_sig, most_common_count = sig_counts.most_common(1)[0]

            if most_common_count >= 3:
                logger.warning(
                    f"[LoopGuard] True loop: '{most_common_sig}' repeated {most_common_count} times"
                )
                working_messages.append({
                    "role": "user",
                    "content": (
                        "[ç³»ç»Ÿæç¤º] ä½ åœ¨æœ€è¿‘å‡ è½®ä¸­ç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°é‡å¤è°ƒç”¨äº†åŒä¸€ä¸ªå·¥å…·ã€‚"
                        "è¯·è¯„ä¼°ï¼š1. ä»»åŠ¡å·²å®Œæˆåˆ™åœæ­¢è°ƒç”¨ã€‚2. é‡åˆ°å›°éš¾åˆ™æ¢æ–¹æ³•ã€‚"
                    ),
                })

                if most_common_count >= 5:
                    logger.error(f"[LoopGuard] Dead loop ({most_common_count} repeats). Terminating.")
                    return "terminate"

        # å®šæœŸ LLM è‡ªæ£€
        if consecutive_rounds > 0 and consecutive_rounds % self_check_interval == 0:
            has_plan = self._has_active_plan_pending(conversation_id)
            if has_plan:
                working_messages.append({
                    "role": "user",
                    "content": (
                        f"[ç³»ç»Ÿæç¤º] å·²è¿ç»­æ‰§è¡Œ {consecutive_rounds} è½®ï¼ŒPlan ä»æœ‰æœªå®Œæˆæ­¥éª¤ã€‚"
                        "å¦‚æœé‡åˆ°å›°éš¾ï¼Œè¯·æ¢ä¸€ç§æ–¹æ³•ç»§ç»­æ¨è¿›ã€‚"
                    ),
                })
            else:
                working_messages.append({
                    "role": "user",
                    "content": (
                        f"[ç³»ç»Ÿæç¤º] ä½ å·²è¿ç»­æ‰§è¡Œäº† {consecutive_rounds} è½®å·¥å…·è°ƒç”¨ã€‚è¯·è‡ªæˆ‘è¯„ä¼°ï¼š\n"
                        "1. å½“å‰ä»»åŠ¡è¿›åº¦å¦‚ä½•ï¼Ÿ\n"
                        "2. æ˜¯å¦é™·å…¥äº†å¾ªç¯ï¼Ÿ\n"
                        "3. å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œè¯·åœæ­¢å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›å¤ç”¨æˆ·ã€‚"
                    ),
                })

        # æç«¯å®‰å…¨é˜ˆå€¼
        if consecutive_rounds == extreme_threshold:
            logger.warning(f"[LoopGuard] Extreme safety threshold ({extreme_threshold})")
            working_messages.append({
                "role": "user",
                "content": (
                    f"[ç³»ç»Ÿæç¤º] å½“å‰ä»»åŠ¡å·²è¿ç»­æ‰§è¡Œäº† {extreme_threshold} è½®ã€‚"
                    "è¯·å‘ç”¨æˆ·æ±‡æŠ¥è¿›åº¦å¹¶è¯¢é—®æ˜¯å¦ç»§ç»­ã€‚"
                ),
            })
            return "disable_force"

        return None

    # ==================== æ¨¡å‹åˆ‡æ¢ ====================

    def _check_model_switch(
        self,
        task_monitor: Any,
        state: TaskState,
        working_messages: list[dict],
        current_model: str,
    ) -> tuple[str, list[dict]] | None:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨¡å‹åˆ‡æ¢ã€‚è¿”å› (new_model, new_messages) æˆ– None"""
        if not task_monitor or not task_monitor.should_switch_model:
            return None

        new_model = task_monitor.fallback_model
        self._switch_llm_endpoint(new_model, reason="task_monitor timeout fallback")
        task_monitor.switch_model(
            new_model,
            "ä»»åŠ¡è¶…æ—¶ååˆ‡æ¢",
            reset_context=True,
        )

        try:
            llm_client = getattr(self._brain, "_llm_client", None)
            current = llm_client.get_current_model() if llm_client else None
            new_model = current.model if current else new_model
        except Exception:
            pass

        new_messages = list(state.original_user_messages)
        new_messages.append({
            "role": "user",
            "content": (
                "[ç³»ç»Ÿæç¤º] å‘ç”Ÿæ¨¡å‹åˆ‡æ¢ï¼šä¹‹å‰çš„ tool_use/tool_result å†å²å·²æ¸…é™¤ã€‚"
                "è¯·ä»å¤´å¼€å§‹å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚"
            ),
        })

        # æ³¨æ„ï¼š_check_model_switch ä¸åšçŠ¶æ€è½¬æ¢ï¼Œå› ä¸ºå®ƒä¸ä½¿ç”¨ continueï¼Œ
        # æ‰§è¡Œåè‡ªç„¶èµ°åˆ°ä¸»å¾ªç¯çš„ REASONING è½¬æ¢é€»è¾‘ã€‚
        state.reset_for_model_switch()
        return new_model, new_messages

    # æœ€å¤§æ¨¡å‹åˆ‡æ¢æ¬¡æ•°ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰
    MAX_MODEL_SWITCHES = 5

    def _handle_llm_error(
        self,
        error: Exception,
        task_monitor: Any,
        state: TaskState,
        working_messages: list[dict],
        current_model: str,
    ) -> str | tuple | None:
        """
        å¤„ç† LLM è°ƒç”¨é”™è¯¯ã€‚

        Returns:
            "retry" - é‡è¯•
            (new_model, new_messages) - åˆ‡æ¢æ¨¡å‹
            None - é‡æ–°æŠ›å‡º
        """
        if not task_monitor:
            return None

        should_retry = task_monitor.record_error(str(error))

        if should_retry:
            logger.info(f"[LLM] Will retry (attempt {task_monitor.retry_count})")
            return "retry"

        # --- ç†”æ–­ï¼šè¶…è¿‡æœ€å¤§æ¨¡å‹åˆ‡æ¢æ¬¡æ•°æ—¶ç»ˆæ­¢ï¼Œé˜²æ­¢æ­»å¾ªç¯ ---
        switch_count = getattr(state, '_model_switch_count', 0) + 1
        state._model_switch_count = switch_count
        if switch_count > self.MAX_MODEL_SWITCHES:
            logger.error(
                f"[ReAct] Exceeded max model switches ({self.MAX_MODEL_SWITCHES}), "
                f"aborting to prevent infinite loop. Last error: {str(error)[:200]}"
            )
            return None  # ç»ˆæ­¢å¾ªç¯

        # --- æ£€æŸ¥ fallback æ˜¯å¦ä¸å½“å‰æ¨¡å‹å®é™…ç›¸åŒ ---
        new_model = task_monitor.fallback_model
        resolved = self._resolve_endpoint_name(new_model)
        current_endpoint = self._resolve_endpoint_name(current_model)
        if resolved and current_endpoint and resolved == current_endpoint:
            logger.warning(
                f"[ModelSwitch] Fallback model '{new_model}' resolves to same endpoint "
                f"as current '{current_model}' ({resolved}), aborting retry loop"
            )
            return None  # åˆ‡æ¢ç›®æ ‡ä¸å½“å‰ç›¸åŒï¼Œæ— æ„ä¹‰ï¼Œç»ˆæ­¢å¾ªç¯

        self._switch_llm_endpoint(new_model, reason=f"LLM error fallback: {error}")
        task_monitor.switch_model(new_model, "LLM è°ƒç”¨å¤±è´¥ååˆ‡æ¢", reset_context=True)

        try:
            llm_client = getattr(self._brain, "_llm_client", None)
            current = llm_client.get_current_model() if llm_client else None
            new_model = current.model if current else new_model
        except Exception:
            pass

        new_messages = list(state.original_user_messages)
        new_messages.append({
            "role": "user",
            "content": (
                "[ç³»ç»Ÿæç¤º] å‘ç”Ÿæ¨¡å‹åˆ‡æ¢ï¼šä¹‹å‰çš„å†å²å·²æ¸…é™¤ã€‚"
                "è¯·ä»å¤´å¼€å§‹å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚"
            ),
        })

        state.transition(TaskStatus.MODEL_SWITCHING)
        state.reset_for_model_switch()
        return new_model, new_messages

    def _switch_llm_endpoint(self, model_or_endpoint: str, reason: str = "") -> bool:
        """æ‰§è¡Œæ¨¡å‹åˆ‡æ¢"""
        llm_client = getattr(self._brain, "_llm_client", None)
        if not llm_client:
            return False

        endpoint_name = self._resolve_endpoint_name(model_or_endpoint)
        if not endpoint_name:
            return False

        ok, msg = llm_client.switch_model(
            endpoint_name=endpoint_name,
            hours=0.05,
            reason=reason,
        )
        if not ok:
            return False

        try:
            current = llm_client.get_current_model()
            if current and current.model:
                self._brain.model = current.model
        except Exception:
            pass

        logger.info(f"[ModelSwitch] {msg}")
        return True

    def _resolve_endpoint_name(self, model_or_endpoint: str) -> str | None:
        """è§£æ endpoint åç§°"""
        try:
            llm_client = getattr(self._brain, "_llm_client", None)
            if not llm_client:
                return None
            available = [m.name for m in llm_client.list_available_models()]
            if model_or_endpoint in available:
                return model_or_endpoint
            for m in llm_client.list_available_models():
                if m.model == model_or_endpoint:
                    return m.name
            return None
        except Exception:
            return None

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    @staticmethod
    def _is_human_user_message(msg: dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºäººç±»ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ’é™¤ tool_resultï¼‰"""
        if msg.get("role") != "user":
            return False
        content = msg.get("content")
        if isinstance(content, str):
            return True
        if isinstance(content, list):
            part_types = {
                part.get("type")
                for part in content
                if isinstance(part, dict) and part.get("type")
            }
            return "tool_result" not in part_types
        return False

    @staticmethod
    def _effective_force_retries(base_retries: int, conversation_id: str | None) -> int:
        """è®¡ç®—æœ‰æ•ˆ ForceToolCall é‡è¯•æ¬¡æ•°"""
        retries = base_retries
        try:
            from ..tools.handlers.plan import has_active_plan, is_plan_required
            if conversation_id and (has_active_plan(conversation_id) or is_plan_required(conversation_id)):
                retries = max(retries, 1)
        except Exception:
            pass
        return max(0, int(retries))

    @staticmethod
    def _has_active_plan_pending(conversation_id: str | None) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒ Plan ä¸”æœ‰æœªå®Œæˆæ­¥éª¤"""
        try:
            from ..tools.handlers.plan import get_plan_handler_for_session, has_active_plan
            if conversation_id and has_active_plan(conversation_id):
                handler = get_plan_handler_for_session(conversation_id)
                if handler and handler.current_plan:
                    steps = handler.current_plan.get("steps", [])
                    pending = [s for s in steps if s.get("status") in ("pending", "in_progress")]
                    return bool(pending)
        except Exception:
            pass
        return False
