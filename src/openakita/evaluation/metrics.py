"""
è¯„ä¼°æŒ‡æ ‡å®šä¹‰

å®šä¹‰ Agent æ€§èƒ½è¯„ä¼°çš„å„é¡¹æŒ‡æ ‡å’Œèšåˆé€»è¾‘ã€‚
ä» Tracing ç³»ç»Ÿçš„ Trace æ•°æ®ä¸­æå–é‡åŒ–æŒ‡æ ‡ã€‚
"""

import logging
import time
from dataclasses import dataclass, field
from typing import Any

logger = logging.getLogger(__name__)


@dataclass
class TraceMetrics:
    """
    å•æ¬¡ Trace æå–çš„æŒ‡æ ‡ã€‚

    ä»ä¸€ä¸ª Trace (ä¸€æ¬¡å®Œæ•´ç”¨æˆ·è¯·æ±‚) ä¸­æå–çš„é‡åŒ–æ•°æ®ã€‚
    """

    trace_id: str
    session_id: str = ""
    timestamp: float = field(default_factory=time.time)

    # åŸºæœ¬æŒ‡æ ‡
    total_iterations: int = 0
    total_llm_calls: int = 0
    total_tool_calls: int = 0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_duration_ms: float = 0.0

    # è´¨é‡æŒ‡æ ‡
    task_completed: bool = False  # ä»»åŠ¡æ˜¯å¦å®Œæˆ (by state machine)
    tool_errors: int = 0  # å·¥å…·è°ƒç”¨å¤±è´¥æ¬¡æ•°
    loop_detected: bool = False  # æ˜¯å¦è§¦å‘äº†å¾ªç¯æ£€æµ‹
    rollback_count: int = 0  # å›æ»šæ¬¡æ•°
    context_compressions: int = 0  # ä¸Šä¸‹æ–‡å‹ç¼©æ¬¡æ•°

    # å·¥å…·ä½¿ç”¨
    tools_used: list[str] = field(default_factory=list)
    unique_tools: int = 0

    @classmethod
    def from_trace(cls, trace: Any) -> "TraceMetrics":
        """ä» Trace å¯¹è±¡æå–æŒ‡æ ‡ã€‚"""
        from ..tracing.tracer import SpanStatus, SpanType

        metrics = cls(
            trace_id=trace.trace_id,
            session_id=trace.session_id,
            total_duration_ms=trace.duration_ms or 0.0,
        )

        for span in trace.spans:
            if span.span_type == SpanType.LLM:
                metrics.total_llm_calls += 1
                metrics.total_input_tokens += span.attributes.get("input_tokens", 0)
                metrics.total_output_tokens += span.attributes.get("output_tokens", 0)

            elif span.span_type == SpanType.TOOL:
                metrics.total_tool_calls += 1
                tool_name = span.attributes.get("tool_name", "")
                if tool_name:
                    metrics.tools_used.append(tool_name)
                if span.status == SpanStatus.ERROR:
                    metrics.tool_errors += 1

            elif span.span_type == SpanType.CONTEXT:
                metrics.context_compressions += 1

            elif span.span_type == SpanType.REASONING:
                metrics.total_iterations += 1

        metrics.unique_tools = len(set(metrics.tools_used))

        # ä» trace metadata æå–å®Œæˆä¿¡æ¯
        metadata = trace.metadata or {}
        result = metadata.get("result", "")
        metrics.task_completed = result in ("completed", "completed_end_turn")
        metrics.loop_detected = result == "loop_terminated"
        metrics.rollback_count = metadata.get("rollback_count", 0)

        return metrics


@dataclass
class EvalResult:
    """
    å•æ¬¡è¯„ä¼°ç»“æœã€‚

    åŒ…å«é‡åŒ–æŒ‡æ ‡ + LLM Judge çš„å®šæ€§è¯„ä¼°ã€‚
    """

    trace_id: str
    metrics: TraceMetrics
    judge_score: float = 0.0  # 0-1, ç”± Judge è¯„åˆ†
    judge_reasoning: str = ""
    judge_suggestions: list[str] = field(default_factory=list)
    tags: list[str] = field(default_factory=list)  # æ ‡ç­¾: "failed", "slow", "loop", etc.

    def is_good(self) -> bool:
        """æ˜¯å¦é€šè¿‡è¯„ä¼°"""
        return self.metrics.task_completed and self.judge_score >= 0.7


@dataclass
class EvalMetrics:
    """
    èšåˆè¯„ä¼°æŒ‡æ ‡ã€‚

    ä»å¤šä¸ª EvalResult èšåˆå¾—åˆ°çš„æ•´ä½“æ€§èƒ½æŒ‡æ ‡ã€‚
    """

    # è®¡æ•°
    total_traces: int = 0
    period_start: float = 0.0
    period_end: float = 0.0

    # å®Œæˆç‡
    task_completion_rate: float = 0.0  # ä»»åŠ¡å®Œæˆç‡

    # å·¥å…·ç›¸å…³
    tool_selection_accuracy: float = 0.0  # å·¥å…·æ— é”™ç‡ (æ— é”™trace / æ€»trace)
    avg_tool_calls_per_task: float = 0.0
    most_errored_tools: list[tuple[str, int]] = field(default_factory=list)

    # æ•ˆç‡æŒ‡æ ‡
    avg_iterations: float = 0.0
    avg_token_usage: int = 0  # å¹³å‡æ€» token
    avg_latency_ms: float = 0.0

    # å¼‚å¸¸æ£€æµ‹
    loop_detection_rate: float = 0.0  # è§¦å‘å¾ªç¯æ£€æµ‹çš„æ¯”ä¾‹
    error_recovery_rate: float = 0.0  # æœ‰é”™è¯¯ä½†æœ€ç»ˆå®Œæˆçš„æ¯”ä¾‹
    rollback_rate: float = 0.0  # è§¦å‘å›æ»šçš„æ¯”ä¾‹

    # Judge è¯„åˆ†
    avg_judge_score: float = 0.0

    @classmethod
    def aggregate(cls, results: list[EvalResult]) -> "EvalMetrics":
        """ä»è¯„ä¼°ç»“æœåˆ—è¡¨èšåˆæŒ‡æ ‡ã€‚"""
        if not results:
            return cls()

        total = len(results)
        now = time.time()

        # å®Œæˆç‡
        completed = sum(1 for r in results if r.metrics.task_completed)

        # å·¥å…·å‡†ç¡®ç‡: æ— å·¥å…·é”™è¯¯çš„ trace æ¯”ä¾‹
        no_tool_errors = sum(1 for r in results if r.metrics.tool_errors == 0)

        # å¾ªç¯æ£€æµ‹ç‡
        loops = sum(1 for r in results if r.metrics.loop_detected)

        # é”™è¯¯æ¢å¤ç‡
        had_errors = [r for r in results if r.metrics.tool_errors > 0]
        recovered = sum(1 for r in had_errors if r.metrics.task_completed)

        # å›æ»šç‡
        rollbacks = sum(1 for r in results if r.metrics.rollback_count > 0)

        metrics = cls(
            total_traces=total,
            period_start=min(r.metrics.timestamp for r in results),
            period_end=now,
            task_completion_rate=completed / total,
            tool_selection_accuracy=no_tool_errors / total,
            avg_tool_calls_per_task=(
                sum(r.metrics.total_tool_calls for r in results) / total
            ),
            avg_iterations=sum(r.metrics.total_iterations for r in results) / total,
            avg_token_usage=int(
                sum(r.metrics.total_input_tokens + r.metrics.total_output_tokens for r in results) / total
            ),
            avg_latency_ms=sum(r.metrics.total_duration_ms for r in results) / total,
            loop_detection_rate=loops / total,
            error_recovery_rate=(recovered / len(had_errors)) if had_errors else 1.0,
            rollback_rate=rollbacks / total,
            avg_judge_score=(
                sum(r.judge_score for r in results) / total
            ),
        )

        return metrics

    def to_dict(self) -> dict[str, Any]:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            "total_traces": self.total_traces,
            "task_completion_rate": round(self.task_completion_rate, 3),
            "tool_selection_accuracy": round(self.tool_selection_accuracy, 3),
            "avg_tool_calls_per_task": round(self.avg_tool_calls_per_task, 1),
            "avg_iterations": round(self.avg_iterations, 1),
            "avg_token_usage": self.avg_token_usage,
            "avg_latency_ms": round(self.avg_latency_ms, 0),
            "loop_detection_rate": round(self.loop_detection_rate, 3),
            "error_recovery_rate": round(self.error_recovery_rate, 3),
            "rollback_rate": round(self.rollback_rate, 3),
            "avg_judge_score": round(self.avg_judge_score, 3),
        }

    def format_report(self) -> str:
        """æ ¼å¼åŒ–ä¸ºå¯è¯»æŠ¥å‘Š"""
        lines = [
            "=" * 50,
            "OpenAkita Agent è¯„ä¼°æŠ¥å‘Š",
            "=" * 50,
            f"è¯„ä¼° Trace æ•°: {self.total_traces}",
            "",
            "ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡:",
            f"  ä»»åŠ¡å®Œæˆç‡:     {self.task_completion_rate:.1%}",
            f"  å·¥å…·æ— é”™ç‡:     {self.tool_selection_accuracy:.1%}",
            f"  Judge å¹³å‡åˆ†:   {self.avg_judge_score:.2f}/1.0",
            "",
            "âš¡ æ•ˆç‡æŒ‡æ ‡:",
            f"  å¹³å‡è¿­ä»£æ¬¡æ•°:   {self.avg_iterations:.1f}",
            f"  å¹³å‡ Token:     {self.avg_token_usage:,}",
            f"  å¹³å‡å»¶è¿Ÿ:       {self.avg_latency_ms:.0f}ms",
            f"  å¹³å‡å·¥å…·è°ƒç”¨æ•°: {self.avg_tool_calls_per_task:.1f}",
            "",
            "ğŸ” å¼‚å¸¸æŒ‡æ ‡:",
            f"  å¾ªç¯æ£€æµ‹ç‡:     {self.loop_detection_rate:.1%}",
            f"  é”™è¯¯æ¢å¤ç‡:     {self.error_recovery_rate:.1%}",
            f"  å›æ»šè§¦å‘ç‡:     {self.rollback_rate:.1%}",
            "=" * 50,
        ]
        return "\n".join(lines)
