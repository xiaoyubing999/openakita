"""
LLM ç»Ÿä¸€å®¢æˆ·ç«¯

æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ï¼Œæ”¯æŒï¼š
- å¤šç«¯ç‚¹é…ç½®
- è‡ªåŠ¨æ•…éšœåˆ‡æ¢
- èƒ½åŠ›åˆ†æµï¼ˆæ ¹æ®è¯·æ±‚è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç«¯ç‚¹ï¼‰
- å¥åº·æ£€æŸ¥
- åŠ¨æ€æ¨¡å‹åˆ‡æ¢ï¼ˆä¸´æ—¶/æ°¸ä¹…ï¼‰
"""

import asyncio
import json
import logging
from collections.abc import AsyncIterator
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path

from .config import get_default_config_path, load_endpoints_config
from .providers.anthropic import AnthropicProvider
from .providers.base import LLMProvider
from .providers.openai import OpenAIProvider
from .types import (
    AllEndpointsFailedError,
    AudioBlock,
    AuthenticationError,
    DocumentBlock,
    EndpointConfig,
    ImageBlock,
    LLMError,
    LLMRequest,
    LLMResponse,
    Message,
    Tool,
    UnsupportedMediaError,
    VideoBlock,
)

logger = logging.getLogger(__name__)


def _friendly_error_hint(failed_providers: list | None = None, last_error: str = "") -> str:
    """æ ¹æ®å¤±è´¥ç«¯ç‚¹çš„é”™è¯¯åˆ†ç±»ç”Ÿæˆç”¨æˆ·å‹å¥½çš„æç¤ºä¿¡æ¯ã€‚

    è¿”å›ä¸€æ®µé¢å‘ç”¨æˆ·çš„ä¸­æ–‡æç¤ºï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£é—®é¢˜å¹¶é‡‡å–è¡ŒåŠ¨ã€‚
    """
    hints: list[str] = []
    categories: set[str] = set()

    if failed_providers:
        for p in failed_providers:
            cat = getattr(p, "error_category", "")
            if cat:
                categories.add(cat)

    # æ ¹æ®é”™è¯¯ç±»å‹ç»™å‡ºå…·ä½“å»ºè®®
    if "quota" in categories:
        hints.append("ğŸ’³ æ£€æµ‹åˆ° API é…é¢è€—å°½ï¼Œè¯·å‰å¾€å¯¹åº”å¹³å°å……å€¼æˆ–å‡çº§å¥—é¤ï¼Œå……å€¼åä¼šè‡ªåŠ¨æ¢å¤ã€‚")
    if "auth" in categories:
        hints.append("ğŸ”‘ æ£€æµ‹åˆ° API è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®ã€æ˜¯å¦è¿‡æœŸã€‚")
    if "transient" in categories:
        hints.append("ğŸŒ æ£€æµ‹åˆ°ç½‘ç»œè¶…æ—¶/è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®ã€‚")
    if "structural" in categories:
        hints.append("âš™ï¸ æ£€æµ‹åˆ°è¯·æ±‚æ ¼å¼é”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯æ¨¡å‹å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·å°è¯•åˆ‡æ¢å…¶ä»–æ¨¡å‹ã€‚")

    if not hints:
        # æ— æ³•åˆ†ç±»æ—¶çš„é€šç”¨æç¤º
        hints.append("è¯·æ£€æŸ¥ API Keyã€ç½‘ç»œè¿æ¥å’Œè´¦æˆ·ä½™é¢ã€‚")

    return " ".join(hints)


# ==================== åŠ¨æ€åˆ‡æ¢ç›¸å…³æ•°æ®ç»“æ„ ====================


@dataclass
class EndpointOverride:
    """ç«¯ç‚¹ä¸´æ—¶è¦†ç›–é…ç½®"""

    endpoint_name: str  # è¦†ç›–åˆ°çš„ç«¯ç‚¹åç§°
    expires_at: datetime  # è¿‡æœŸæ—¶é—´
    created_at: datetime = field(default_factory=datetime.now)
    reason: str = ""  # åˆ‡æ¢åŸå› ï¼ˆå¯é€‰ï¼‰

    @property
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¿‡æœŸ"""
        return datetime.now() >= self.expires_at

    @property
    def remaining_hours(self) -> float:
        """å‰©ä½™æœ‰æ•ˆæ—¶é—´ï¼ˆå°æ—¶ï¼‰"""
        if self.is_expired:
            return 0.0
        delta = self.expires_at - datetime.now()
        return delta.total_seconds() / 3600

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰"""
        return {
            "endpoint_name": self.endpoint_name,
            "expires_at": self.expires_at.isoformat(),
            "created_at": self.created_at.isoformat(),
            "reason": self.reason,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "EndpointOverride":
        """ä»å­—å…¸åˆ›å»ºï¼ˆç”¨äºååºåˆ—åŒ–ï¼‰"""
        return cls(
            endpoint_name=data["endpoint_name"],
            expires_at=datetime.fromisoformat(data["expires_at"]),
            created_at=datetime.fromisoformat(data["created_at"]),
            reason=data.get("reason", ""),
        )


@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯ï¼ˆç”¨äºåˆ—è¡¨å±•ç¤ºï¼‰"""

    name: str  # ç«¯ç‚¹åç§°
    model: str  # æ¨¡å‹åç§°
    provider: str  # æä¾›å•†
    priority: int  # ä¼˜å…ˆçº§
    is_healthy: bool  # å¥åº·çŠ¶æ€
    is_current: bool  # æ˜¯å¦å½“å‰ä½¿ç”¨
    is_override: bool  # æ˜¯å¦ä¸´æ—¶è¦†ç›–
    capabilities: list[str]  # æ”¯æŒçš„èƒ½åŠ›
    note: str = ""  # å¤‡æ³¨


class LLMClient:
    """ç»Ÿä¸€ LLM å®¢æˆ·ç«¯"""

    # é»˜è®¤ä¸´æ—¶åˆ‡æ¢æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
    DEFAULT_OVERRIDE_HOURS = 12
    def __init__(
        self,
        config_path: Path | None = None,
        endpoints: list[EndpointConfig] | None = None,
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            endpoints: ç›´æ¥ä¼ å…¥ç«¯ç‚¹é…ç½®ï¼ˆä¼˜å…ˆäº config_pathï¼‰
        """
        self._endpoints: list[EndpointConfig] = []
        self._providers: dict[str, LLMProvider] = {}
        self._settings: dict = {}
        self._config_path: Path | None = config_path

        # åŠ¨æ€åˆ‡æ¢ç›¸å…³
        self._endpoint_override: EndpointOverride | None = None
        # per-conversation ä¸´æ—¶è¦†ç›–ï¼ˆç”¨äºå¹¶å‘éš”ç¦»ï¼‰
        self._conversation_overrides: dict[str, EndpointOverride] = {}

        # ç«¯ç‚¹äº²å’Œæ€§ï¼šè®°å½•ä¸Šä¸€æ¬¡æˆåŠŸçš„ç«¯ç‚¹åç§°
        # æœ‰å·¥å…·ä¸Šä¸‹æ–‡æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç«¯ç‚¹ï¼ˆé¿å… failover ååˆå›åˆ°é«˜ä¼˜å…ˆçº§çš„æ•…éšœç«¯ç‚¹ï¼‰
        self._last_success_endpoint: str | None = None

        if endpoints:
            self._endpoints = sorted(endpoints, key=lambda x: x.priority)
        elif config_path or get_default_config_path().exists():
            self._config_path = config_path or get_default_config_path()
            self._endpoints, _, _, self._settings = load_endpoints_config(config_path)

        # åˆ›å»º Provider å®ä¾‹
        self._init_providers()

    def reload(self) -> bool:
        """çƒ­é‡è½½ï¼šé‡æ–°è¯»å–é…ç½®æ–‡ä»¶å¹¶é‡å»ºæ‰€æœ‰ Providerã€‚

        Returns:
            True è¡¨ç¤ºæˆåŠŸé‡è½½ï¼ŒFalse è¡¨ç¤ºé…ç½®æ–‡ä»¶ä¸å¯ç”¨ã€‚
        """
        if not self._config_path or not self._config_path.exists():
            logger.warning("reload() called but no config_path available")
            return False
        try:
            # Re-read .env so newly written API keys are available in os.environ
            from dotenv import load_dotenv as _reload_dotenv

            env_path = self._config_path.parent.parent / ".env"
            if env_path.exists():
                _reload_dotenv(env_path, override=True)

            new_endpoints, _, _, new_settings = load_endpoints_config(self._config_path)
            self._endpoints = new_endpoints
            self._settings = new_settings
            self._providers.clear()
            self._init_providers()
            self._last_success_endpoint = None  # é‡è½½åé‡ç½®ç«¯ç‚¹äº²å’Œæ€§
            logger.info(
                f"LLMClient reloaded from {self._config_path}: "
                f"{len(self._endpoints)} endpoints, {len(self._providers)} providers"
            )
            return True
        except Exception as e:
            logger.error(f"LLMClient reload failed: {e}", exc_info=True)
            return False

    def _init_providers(self):
        """åˆå§‹åŒ–æ‰€æœ‰ Provider"""
        for ep in self._endpoints:
            provider = self._create_provider(ep)
            if provider:
                self._providers[ep.name] = provider

    def _create_provider(self, config: EndpointConfig) -> LLMProvider | None:
        """æ ¹æ®é…ç½®åˆ›å»º Provider"""
        try:
            if config.api_type == "anthropic":
                return AnthropicProvider(config)
            elif config.api_type == "openai":
                return OpenAIProvider(config)
            else:
                logger.warning(f"Unknown api_type '{config.api_type}' for endpoint '{config.name}'")
                return None
        except Exception as e:
            logger.error(f"Failed to create provider for '{config.name}': {e}")
            return None

    @property
    def endpoints(self) -> list[EndpointConfig]:
        """è·å–æ‰€æœ‰ç«¯ç‚¹é…ç½®"""
        return self._endpoints

    @property
    def providers(self) -> dict[str, LLMProvider]:
        """è·å–æ‰€æœ‰ Provider"""
        return self._providers

    async def chat(
        self,
        messages: list[Message],
        system: str = "",
        tools: list[Tool] | None = None,
        max_tokens: int = 0,
        temperature: float = 1.0,
        enable_thinking: bool = False,
        thinking_depth: str | None = None,
        conversation_id: str | None = None,
        **kwargs,
    ) -> LLMResponse:
        """
        ç»Ÿä¸€èŠå¤©æ¥å£

        è‡ªåŠ¨å¤„ç†ï¼š
        1. æ ¹æ®è¯·æ±‚å†…å®¹æ¨æ–­æ‰€éœ€èƒ½åŠ›
        2. ç­›é€‰æ”¯æŒæ‰€éœ€èƒ½åŠ›çš„ç«¯ç‚¹
        3. æŒ‰ä¼˜å…ˆçº§å°è¯•è°ƒç”¨
        4. è‡ªåŠ¨æ•…éšœåˆ‡æ¢

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤º
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            max_tokens: æœ€å¤§è¾“å‡º token
            temperature: æ¸©åº¦
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            thinking_depth: æ€è€ƒæ·±åº¦ ('low'/'medium'/'high')
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ç»Ÿä¸€å“åº”æ ¼å¼

        Raises:
            UnsupportedMediaError: è§†é¢‘å†…å®¹ä½†æ²¡æœ‰æ”¯æŒè§†é¢‘çš„ç«¯ç‚¹
            AllEndpointsFailedError: æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥
        """
        request = LLMRequest(
            messages=messages,
            system=system,
            tools=tools,
            max_tokens=max_tokens,
            temperature=temperature,
            enable_thinking=enable_thinking,
            thinking_depth=thinking_depth,
            extra_params=kwargs.get("extra_params"),
        )

        # æ¨æ–­æ‰€éœ€èƒ½åŠ›
        require_tools = bool(tools)
        require_vision = self._has_images(messages)
        require_video = self._has_videos(messages)
        require_audio = self._has_audio(messages)
        require_pdf = self._has_documents(messages)
        require_thinking = bool(enable_thinking)

        # æ£€æµ‹å·¥å…·ä¸Šä¸‹æ–‡ï¼šå¯¹ failover éœ€è¦æ›´ä¿å®ˆ
        #
        # å…³é”®åŸå› ï¼š
        # - å·¥å…·é“¾çš„â€œè¿ç»­æ€§â€ä¸ä»…æ˜¯æ¶ˆæ¯æ ¼å¼å…¼å®¹ï¼ˆOpenAI-compatible / Anthropicï¼‰
        # - è¿˜åŒ…å«æ¨¡å‹ç‰¹å®šçš„æ€ç»´é“¾/å…ƒæ•°æ®è¿ç»­æ€§ï¼ˆä¾‹å¦‚ MiniMax M2.1 çš„ interleaved thinkingï¼‰
        #   è¿™ç±»ä¿¡æ¯è‹¥æœªå®Œæ•´ä¿ç•™/å›ä¼ ï¼Œæˆ–ä¸­é€”åˆ‡æ¢åˆ°å¦ä¸€æ¨¡å‹ï¼Œå·¥å…·è°ƒç”¨è´¨é‡ä¼šæ˜æ˜¾ä¸‹é™
        #
        # å› æ­¤é»˜è®¤ï¼šåªè¦æ£€æµ‹åˆ°å·¥å…·ä¸Šä¸‹æ–‡ï¼Œå°±ç¦ç”¨ failoverï¼ˆä¿æŒåŒä¸€ç«¯ç‚¹/åŒä¸€æ¨¡å‹ï¼‰
        # ä½†å…è®¸é€šè¿‡é…ç½®æ˜¾å¼å¼€å¯â€œåŒåè®®å†… failoverâ€ï¼ˆé»˜è®¤ä¸å¼€å¯ï¼‰ã€‚
        has_tool_context = self._has_tool_context(messages)
        allow_failover = not has_tool_context

        if has_tool_context:
            logger.debug(
                "[LLM] Tool context detected in messages; failover disabled by default "
                "(set settings.allow_failover_with_tool_context=true to override)."
            )

        # ç­›é€‰æ”¯æŒæ‰€éœ€èƒ½åŠ›çš„ç«¯ç‚¹
        # æœ‰å·¥å…·ä¸Šä¸‹æ–‡æ—¶ä¼ å…¥ç«¯ç‚¹äº²å’Œæ€§ï¼šä¼˜å…ˆä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç«¯ç‚¹
        eligible = self._filter_eligible_endpoints(
            require_tools=require_tools,
            require_vision=require_vision,
            require_video=require_video,
            require_thinking=require_thinking,
            require_audio=require_audio,
            require_pdf=require_pdf,
            conversation_id=conversation_id,
            prefer_endpoint=self._last_success_endpoint if has_tool_context else None,
        )

        # å¯é€‰ï¼šå·¥å…·ä¸Šä¸‹æ–‡ä¸‹å¯ç”¨ failoverï¼ˆæ˜¾å¼é…ç½®æ‰å¼€å¯ï¼‰
        if has_tool_context and eligible:
            if self._settings.get("allow_failover_with_tool_context", False):
                # é»˜è®¤åªå…è®¸åŒåè®®å†…åˆ‡æ¢ï¼›é¿å… anthropic/openai æ··ç”¨å¯¼è‡´ tool message ä¸å…¼å®¹
                api_types = {p.config.api_type for p in eligible}
                if len(api_types) == 1:
                    allow_failover = True
                    logger.debug(
                        "[LLM] Tool context failover explicitly enabled; "
                        f"api_type={next(iter(api_types))}."
                    )
                else:
                    allow_failover = False
                    logger.debug(
                        "[LLM] Tool context failover requested but eligible endpoints have mixed "
                        f"api_types={sorted(api_types)}; failover remains disabled."
                    )

        if eligible:
            return await self._try_endpoints(eligible, request, allow_failover=allow_failover)

        # eligible ä¸ºç©º â€” ä½¿ç”¨å…¬å…±é™çº§ç­–ç•¥
        providers = await self._resolve_providers_with_fallback(
            request=request,
            require_tools=require_tools,
            require_vision=require_vision,
            require_video=require_video,
            require_thinking=require_thinking,
            require_audio=require_audio,
            require_pdf=require_pdf,
            conversation_id=conversation_id,
            prefer_endpoint=self._last_success_endpoint if has_tool_context else None,
        )
        return await self._try_endpoints(providers, request, allow_failover=allow_failover)

    async def chat_stream(
        self,
        messages: list[Message],
        system: str = "",
        tools: list[Tool] | None = None,
        max_tokens: int = 0,
        temperature: float = 1.0,
        enable_thinking: bool = False,
        thinking_depth: str | None = None,
        conversation_id: str | None = None,
        **kwargs,
    ) -> AsyncIterator[dict]:
        """
        æµå¼èŠå¤©æ¥å£ï¼ˆå¸¦å®Œæ•´é™çº§ç­–ç•¥ï¼‰

        ä¸ chat() å…±ç”¨é™çº§é€»è¾‘ï¼šthinking è½¯é™çº§ã€å†·é™æœŸç­‰å¾…ã€å¤šç«¯ç‚¹è½®è¯¢ã€‚
        æµå¼ç‰¹æ®Šå¤„ç†ï¼šä¸€æ—¦å¼€å§‹äº§å‡ºäº‹ä»¶ï¼ˆyielded=Trueï¼‰ï¼Œä¸­é€”å¤±è´¥ä¸å†åˆ‡æ¢ç«¯ç‚¹
        ï¼ˆé¿å…å‘å®¢æˆ·ç«¯å‘é€æ··åˆçš„éƒ¨åˆ†å“åº”ï¼‰ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤º
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            max_tokens: æœ€å¤§è¾“å‡º token
            temperature: æ¸©åº¦
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            thinking_depth: æ€è€ƒæ·±åº¦ ('low'/'medium'/'high')
            conversation_id: å¯¹è¯ ID
            **kwargs: é¢å¤–å‚æ•°

        Yields:
            æµå¼äº‹ä»¶
        """
        request = LLMRequest(
            messages=messages,
            system=system,
            tools=tools,
            max_tokens=max_tokens,
            temperature=temperature,
            enable_thinking=enable_thinking,
            thinking_depth=thinking_depth,
            extra_params=kwargs.get("extra_params"),
        )

        # æ¨æ–­æ‰€éœ€èƒ½åŠ›
        require_tools = bool(tools)
        require_vision = self._has_images(messages)
        require_video = self._has_videos(messages)
        require_audio = self._has_audio(messages)
        require_pdf = self._has_documents(messages)
        require_thinking = bool(enable_thinking)

        # ä½¿ç”¨å…¬å…±é™çº§ç­–ç•¥è§£æç«¯ç‚¹åˆ—è¡¨
        eligible = self._filter_eligible_endpoints(
            require_tools=require_tools,
            require_vision=require_vision,
            require_video=require_video,
            require_thinking=require_thinking,
            require_audio=require_audio,
            require_pdf=require_pdf,
            conversation_id=conversation_id,
        )

        if not eligible:
            eligible = await self._resolve_providers_with_fallback(
                request=request,
                require_tools=require_tools,
                require_vision=require_vision,
                require_video=require_video,
                require_thinking=require_thinking,
                require_audio=require_audio,
                require_pdf=require_pdf,
                conversation_id=conversation_id,
            )

        # å¤šç«¯ç‚¹è½®è¯¢ï¼šä¾æ¬¡å°è¯•æ¯ä¸ªç«¯ç‚¹
        # æµå¼ç‰¹æ®Šå¤„ç†ï¼šä¸€æ—¦æœ‰äº‹ä»¶äº§å‡ºå°±ä¸å†åˆ‡æ¢ï¼ˆé¿å…æ··åˆå“åº”ï¼‰
        last_error: Exception | None = None
        for i, provider in enumerate(eligible):
            yielded = False
            try:
                logger.info(
                    f"[LLM-Stream] endpoint={provider.name} model={provider.model} "
                    f"action=stream_request"
                )
                async for event in provider.chat_stream(request):
                    yielded = True
                    yield event
                # æµå®Œæˆï¼šprovider å†…éƒ¨å·²è°ƒç”¨ mark_healthy()
                self._last_success_endpoint = provider.name
                return
            except LLMError as e:
                last_error = e
                if yielded:
                    # å·²äº§å‡ºéƒ¨åˆ†äº‹ä»¶ï¼Œä¸èƒ½åˆ‡æ¢ç«¯ç‚¹ï¼ˆå®¢æˆ·ç«¯ä¼šæ”¶åˆ°æ··åˆå“åº”ï¼‰
                    logger.error(
                        f"[LLM-Stream] endpoint={provider.name} mid-stream failure: {e}. "
                        f"Cannot failover (partial response already sent)."
                    )
                    raise
                # æœªäº§å‡ºä»»ä½•äº‹ä»¶ â†’ å®‰å…¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªç«¯ç‚¹
                # provider å†…éƒ¨å·²è°ƒç”¨ mark_unhealthy()
                logger.warning(
                    f"[LLM-Stream] endpoint={provider.name} error={e}"
                    + (f", trying next endpoint..." if i < len(eligible) - 1 else "")
                )
            except Exception as e:
                last_error = e
                if yielded:
                    raise
                provider.mark_unhealthy(str(e))
                logger.warning(
                    f"[LLM-Stream] endpoint={provider.name} unexpected_error={e}"
                    + (f", trying next endpoint..." if i < len(eligible) - 1 else "")
                )

        hint = _friendly_error_hint(eligible)
        raise AllEndpointsFailedError(
            f"Stream: all {len(eligible)} endpoints failed. {hint} Last error: {last_error}"
        )

    # ==================== å…¬å…±é™çº§ç­–ç•¥ ====================

    async def _resolve_providers_with_fallback(
        self,
        request: LLMRequest,
        require_tools: bool = False,
        require_vision: bool = False,
        require_video: bool = False,
        require_thinking: bool = False,
        require_audio: bool = False,
        require_pdf: bool = False,
        conversation_id: str | None = None,
        prefer_endpoint: str | None = None,
    ) -> list[LLMProvider]:
        """å…¬å…±åˆ†å±‚é™çº§ç­–ç•¥ â€” ä¾› chat() å’Œ chat_stream() å¤ç”¨

        å½“ _filter_eligible_endpoints() è¿”å›ç©ºåˆ—è¡¨æ—¶è°ƒç”¨æ­¤æ–¹æ³•ï¼Œ
        æŒ‰ä»¥ä¸‹é¡ºåºé€çº§é™çº§ï¼Œç›´åˆ°æ‰¾åˆ°å¯ç”¨ç«¯ç‚¹ï¼š

        1. thinking è½¯é™çº§ï¼šæ”¾å¼ƒ thinking è¦æ±‚ï¼Œç”¨é thinking ç«¯ç‚¹
        2. ç­‰å¾…å†·é™æœŸæ¢å¤ï¼šç­‰æœ€çŸ­çš„ç¬æ—¶å†·é™æœŸï¼ˆæœ€å¤šç­‰ 35sï¼‰
        3. å¼ºåˆ¶é‡è¯•ï¼šå¿½ç•¥å†·é™æœŸï¼Œå¼ºåˆ¶è°ƒç”¨åŒ¹é…åŸºç¡€èƒ½åŠ›çš„ç«¯ç‚¹
        4. æœ€ç»ˆå…œåº•ï¼šæ‰€æœ‰ç«¯ç‚¹éƒ½è¯•ä¸€é

        å‰¯ä½œç”¨ï¼š
            - å¯èƒ½ä¿®æ”¹ request.enable_thinking = Falseï¼ˆthinking é™çº§æ—¶ï¼‰

        Raises:
            UnsupportedMediaError: éœ€è¦è§†é¢‘ä½†æ— è§†é¢‘èƒ½åŠ›ç«¯ç‚¹
            AllEndpointsFailedError: æ‰€æœ‰ç«¯ç‚¹å‡ä¸ºç»“æ„æ€§é”™è¯¯

        Returns:
            æŒ‰ä¼˜å…ˆçº§æ’åºçš„ç«¯ç‚¹åˆ—è¡¨ï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ªç«¯ç‚¹ï¼‰
        """
        providers_sorted = sorted(self._providers.values(), key=lambda p: p.config.priority)

        # â”€â”€ é™çº§ 1: thinking è½¯é™çº§ â”€â”€
        # thinking ä¸åŒäº tools/vision/videoï¼šæ²¡æœ‰å®ƒè¯·æ±‚ä»èƒ½æ­£å¸¸å·¥ä½œ
        # å¦‚æœå› ä¸º thinking è¦æ±‚å¯¼è‡´æ— å¯ç”¨ç«¯ç‚¹ï¼Œé™çº§åˆ°æ—  thinking æ¨¡å¼
        if require_thinking:
            eligible_no_thinking = self._filter_eligible_endpoints(
                require_tools=require_tools,
                require_vision=require_vision,
                require_video=require_video,
                require_thinking=False,
                require_audio=require_audio,
                require_pdf=require_pdf,
                conversation_id=conversation_id,
                prefer_endpoint=prefer_endpoint,
            )
            if eligible_no_thinking:
                logger.info(
                    f"[LLM] No healthy thinking-capable endpoint. "
                    f"Falling back to non-thinking mode "
                    f"({len(eligible_no_thinking)} endpoints available)."
                )
                request.enable_thinking = False
                return eligible_no_thinking

        # â”€â”€ é™çº§ 2+3+4: æ‰€æœ‰ç«¯ç‚¹éƒ½åœ¨å†·é™æœŸ â”€â”€
        # æ„å»ºåŸºç¡€èƒ½åŠ›åŒ¹é…åˆ—è¡¨ï¼ˆä¸å« thinking è¦æ±‚ï¼Œå¿½ç•¥å¥åº·çŠ¶æ€ï¼‰
        base_capability_matched = [
            p
            for p in providers_sorted
            if (not require_tools or p.config.has_capability("tools"))
            and (not require_vision or p.config.has_capability("vision"))
            and (not require_video or p.config.has_capability("video"))
            and (not require_audio or p.config.has_capability("audio"))
            and (not require_pdf or p.config.has_capability("pdf"))
        ]

        # å¤šæ¨¡æ€è½¯é™çº§: è§†é¢‘/éŸ³é¢‘/PDF ç«¯ç‚¹ä¸åŒ¹é…æ—¶ä¸ç¡¬å¤±è´¥
        if not base_capability_matched:
            degraded = []
            if require_video:
                degraded.append("video")
                require_video = False
            if require_audio:
                degraded.append("audio")
                require_audio = False
            if require_pdf:
                degraded.append("pdf")
                require_pdf = False
            if degraded:
                logger.warning(
                    f"[LLM] No endpoint supports {'/'.join(degraded)}. "
                    "Content will be degraded (keyframes/text/STT)."
                )
                base_capability_matched = [
                    p
                    for p in providers_sorted
                    if (not require_tools or p.config.has_capability("tools"))
                    and (not require_vision or p.config.has_capability("vision"))
                ]

        # å¦‚æœé™çº§äº† thinkingï¼Œæ›´æ–° request
        if require_thinking:
            request.enable_thinking = False
            logger.info("[LLM] All endpoints in cooldown. Disabling thinking for fallback attempt.")

        if base_capability_matched:
            unhealthy = [p for p in base_capability_matched if not p.is_healthy]
            unhealthy_count = len(unhealthy)

            if unhealthy_count > 0:
                # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
                structural = [p for p in unhealthy if p.error_category == "structural"]
                quota_or_auth = [
                    p for p in unhealthy
                    if p.error_category in ("quota", "auth")
                ]
                non_structural = [p for p in unhealthy if p.error_category != "structural"]

                # â”€â”€ é™çº§ 2: ç­‰å¾…ç¬æ—¶å†·é™æœŸæ¢å¤ â”€â”€
                transient_like = [
                    p for p in non_structural
                    if p.error_category not in ("quota", "auth")
                ]
                if transient_like:
                    min_transient_cd = min(p.cooldown_remaining for p in transient_like)
                    if 0 < min_transient_cd <= 35:
                        logger.info(
                            f"[LLM] All endpoints in cooldown. "
                            f"Waiting {min_transient_cd}s for transient recovery..."
                        )
                        await asyncio.sleep(min(min_transient_cd + 1, 35))
                        # ç­‰å¾…åé‡æ–°ç­›é€‰
                        eligible = self._filter_eligible_endpoints(
                            require_tools=require_tools,
                            require_vision=require_vision,
                            require_video=require_video,
                            require_thinking=False,
                            require_audio=require_audio,
                            require_pdf=require_pdf,
                            conversation_id=conversation_id,
                            prefer_endpoint=prefer_endpoint,
                        )
                        if eligible:
                            logger.info(
                                f"[LLM] Recovery detected: "
                                f"{len(eligible)} endpoints available after wait"
                            )
                            return eligible

                # â”€â”€ å…¨éƒ¨æ˜¯ç»“æ„æ€§é”™è¯¯ï¼ˆ400 å‚æ•°é”™è¯¯ç­‰ï¼‰ï¼Œé‡è¯•æ— æ„ä¹‰ â†’ æŠ¥é”™ â”€â”€
                if structural and len(structural) == unhealthy_count:
                    last_err = structural[0]._last_error or "unknown structural error"
                    min_cd = min(p.cooldown_remaining for p in structural)
                    hint = _friendly_error_hint(structural)
                    raise AllEndpointsFailedError(
                        f"All endpoints failed with structural errors "
                        f"(cooldown {min_cd}s). {hint} Last error: {last_err}"
                    )

                # â”€â”€ å…¨éƒ¨æ˜¯é…é¢/è®¤è¯é”™è¯¯ï¼Œé‡è¯•æ— æ„ä¹‰ â†’ å¿«é€ŸæŠ¥é”™ â”€â”€
                if quota_or_auth and len(quota_or_auth) == unhealthy_count:
                    last_err = quota_or_auth[0]._last_error or "unknown auth/quota error"
                    categories = sorted({p.error_category for p in quota_or_auth})
                    hint = _friendly_error_hint(quota_or_auth)
                    raise AllEndpointsFailedError(
                        f"All endpoints failed with {'/'.join(categories)} errors. "
                        f"{hint} Last error: {last_err}"
                    )

            # â”€â”€ é™çº§ 3: "æœ€åé˜²çº¿æ—è·¯" â€” ç»•è¿‡å†·é™æœŸï¼ˆå¯¹é½ Portkeyï¼‰ â”€â”€
            # Portkey æ ¸å¿ƒè§„åˆ™ï¼šå½“æ²¡æœ‰å¥åº·ç›®æ ‡æ—¶ï¼Œç»•è¿‡ circuit breaker å°è¯•æ‰€æœ‰ç›®æ ‡
            # æ’é™¤ quota/auth é”™è¯¯çš„ç«¯ç‚¹ï¼ˆè¿™ç±»é”™è¯¯é‡è¯•æ— æ„ä¹‰ï¼‰
            retryable = [
                p for p in base_capability_matched
                if p.is_healthy or p.error_category not in ("quota", "auth")
            ]
            if retryable:
                logger.warning(
                    f"[LLM] No healthy endpoint available. "
                    f"Bypassing cooldowns for {len(retryable)} endpoints "
                    f"(last resort, Portkey-style)."
                )
                for p in retryable:
                    if not p.is_healthy:
                        p.reset_cooldown()
                return retryable

            # å¦‚æœæ‰€æœ‰ç«¯ç‚¹éƒ½æ˜¯ quota/authï¼Œä»ç„¶è¿”å›å®ƒä»¬ï¼ˆè®© _try_endpoints å†³å®šæœ€ç»ˆé”™è¯¯ï¼‰
            logger.warning(
                f"[LLM] All {len(base_capability_matched)} endpoints have "
                f"non-retryable errors. Returning for final error handling."
            )
            return base_capability_matched

        # â”€â”€ é™çº§ 4: æœ€ç»ˆå…œåº• â€” å°è¯•æ‰€æœ‰ç«¯ç‚¹ â”€â”€
        logger.warning(
            f"[LLM] No endpoint matches required capabilities "
            f"(tools={require_tools}, vision={require_vision}, video={require_video}). "
            f"Trying all {len(providers_sorted)} endpoints as last resort."
        )
        return providers_sorted

    # ==================== ç«¯ç‚¹ç­›é€‰ ====================

    def _filter_eligible_endpoints(
        self,
        require_tools: bool = False,
        require_vision: bool = False,
        require_video: bool = False,
        require_thinking: bool = False,
        require_audio: bool = False,
        require_pdf: bool = False,
        conversation_id: str | None = None,
        prefer_endpoint: str | None = None,
    ) -> list[LLMProvider]:
        """ç­›é€‰æ”¯æŒæ‰€éœ€èƒ½åŠ›çš„ç«¯ç‚¹

        æ³¨æ„ï¼š
        - enable_thinking=True æ—¶ï¼Œä¼˜å…ˆ/è¦æ±‚ç«¯ç‚¹å…·å¤‡ thinking èƒ½åŠ›ï¼ˆé¿å…èƒ½åŠ›/æ ¼å¼é€€åŒ–ï¼‰
        - å¦‚æœæœ‰ä¸´æ—¶è¦†ç›–ä¸”è¦†ç›–ç«¯ç‚¹æ”¯æŒæ‰€éœ€èƒ½åŠ›ï¼Œä¼˜å…ˆä½¿ç”¨è¦†ç›–ç«¯ç‚¹
        - prefer_endpoint: ç«¯ç‚¹äº²å’Œæ€§ï¼Œæœ‰å·¥å…·ä¸Šä¸‹æ–‡æ—¶ä¼ å…¥ä¸Šæ¬¡æˆåŠŸçš„ç«¯ç‚¹åç§°ï¼Œ
          å°†å…¶æå‡åˆ°é˜Ÿåˆ—å‰ç«¯ï¼ˆä¼˜å…ˆäº priority æ’åºï¼Œä½†ä½äº overrideï¼‰
        """
        # æ¸…ç†è¿‡æœŸçš„ override
        # 1) æ¸…ç†å½“å‰ conversation çš„è¿‡æœŸ override
        if conversation_id:
            ov = self._conversation_overrides.get(conversation_id)
            if ov and ov.is_expired:
                self._conversation_overrides.pop(conversation_id, None)
        # 2) æ¸…ç†å…¨å±€ override
        if self._endpoint_override and self._endpoint_override.is_expired:
            logger.info("[LLM] Override expired, restoring default")
            self._endpoint_override = None
        # 3) å®šæœŸæ¸…ç†æ‰€æœ‰è¿‡æœŸçš„ conversation overridesï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        #    ä»…å½“ç§¯ç´¯è¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨éƒ½éå†
        if len(self._conversation_overrides) > 50:
            expired_keys = [
                k for k, v in self._conversation_overrides.items() if v.is_expired
            ]
            for k in expired_keys:
                self._conversation_overrides.pop(k, None)
            if expired_keys:
                logger.debug(
                    f"[LLM] Cleaned {len(expired_keys)} expired conversation overrides"
                )

        eligible = []
        override_provider = None

        # å¦‚æœæœ‰ä¸´æ—¶è¦†ç›–ï¼Œæ£€æŸ¥è¦†ç›–ç«¯ç‚¹ï¼ˆconversation > globalï¼‰
        effective_override = None
        if conversation_id and conversation_id in self._conversation_overrides:
            effective_override = self._conversation_overrides.get(conversation_id)
        else:
            effective_override = self._endpoint_override

        if effective_override:
            override_name = effective_override.endpoint_name
            if override_name in self._providers:
                provider = self._providers[override_name]
                if provider.is_healthy:
                    config = provider.config
                    # æ£€æŸ¥èƒ½åŠ›æ˜¯å¦æ»¡è¶³
                    tools_ok = not require_tools or config.has_capability("tools")
                    vision_ok = not require_vision or config.has_capability("vision")
                    video_ok = not require_video or config.has_capability("video")
                    thinking_ok = (not require_thinking) or config.has_capability("thinking")
                    audio_ok = not require_audio or config.has_capability("audio")
                    pdf_ok = not require_pdf or config.has_capability("pdf")

                    if tools_ok and vision_ok and video_ok and thinking_ok and audio_ok and pdf_ok:
                        override_provider = provider
                        logger.debug(f"[LLM] Using override endpoint: {override_name}")
                    else:
                        logger.warning(
                            f"[LLM] Override endpoint {override_name} doesn't support "
                            f"required capabilities, falling back to default selection"
                        )

        for name, provider in self._providers.items():
            # æ£€æŸ¥å¥åº·çŠ¶æ€ï¼ˆåŒ…æ‹¬å†·é™æœŸï¼‰
            if not provider.is_healthy:
                cooldown = provider.cooldown_remaining
                if cooldown > 0:
                    logger.debug(f"[LLM] endpoint={name} skipped (cooldown: {cooldown}s remaining)")
                continue

            config = provider.config

            if require_tools and not config.has_capability("tools"):
                continue
            if require_vision and not config.has_capability("vision"):
                continue
            if require_video and not config.has_capability("video"):
                continue
            if require_thinking and not config.has_capability("thinking"):
                continue
            if require_audio and not config.has_capability("audio"):
                continue
            if require_pdf and not config.has_capability("pdf"):
                continue

            eligible.append(provider)

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        eligible.sort(key=lambda p: p.config.priority)

        # ç«¯ç‚¹äº²å’Œæ€§ï¼šæœ‰å·¥å…·ä¸Šä¸‹æ–‡æ—¶ï¼Œå°†ä¸Šæ¬¡æˆåŠŸçš„ç«¯ç‚¹æå‡åˆ°é˜Ÿåˆ—å‰ç«¯
        # è¿™æ · failover åçš„ä¸‹ä¸€æ¬¡è°ƒç”¨ä¼šç»§ç»­ä½¿ç”¨æˆåŠŸçš„ç«¯ç‚¹ï¼Œè€Œä¸æ˜¯å›åˆ°é«˜ä¼˜å…ˆçº§çš„æ•…éšœç«¯ç‚¹
        if prefer_endpoint:
            prefer_provider = next(
                (p for p in eligible if p.name == prefer_endpoint), None
            )
            if prefer_provider:
                eligible.remove(prefer_provider)
                eligible.insert(0, prefer_provider)
                logger.debug(
                    f"[LLM] Endpoint affinity: prefer {prefer_endpoint} "
                    f"(last successful endpoint with tool context)"
                )

        # å¦‚æœæœ‰æœ‰æ•ˆçš„ overrideï¼Œå°†å…¶æ”¾åˆ°æœ€å‰é¢ï¼ˆoverride ä¼˜å…ˆäºäº²å’Œæ€§ï¼‰
        if override_provider and override_provider in eligible:
            eligible.remove(override_provider)
            eligible.insert(0, override_provider)

        return eligible

    async def _try_endpoints(
        self,
        providers: list[LLMProvider],
        request: LLMRequest,
        allow_failover: bool = True,
    ) -> LLMResponse:
        """å°è¯•å¤šä¸ªç«¯ç‚¹

        ç­–ç•¥å¯é…ç½®ï¼š
        - retry_same_endpoint_first: True æ—¶ï¼Œå³ä½¿æœ‰å¤‡é€‰ä¹Ÿå…ˆåœ¨å½“å‰ç«¯ç‚¹é‡è¯•
        - retry_count: é‡è¯•æ¬¡æ•°
        - retry_delay_seconds: é‡è¯•é—´éš”

        Args:
            providers: ç«¯ç‚¹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            request: LLM è¯·æ±‚
            allow_failover: æ§åˆ¶ç«¯ç‚¹åˆ‡æ¢ç­–ç•¥
                - True: æ— å·¥å…·ä¸Šä¸‹æ–‡ï¼Œå¿«é€Ÿåˆ‡æ¢ï¼ˆæ¯ä¸ªç«¯ç‚¹åªè¯• 1 æ¬¡ï¼‰
                - False: æœ‰å·¥å…·ä¸Šä¸‹æ–‡ï¼Œå…ˆé‡è¯•å½“å‰ç«¯ç‚¹å¤šæ¬¡å†åˆ‡åˆ°ä¸‹ä¸€ä¸ª

        é»˜è®¤ç­–ç•¥ï¼šæœ‰å¤‡é€‰ç«¯ç‚¹æ—¶å¿«é€Ÿåˆ‡æ¢ï¼Œä¸é‡è¯•åŒä¸€ä¸ªç«¯ç‚¹ï¼ˆæé«˜å“åº”é€Ÿåº¦ï¼‰
        å·¥å…·ä¸Šä¸‹æ–‡ï¼šæ¯ä¸ªç«¯ç‚¹é‡è¯• retry_count æ¬¡åæ‰åˆ‡åˆ°ä¸‹ä¸€ä¸ªï¼ˆä¿æŒè¿ç»­æ€§ï¼‰
        æ‰€æœ‰ç«¯ç‚¹éƒ½æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•ï¼Œæ— è®º allow_failover å€¼
        """
        from .providers.base import COOLDOWN_GLOBAL_FAILURE

        errors = []
        failed_providers: list[LLMProvider] = []  # è·Ÿè¸ªæœ¬æ¬¡è°ƒç”¨ä¸­å¤±è´¥çš„ç«¯ç‚¹
        retry_count = self._settings.get("retry_count", 2)
        retry_delay = self._settings.get("retry_delay_seconds", 2)
        retry_same_first = self._settings.get("retry_same_endpoint_first", False)

        # æœ‰å¤‡é€‰æ—¶é»˜è®¤å¿«é€Ÿåˆ‡æ¢ï¼ˆé™¤éé…ç½®äº†å…ˆé‡è¯•æˆ–ç¦æ­¢ failoverï¼‰
        has_fallback = len(providers) > 1
        if retry_same_first or not allow_failover:
            # å…ˆé‡è¯•å½“å‰ç«¯ç‚¹ï¼ˆæœ‰å·¥å…·ä¸Šä¸‹æ–‡æ—¶å¼ºåˆ¶æ­¤æ¨¡å¼ï¼šå¤šæ¬¡é‡è¯•åå†åˆ‡æ¢ï¼‰
            max_attempts = retry_count + 1
        else:
            # æœ‰å¤‡é€‰æ—¶æ¯ä¸ªç«¯ç‚¹åªå°è¯•ä¸€æ¬¡ï¼Œæ— å¤‡é€‰æ—¶é‡è¯•å¤šæ¬¡
            max_attempts = 1 if (has_fallback and allow_failover) else (retry_count + 1)

        # å§‹ç»ˆå°è¯•æ‰€æœ‰ç«¯ç‚¹ï¼ˆå·¥å…·ä¸Šä¸‹æ–‡æ—¶æ¯ä¸ªç«¯ç‚¹å¤šæ¬¡é‡è¯•åå†åˆ‡åˆ°ä¸‹ä¸€ä¸ªï¼‰
        providers_to_try = providers

        for i, provider in enumerate(providers_to_try):
            for attempt in range(max_attempts):
                try:
                    tools_count = len(request.tools) if request.tools else 0
                    logger.info(
                        f"[LLM] endpoint={provider.name} model={provider.model} "
                        f"action=request tools={tools_count}"
                    )

                    response = await provider.chat(request)

                    # æˆåŠŸï¼šé‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                    provider.record_success()

                    logger.info(
                        f"[LLM] endpoint={provider.name} model={provider.model} "
                        f"action=response tokens_in={response.usage.input_tokens} tokens_out={response.usage.output_tokens}"
                    )

                    # æ³¨æ„ï¼šè¿™é‡Œä¸ç¼©çŸ­å…¶ä»–å¤±è´¥ç«¯ç‚¹çš„å†·é™æœŸã€‚
                    # A å¤±è´¥ã€B æˆåŠŸ â‰  å…¨å±€ç½‘ç»œæ³¢åŠ¨ï¼›A çš„è¿œç¨‹æœåŠ¡å¯èƒ½ä»ç„¶æœ‰é—®é¢˜ã€‚
                    # å…¨å±€ç½‘ç»œæ³¢åŠ¨åˆ¤å®šè§æœ¬æ–¹æ³•æœ«å°¾ï¼šæ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥æ—¶æ‰è§¦å‘ã€‚

                    # ç«¯ç‚¹äº²å’Œæ€§ï¼šè®°å½•æœ¬æ¬¡æˆåŠŸçš„ç«¯ç‚¹ï¼Œä¾›åç»­æœ‰å·¥å…·ä¸Šä¸‹æ–‡çš„è°ƒç”¨ä¼˜å…ˆä½¿ç”¨
                    self._last_success_endpoint = provider.name

                    return response

                except AuthenticationError as e:
                    # è®¤è¯/é…é¢é”™è¯¯ï¼šé•¿å†·é™æœŸï¼Œç›´æ¥åˆ‡æ¢ï¼ˆä¸é‡è¯•å½“å‰ç«¯ç‚¹ï¼‰
                    error_str = str(e)
                    # åŒºåˆ†é…é¢è€—å°½å’ŒçœŸæ­£çš„è®¤è¯é”™è¯¯
                    from .providers.base import LLMProvider as _BaseProvider
                    error_cat = _BaseProvider._classify_error(error_str)
                    if error_cat == "quota":
                        logger.error(f"[LLM] endpoint={provider.name} quota_exhausted={e}")
                        provider.mark_unhealthy(error_str, category="quota")
                    else:
                        logger.error(f"[LLM] endpoint={provider.name} auth_error={e}")
                        provider.mark_unhealthy(error_str, category="auth")
                    errors.append(f"{provider.name}: {e}")
                    failed_providers.append(provider)
                    logger.warning(
                        f"[LLM] endpoint={provider.name} cooldown={provider.cooldown_remaining}s "
                        f"(category={provider.error_category})"
                    )
                    break

                except LLMError as e:
                    error_str = str(e)
                    logger.warning(f"[LLM] endpoint={provider.name} action=error error={e}")
                    errors.append(f"{provider.name}: {e}")

                    # è‡ªåŠ¨åˆ†ç±»é”™è¯¯
                    from .providers.base import LLMProvider as _BaseProvider
                    auto_category = _BaseProvider._classify_error(error_str)

                    # é…é¢è€—å°½ï¼šä¸å¯æ¢å¤ï¼Œç«‹å³è·³è¿‡æ­¤ç«¯ç‚¹ï¼ˆä¸ auth åŒç­‰å¤„ç†ï¼‰
                    if auto_category == "quota":
                        logger.error(
                            f"[LLM] endpoint={provider.name} quota exhausted detected in LLMError, "
                            f"skipping remaining retries. Error: {error_str[:200]}"
                        )
                        provider.mark_unhealthy(error_str, category="quota")
                        failed_providers.append(provider)
                        break

                    # æ£€æµ‹ä¸å¯é‡è¯•çš„ç»“æ„æ€§é”™è¯¯ï¼ˆé‡è¯•ä¸ä¼šä¿®å¤ï¼Œæµªè´¹é…é¢ï¼‰
                    non_retryable_patterns = [
                        "invalid_request_error",
                        "invalid_parameter",
                        "messages with role",
                        "must be a response to a preceeding message",
                        "does not support",  # Ollama: "model does not support thinking" ç­‰
                        "not supported",     # é€šç”¨çš„"ä¸æ”¯æŒ"æ ¼å¼
                    ]
                    is_non_retryable = any(
                        pattern in error_str.lower() for pattern in non_retryable_patterns
                    )

                    if is_non_retryable:
                        logger.error(
                            f"[LLM] endpoint={provider.name} non-retryable structural error detected, "
                            f"skipping remaining retries. Error: {error_str[:200]}"
                        )
                        provider.mark_unhealthy(error_str, category="structural")
                        failed_providers.append(provider)
                        break

                    # é‡è¯•å½“å‰ç«¯ç‚¹ï¼š
                    # - å·¥å…·ä¸Šä¸‹æ–‡/retry_same_first æ—¶æ¯ä¸ªç«¯ç‚¹é‡è¯•å¤šæ¬¡å†åˆ‡
                    # - æ— å¤‡é€‰ç«¯ç‚¹æ—¶ä¹Ÿé‡è¯•å¤šæ¬¡
                    should_retry = attempt < max_attempts - 1
                    if should_retry:
                        logger.info(
                            f"[LLM] endpoint={provider.name} retry={attempt + 1}/{max_attempts - 1}"
                            + (" (tool_context)" if not allow_failover else "")
                        )
                        await asyncio.sleep(retry_delay)
                    else:
                        # å½“å‰ç«¯ç‚¹é‡è¯•å…¨éƒ¨å¤±è´¥ï¼Œè®¾ç½®å†·é™æœŸååˆ‡åˆ°ä¸‹ä¸€ä¸ªç«¯ç‚¹
                        provider.mark_unhealthy(error_str)
                        failed_providers.append(provider)
                        logger.warning(
                            f"[LLM] endpoint={provider.name} "
                            f"cooldown={provider.cooldown_remaining}s "
                            f"(category={provider.error_category})"
                        )

                except Exception as e:
                    logger.error(f"[LLM] endpoint={provider.name} unexpected_error={e}")
                    provider.mark_unhealthy(str(e))
                    errors.append(f"{provider.name}: {e}")
                    failed_providers.append(provider)
                    logger.warning(
                        f"[LLM] endpoint={provider.name} "
                        f"cooldown={provider.cooldown_remaining}s "
                        f"(category={provider.error_category})"
                    )
                    break

            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªç«¯ç‚¹
            if i < len(providers_to_try) - 1:
                next_provider = providers_to_try[i + 1]
                logger.warning(
                    f"[LLM] endpoint={provider.name} action=failover target={next_provider.name}"
                    + (" (tool_context, retried same endpoint first)" if not allow_failover else "")
                )

        # â”€â”€ å…¨å±€æ•…éšœæ£€æµ‹ â”€â”€
        # æ‰€æœ‰ç«¯ç‚¹åœ¨åŒä¸€æ¬¡è¯·æ±‚ä¸­è¿ç»­å¤±è´¥ï¼Œä¸”ä¸»è¦æ˜¯ç¬æ—¶é”™è¯¯ï¼ˆè¶…æ—¶/è¿æ¥ï¼‰
        # â†’ å¾ˆå¯èƒ½æ˜¯ä¸»æœºç½‘ç»œæ³¢åŠ¨ï¼Œè€Œéç«¯ç‚¹æœ¬èº«å¼‚å¸¸
        # â†’ ç¼©çŸ­å†·é™æœŸï¼Œè®©ç³»ç»Ÿå°½å¿«æ¢å¤
        if len(failed_providers) >= 2:
            transient_count = sum(
                1 for fp in failed_providers if fp.error_category == "transient"
            )
            if transient_count >= len(failed_providers) * 0.5:
                # å¤šæ•°ä¸ºç¬æ—¶é”™è¯¯ â†’ åˆ¤å®šä¸ºå…¨å±€ç½‘ç»œæ•…éšœ
                logger.warning(
                    f"[LLM] Global failure detected: {len(failed_providers)} endpoints failed "
                    f"({transient_count} transient). Likely network issue on host. "
                    f"Shortening cooldowns to {COOLDOWN_GLOBAL_FAILURE}s for quick recovery."
                )
                for fp in failed_providers:
                    if fp.error_category == "transient":
                        fp.shorten_cooldown(COOLDOWN_GLOBAL_FAILURE)

        # å·¥å…·ä¸Šä¸‹æ–‡ä¸‹æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥
        if not allow_failover:
            logger.warning(
                "[LLM] Tool context detected. All endpoints exhausted (each retried before failover). "
                "Upper layer (Agent/TaskMonitor) may restart with a different strategy."
            )

        hint = _friendly_error_hint(failed_providers)
        raise AllEndpointsFailedError(
            f"All endpoints failed: {'; '.join(errors)}\n{hint}"
        )

    def _has_images(self, messages: list[Message]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        for msg in messages:
            if isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, ImageBlock):
                        return True
        return False

    def _has_videos(self, messages: list[Message]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«è§†é¢‘"""
        for msg in messages:
            if isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, VideoBlock):
                        return True
        return False

    def _has_audio(self, messages: list[Message]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«éŸ³é¢‘"""
        for msg in messages:
            if isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, AudioBlock):
                        return True
        return False

    def _has_documents(self, messages: list[Message]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«æ–‡æ¡£ï¼ˆPDF ç­‰ï¼‰"""
        for msg in messages:
            if isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, DocumentBlock):
                        return True
        return False

    def has_any_endpoint_with_capability(self, capability: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç«¯ç‚¹æ”¯æŒæŒ‡å®šèƒ½åŠ›ï¼ˆä¾› Agent æŸ¥è¯¢ï¼‰"""
        return any(p.config.has_capability(capability) for p in self._providers)

    def _has_tool_context(self, messages: list[Message]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ä¸Šä¸‹æ–‡ï¼ˆtool_use æˆ– tool_resultï¼‰

        ç”¨äºåˆ¤æ–­æ˜¯å¦å…è®¸ failoverï¼š
        - æ— å·¥å…·ä¸Šä¸‹æ–‡ï¼šå¯ä»¥å®‰å…¨ failover åˆ°å…¶ä»–ç«¯ç‚¹
        - æœ‰å·¥å…·ä¸Šä¸‹æ–‡ï¼šç¦æ­¢ failoverï¼Œå› ä¸ºä¸åŒæ¨¡å‹å¯¹å·¥å…·è°ƒç”¨æ ¼å¼å¯èƒ½ä¸å…¼å®¹

        Returns:
            True è¡¨ç¤ºåŒ…å«å·¥å…·ä¸Šä¸‹æ–‡ï¼Œåº”ç¦æ­¢ failover
        """
        from .types import ToolResultBlock, ToolUseBlock

        for msg in messages:
            if isinstance(msg.content, list):
                for block in msg.content:
                    if isinstance(block, (ToolUseBlock, ToolResultBlock)):
                        return True
                    # å…¼å®¹å­—å…¸æ ¼å¼ï¼ˆæŸäº›è½¬æ¢åçš„æ¶ˆæ¯å¯èƒ½æ˜¯å­—å…¸ï¼‰
                    if isinstance(block, dict):
                        block_type = block.get("type", "")
                        if block_type in ("tool_use", "tool_result"):
                            return True
        return False

    def reset_all_cooldowns(self):
        """é‡ç½®æ‰€æœ‰ç«¯ç‚¹å†·é™æœŸ

        ç”¨äºå…¨å±€æ•…éšœæ¢å¤åœºæ™¯ï¼šå½“æ£€æµ‹åˆ°ç½‘ç»œå·²æ¢å¤æ—¶ï¼Œ
        ç«‹å³æ¸…é™¤æ‰€æœ‰ç¬æ—¶é”™è¯¯çš„å†·é™æœŸï¼Œè®©ç«¯ç‚¹å¯ä»¥ç«‹å³è¢«ä½¿ç”¨ã€‚
        """
        reset_count = 0
        for name, provider in self._providers.items():
            if not provider.is_healthy and provider.error_category == "transient":
                provider.reset_cooldown()
                reset_count += 1
                logger.info(f"[LLM] endpoint={name} cooldown reset (global recovery)")
        if reset_count:
            logger.info(f"[LLM] Reset cooldowns for {reset_count} transient-error endpoints")
        return reset_count

    async def health_check(self) -> dict[str, bool]:
        """
        æ£€æŸ¥æ‰€æœ‰ç«¯ç‚¹å¥åº·çŠ¶æ€

        Returns:
            {endpoint_name: is_healthy}
        """
        results = {}

        tasks = [(name, provider.health_check()) for name, provider in self._providers.items()]

        for name, task in tasks:
            try:
                results[name] = await task
            except Exception as e:
                logger.error(f"Health check failed for {name}: {e}")
                results[name] = False

        return results

    def get_provider(self, name: str) -> LLMProvider | None:
        """è·å–æŒ‡å®šåç§°çš„ Provider"""
        return self._providers.get(name)

    def add_endpoint(self, config: EndpointConfig):
        """åŠ¨æ€æ·»åŠ ç«¯ç‚¹"""
        provider = self._create_provider(config)
        if provider:
            self._endpoints.append(config)
            self._endpoints.sort(key=lambda x: x.priority)
            self._providers[config.name] = provider

    def remove_endpoint(self, name: str):
        """åŠ¨æ€ç§»é™¤ç«¯ç‚¹"""
        if name in self._providers:
            del self._providers[name]
        self._endpoints = [ep for ep in self._endpoints if ep.name != name]

    # ==================== åŠ¨æ€æ¨¡å‹åˆ‡æ¢ ====================

    def switch_model(
        self,
        endpoint_name: str,
        hours: float = DEFAULT_OVERRIDE_HOURS,
        reason: str = "",
        conversation_id: str | None = None,
    ) -> tuple[bool, str]:
        """
        ä¸´æ—¶åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹

        Args:
            endpoint_name: ç«¯ç‚¹åç§°
            hours: æœ‰æ•ˆæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤ 12 å°æ—¶
            reason: åˆ‡æ¢åŸå› 

        Returns:
            (æˆåŠŸ, æ¶ˆæ¯)
        """
        # æ£€æŸ¥ç«¯ç‚¹æ˜¯å¦å­˜åœ¨
        if endpoint_name not in self._providers:
            available = list(self._providers.keys())
            return False, f"ç«¯ç‚¹ '{endpoint_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨ç«¯ç‚¹: {', '.join(available)}"

        # æ£€æŸ¥ç«¯ç‚¹æ˜¯å¦å¥åº·
        provider = self._providers[endpoint_name]
        if not provider.is_healthy:
            cooldown = provider.cooldown_remaining
            return False, f"ç«¯ç‚¹ '{endpoint_name}' å½“å‰ä¸å¯ç”¨ï¼ˆå†·é™æœŸå‰©ä½™ {cooldown:.0f} ç§’ï¼‰"

        # åˆ›å»ºè¦†ç›–é…ç½®
        expires_at = datetime.now() + timedelta(hours=hours)
        override = EndpointOverride(
            endpoint_name=endpoint_name,
            expires_at=expires_at,
            reason=reason,
        )
        if conversation_id:
            self._conversation_overrides[conversation_id] = override
        else:
            self._endpoint_override = override

        model = provider.config.model
        expires_str = expires_at.strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"[LLM] Model switched to {endpoint_name} ({model}), expires at {expires_str}")

        return True, f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model}\næœ‰æ•ˆæœŸè‡³: {expires_str}"

    def restore_default(self, conversation_id: str | None = None) -> tuple[bool, str]:
        """
        æ¢å¤é»˜è®¤æ¨¡å‹ï¼ˆæ¸…é™¤ä¸´æ—¶è¦†ç›–ï¼‰

        Returns:
            (æˆåŠŸ, æ¶ˆæ¯)
        """
        if conversation_id:
            if conversation_id not in self._conversation_overrides:
                return False, "å½“å‰ä¼šè¯æ²¡æœ‰ä¸´æ—¶åˆ‡æ¢ï¼Œå·²åœ¨ä½¿ç”¨é»˜è®¤æ¨¡å‹"
            self._conversation_overrides.pop(conversation_id, None)
        else:
            if not self._endpoint_override:
                return False, "å½“å‰æ²¡æœ‰ä¸´æ—¶åˆ‡æ¢ï¼Œå·²åœ¨ä½¿ç”¨é»˜è®¤æ¨¡å‹"
            self._endpoint_override = None

        # è·å–å½“å‰é»˜è®¤æ¨¡å‹
        default = self.get_current_model()
        default_model = default.model if default else "æœªçŸ¥"

        logger.info(f"[LLM] Restored to default model: {default_model}")
        return True, f"å·²æ¢å¤é»˜è®¤æ¨¡å‹: {default_model}"

    def get_current_model(self) -> ModelInfo | None:
        """
        è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯

        Returns:
            å½“å‰æ¨¡å‹ä¿¡æ¯ï¼Œæ— å¯ç”¨æ¨¡å‹æ—¶è¿”å› None
        """
        # æ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸçš„ override
        if self._endpoint_override and self._endpoint_override.is_expired:
            logger.info("[LLM] Override expired, restoring default")
            self._endpoint_override = None

        # å¦‚æœæœ‰ä¸´æ—¶è¦†ç›–ï¼Œè¿”å›è¦†ç›–çš„ç«¯ç‚¹
        if self._endpoint_override:
            name = self._endpoint_override.endpoint_name
            if name in self._providers:
                provider = self._providers[name]
                config = provider.config
                return ModelInfo(
                    name=name,
                    model=config.model,
                    provider=config.provider,
                    priority=config.priority,
                    is_healthy=provider.is_healthy,
                    is_current=True,
                    is_override=True,
                    capabilities=config.capabilities,
                    note=config.note,
                )

        # å¦åˆ™è¿”å›ä¼˜å…ˆçº§æœ€é«˜çš„å¥åº·ç«¯ç‚¹
        for provider in sorted(self._providers.values(), key=lambda p: p.config.priority):
            if provider.is_healthy:
                config = provider.config
                return ModelInfo(
                    name=config.name,
                    model=config.model,
                    provider=config.provider,
                    priority=config.priority,
                    is_healthy=True,
                    is_current=True,
                    is_override=False,
                    capabilities=config.capabilities,
                    note=config.note,
                )

        return None

    def get_next_endpoint(self, conversation_id: str | None = None) -> str | None:
        """
        è·å–ä¸‹ä¸€ä¼˜å…ˆçº§çš„å¥åº·ç«¯ç‚¹åç§°ï¼ˆç”¨äº fallbackï¼‰

        é€»è¾‘ï¼šæ‰¾åˆ°å½“å‰ç”Ÿæ•ˆç«¯ç‚¹ï¼ŒæŒ‰ priority æ’åºåè¿”å›å®ƒä¹‹åçš„ç¬¬ä¸€ä¸ªå¥åº·ç«¯ç‚¹ã€‚
        å¦‚æœå½“å‰ç«¯ç‚¹å·²æ˜¯æœ€ä½ä¼˜å…ˆçº§æˆ–æ— å¯ç”¨ç«¯ç‚¹ï¼Œè¿”å› Noneã€‚

        Args:
            conversation_id: å¯é€‰çš„ä¼šè¯ IDï¼ˆç”¨äºè¯†åˆ« per-conversation overrideï¼‰

        Returns:
            ä¸‹ä¸€ä¸ªç«¯ç‚¹åç§°ï¼Œæˆ– None
        """
        current = self.get_current_model()
        if not current:
            return None

        sorted_providers = sorted(
            (p for p in self._providers.values() if p.is_healthy),
            key=lambda p: p.config.priority,
        )

        found_current = False
        for p in sorted_providers:
            if p.config.name == current.name:
                found_current = True
                continue
            if found_current:
                return p.config.name

        return None

    def list_available_models(self) -> list[ModelInfo]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹

        Returns:
            æ¨¡å‹ä¿¡æ¯åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        """
        # æ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸçš„ override
        if self._endpoint_override and self._endpoint_override.is_expired:
            self._endpoint_override = None

        current_name = None
        if self._endpoint_override:
            current_name = self._endpoint_override.endpoint_name

        models = []
        for provider in sorted(self._providers.values(), key=lambda p: p.config.priority):
            config = provider.config
            is_current = False
            is_override = False

            if current_name:
                is_current = config.name == current_name
                is_override = is_current
            elif provider.is_healthy and not models:
                # ç¬¬ä¸€ä¸ªå¥åº·çš„ç«¯ç‚¹æ˜¯å½“å‰é»˜è®¤
                is_current = True

            models.append(
                ModelInfo(
                    name=config.name,
                    model=config.model,
                    provider=config.provider,
                    priority=config.priority,
                    is_healthy=provider.is_healthy,
                    is_current=is_current,
                    is_override=is_override,
                    capabilities=config.capabilities,
                    note=config.note,
                )
            )

        return models

    def get_override_status(self) -> dict | None:
        """
        è·å–å½“å‰è¦†ç›–çŠ¶æ€

        Returns:
            è¦†ç›–çŠ¶æ€ä¿¡æ¯ï¼Œæ— è¦†ç›–æ—¶è¿”å› None
        """
        if not self._endpoint_override:
            return None

        if self._endpoint_override.is_expired:
            self._endpoint_override = None
            return None

        return {
            "endpoint_name": self._endpoint_override.endpoint_name,
            "remaining_hours": round(self._endpoint_override.remaining_hours, 2),
            "expires_at": self._endpoint_override.expires_at.strftime("%Y-%m-%d %H:%M:%S"),
            "reason": self._endpoint_override.reason,
        }

    def update_priority(self, priority_order: list[str]) -> tuple[bool, str]:
        """
        æ›´æ–°ç«¯ç‚¹ä¼˜å…ˆçº§é¡ºåº

        Args:
            priority_order: ç«¯ç‚¹åç§°åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½æ’åº

        Returns:
            (æˆåŠŸ, æ¶ˆæ¯)
        """
        # éªŒè¯æ‰€æœ‰ç«¯ç‚¹éƒ½å­˜åœ¨
        unknown = [name for name in priority_order if name not in self._providers]
        if unknown:
            return False, f"æœªçŸ¥ç«¯ç‚¹: {', '.join(unknown)}"

        # æ›´æ–°ä¼˜å…ˆçº§
        for i, name in enumerate(priority_order):
            for ep in self._endpoints:
                if ep.name == name:
                    ep.priority = i
                    break

        # é‡æ–°æ’åº
        self._endpoints.sort(key=lambda x: x.priority)

        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        if self._config_path and self._config_path.exists():
            try:
                self._save_config()
                logger.info(f"[LLM] Priority updated and saved: {priority_order}")
                return True, f"ä¼˜å…ˆçº§å·²æ›´æ–°å¹¶ä¿å­˜: {' > '.join(priority_order)}"
            except Exception as e:
                logger.error(f"[LLM] Failed to save config: {e}")
                return True, f"ä¼˜å…ˆçº§å·²æ›´æ–°ï¼ˆå†…å­˜ï¼‰ï¼Œä½†ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}"

        return True, f"ä¼˜å…ˆçº§å·²æ›´æ–°: {' > '.join(priority_order)}"

    def _save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        if not self._config_path:
            return

        # è¯»å–åŸé…ç½®
        with open(self._config_path, encoding="utf-8") as f:
            config_data = json.load(f)

        # æ›´æ–°ç«¯ç‚¹ä¼˜å…ˆçº§
        name_to_priority = {ep.name: ep.priority for ep in self._endpoints}
        for ep_data in config_data.get("endpoints", []):
            name = ep_data.get("name")
            if name in name_to_priority:
                ep_data["priority"] = name_to_priority[name]

        # å†™å›æ–‡ä»¶
        with open(self._config_path, "w", encoding="utf-8") as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)

    async def close(self):
        """å…³é—­æ‰€æœ‰ Provider"""
        for provider in self._providers.values():
            if hasattr(provider, "close"):
                await provider.close()


# å…¨å±€å•ä¾‹
_default_client: LLMClient | None = None


def get_default_client() -> LLMClient:
    """è·å–é»˜è®¤å®¢æˆ·ç«¯å®ä¾‹"""
    global _default_client
    if _default_client is None:
        _default_client = LLMClient()
    return _default_client


def set_default_client(client: LLMClient):
    """è®¾ç½®é»˜è®¤å®¢æˆ·ç«¯å®ä¾‹"""
    global _default_client
    _default_client = client


async def chat(
    messages: list[Message],
    system: str = "",
    tools: list[Tool] | None = None,
    **kwargs,
) -> LLMResponse:
    """ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨é»˜è®¤å®¢æˆ·ç«¯èŠå¤©"""
    client = get_default_client()
    return await client.chat(messages, system=system, tools=tools, **kwargs)
